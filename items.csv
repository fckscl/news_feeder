link,next,text,title
[<200 https://habr.com/ru/company/ruvds/blog/659263/>],page2,Пользователь,DIY Zigbee датчик температуры / Хабр
[<200 https://habr.com/ru/post/662930/>],page2,"Всем привет! Меня зовут Егор Карташов, и я Android-разработчик в команде мобильного оператора Yota. ,Виджет домашнего экрана (App widget) – один из компонентов ОС Android, который появился в одной из первых версий системы (Android 1.5) и сохранил свою концепцию до наших дней. Почти 9 лет они были забыты Google – выпустив обновление в Android 4.2, виджеты надолго были отложены в долгий ящик. ,Однако всё изменилось, когда Apple выпустила iOS 14, в которой обновила свои виджеты и добавила возможность их размещения на рабочем столе. Google отреагировал почти сразу и в следующей версии Android получил масштабную переработку виджетов – дизайн обновили в соответствии с Material You, расширили возможности API, освежили порядочно устаревшую документацию.,Глядя на всё это, мы решили, что мобильному приложению Yota пора обзавестись своими виджетами, и принялись за работу. В этой статье расскажем, с какими проблемами мы столкнулись по ходу разработки и как эти проблемы решали. ,Я не буду расписывать, как инициализировать виджет в приложении и добавить простую верстку. Всё это достаточно подробно описано в ,. Кроме того, с выходом Android 12 она была обновлена с учетом нововведений API 31 – большинство белых пятен закрыто.,Вместо этого речь пойдет о двух вещах: обновлении необходимых данных и их последующем корректном рендеринге.,Самое очевидное и основное требование к виджету – отображение информации о состоянии счета и подключенного продукта: баланс, дату и сумму следующего списания средств и оставшееся количество минут и гигабайтов. Если немного расширить и формализовать требования, то получим следующие пункты:,отображение актуальных данных о счете пользователя;,автоматическое обновление данных по истечении определенного времени;,возможность ручного обновления данных по нажатию соответствующей кнопки;,поддержка двух размеров виджета – 2x2 и 4x1;,корректное отображение виджета на телефонах и планшетах с самыми разными диагоналями и сетками launcher’ов.,На первый взгляд требования выглядят довольно просто. В рамках экранов приложения подобные вещи реализовывались многократно без особых трудностей. Однако факт того, что всё это должно функционировать отдельно от приложения, не учитывая его состояние, заметно повышает градус челленджа.,Исходя из требований, разработку нашего виджета можно разделить на два этапа: ,реализация логики обновления данных;,верстка, её отладка и адаптация.,На каждом из этапов порядочно нюансов и подводных камней. Давайте пойдем по порядку и рассмотрим каждый из них.,В целом, для автоматического и ручного обновления алгоритм одинаков:,стартуем фоновую работу, в рамках которой идём в сеть за свежими данными;,получив данные, сохраняем текущий timestamp как время последней актуализации и запускаем таймер следующего обновления;,по завершении таймера или по нажатию на кнопку ручного обновления возвращаемся в п. 1.,Время работы таймера = ,. Предполагаемый интервал обновления виджета – 3-20 минут. Величина зависит от значения, пришедшего с сервера. Возникает вопрос – зачем такое усложнение, если можно зашить какое-то фиксированное значение в приложении? Всё это нужно для того, чтобы сервер мог распределять нагрузку в зависимости от частоты запросов.,Окей, с общим алгоритмом всё понятно. Закономерный вопрос – какие инструменты выбрать для его реализации?,Самый очевидный вариант – Work Manager. Он успешно используется в приложении для выполнения различных фоновых задач. Кроме того, ,, поэтому логично рассмотреть возможности Work Manager для реализации обновления данных виджета. Однако после написания и тестирования worker’а на поверхность всплывают несколько критичных недостатков, из-за которых от Work Manager стоит отказаться.,Для выполнения работ с определенным интервалом в Work Manager предлагается использовать ,.,В билдер в качестве параметра передается значение таймаута повторного старта работы. Минимальное допустимое значение – 15 минут. Значения меньше игнорируются. ,Таким образом, если от сервера придет таймаут равный 5 минутам, то оно будет пропущено. Это нас совсем не устраивало.,Но что, если не использовать предлагаемый , и сымитировать периодическое выполнение работы? После выполнения сетевого запроса не завершать worker, а ждать указанное сервером время, после которого повторно планировать работу?,Согласно документации, максимальное время работы worker’а – 10 минут, по истечении которых система принудительно остановит его выполнение.,Следовательно, интервалы обновления виджета в 10+ минут будут проигнорированы.,Из этого затруднительного положения можно было бы выйти написанием костылей, отслеживающих отмену работы и повторяющих планирование работы с учетом оставшегося время таймаута. Но от этого нас избавила очень занятная , Work Manager’а.,Допустим, с помощью Work Manager мы запланировали определенное количество фоновых работ. Что произойдет, если после этого устройство будет перезапущено? Все запланированные работы потеряются? ,Если копнуть вглубь Work Manager, то можно заметить два интересных момента. Сначала запланированная работа сохраняется во внутренней БД Work Manager’а.,Затем включается компонент ,.,Его задача – отслеживание броадкастов со следующими action’ами:,ACTION_BOOT_COMPLETED;,ACTION_TIME_CHANGED;,ACTION_TIMEZONE_CHANGED.,После получения одного из них проверяется наличие в БД необработанных задач и  при необходимости производится их повторное планирование.,По завершении всех работ , отключается, чтобы не тратить ресурсы системы впустую.,Всё это нужно для того, чтобы в некоторых кейсах (например, перезагрузка устройства) не потерять запланированные задачи и довести их выполнение до конца.,Вроде бы полезная и правильная фича, но , При включении/выключении , отправляется броадкаст с ACTION_PACKAGE_CHANGED. На него реагируют разные компоненты системы, в том числе ,. После каждого такого броадкаста у виджета будет вызван метод ,. ,, именно здесь стоит планировать обновление виджета с помощью Work Manager. В итоге мы получаем бесконечный цикл:,Кроме того, обновление виджета будет триггериться при использовании Work Manager в других юзкейсах приложения.,Решения этой проблемы пока нет, но, возможно, оно появится – Google решили доработать поддержку виджетов в Work Manager. Как скоро это будет сделано – неизвестно, но связанный , переехал в статус ,. Пока что возможный костыль – планирование задачи-заглушки, ожидающей выполнение через, например, 10 000 лет. , будет зарегистрирован единожды и никогда не выключится, тем самым предотвращая зацикливание.,Плодить костыли в приложении – сомнительное решение, поэтому рассмотрим другие варианты.,Для обновления виджета средствами системы предусмотрен атрибут , в ,. Может быть, можно использовать его в связке с каким-нибудь сервисом? ,К сожалению, это невозможно по нескольким причинам:,Минимальное значение атрибута – 30 минут. Значения меньше игнорируются системой и приводятся к нижней границе.,Значение атрибута нельзя задать программно.,Система может не стриггерить обновление виджета по истечении заданного периода.,Сервисы тоже использовать не получится – вспоминаем , на их запуск и фоновую работу. , – вроде неплохой вариант, лишенный недостатков обычных сервисов. Но в настоящий момент он задепрекейчен в пользу Work Manager. Кроме того, перед подготовкой приложения к Android 12 мы избавились от оставшихся ,’ов – возвращать устаревшее решение в проект нет никакого желания.,Из вариантов остается явным образом использовать ,, что мы в итоге и сделали. Рассмотрим подробнее, что у нас получилось.,Любое обновление виджета – ручное или автоматическое – начинается с вызова метода , в ,:,В нём мы обращаемся к , и с его помощью планируем задачу обновления данных виджета:, – тип инициированного обновления виджета. Может принимать следующие значения:,PLANNED – обновление по истечении таймера;,MANUAL – обновление по нажатию на соответствующую кнопку на виджете;,SYSTEM – обновление виджета, инициированное системой.,Система может инициировать обновление в основном в трех случаях – при установке виджета на рабочий стол, после перезапуска системы и в результате реакции на интент с ACTION_PACKAGE_CHANGED, о котором шла речь при рассмотрении нюансов Work Manager. Чтобы лишний раз не ходить на сервер, эти кейсы обрабатываются отдельно – учитывается наличие кэшированного значения и дата последнего обновления.,После планирования задачи начинает свою работу ,. Сначала получаем набор информации о состоянии виджета – из кэша или с сервера. После этого конструируем новую , для всех установленных на рабочем столе виджетов и обновляем их через ,. Затем либо стартуем таймер следующего обновления, либо заканчиваем работу сервиса, если пользователь не авторизован в приложении.,По истечении таймера (или по нажатию пользователем соответствующей кнопки) рассылаются броадкасты с ACTION_APPWIDGET_UPDATE. , реагирует на них, вызывает метод ,, и всё начинается сначала.,В целом, с логикой обновления закончили. По большому счету она довольно проста – достаточно только выбрать правильный инструмент для её реализации. На текущий момент наиболее оптимальное решение – ,. Также не стоит упускать из виду Work Manager – возможно, в ближайшее время нас ждут обновления, улучшающие интеграцию с AppWidget.,Переходим к разбору следующего этапа и набора проблем – верстке и её корректному отображению.,По большому счёту верстка виджета ничем не отличается от того, что мы привыкли верстать для наших экранов. Тот же XML, те же ,, ,, , и т. д. Однако есть одно важное отличие, о котором всегда нужно помнить. Виджет рендерится в launcher’е с помощью ,. Это класс, представляющий иерархию вьюх, отображающихся в другом процессе. Чаще всего применяется для отображения уведомлений в notification drawer и виджетов на рабочем столе.,Возможности , ограничены – поддерживается только ,. А это значит, что кастомные компоненты нашего приложения использовать не получится. Если засунуть в XML неподдерживаемый компонент, то после добавления виджета на рабочий стол на нем будет красоваться такая надпись:,А в логах – ошибка:,Но, несмотря на ограничения, большинство кейсов , вполне покрывает.,После того, как верстка готова, нас ждет другая проблема – никто не гарантирует, что виджет будет выглядеть так, как задумывалось изначально. На великое многообразие экранов устройств накладывается особенность рендеринга экрана launcher’а. В большинстве случаев свободное пространство разбивается на клетки так, что получается сетка 4x4, 5x4, 6x5 и т. д. Контент, который пользователь добавляет на рабочий стол, распределяется по этим ячейкам и не вылезает за их границы.,Ячейка сетки далеко не всегда квадратная. Размер может быть самым разным – например, 30x60dp. Поэтому не стоит удивляться тому, что вы дизайнили и верстали квадратный виджет, а на тестовом девайсе получили прямоугольник.,Также нужно помнить о размерах экрана и их разрешениях, о возможности настройки размеров шрифтов в настройках приложения. Всё это тоже сильно влияет на финальное отображение виджета на рабочем столе.,И напоследок немного недокументированных особенностей Android 12. Во время тестирования виджета на девайсах с последней версией системы обнаружилось, что наши кастомные шрифты игнорируются – используется дефолтный Roboto.,После непродолжительных поисков по stackoverflow выяснилось, ,.,Позже выяснилось, что шрифты игнорируются и на некоторых китайфонах с более старыми версиями Android.,Что же делать, если хочется писать текст на виджете своим кастомным шрифтом? Очень просто – рисовать текст с помощью , и запихивать в ,. ,Нам очень хотелось использовать наш кастомный шрифт, поэтому для этого мы написали экстеншн ,:,Однако у такого решения есть недостатки. Для такой вроде бы безобидной вещи, как отображение текста, тратится много ресурсов – как системы, так и человека, верстающего виджет. Для некоторых компонентов придется руками рассчитывать размеры, что знатно добавляет коду вербозности и усложняет его дальнейшую поддержку. Поэтому, посоветовавшись, решили не плодить костыли и оставить стандартный Roboto для не поддерживающих кастомизацию устройств.,Несмотря на проделанный путь, наши виджеты пока не идеальны – например, кое-где вылезают косяки масштабирования. С ними мы планируем при первой возможности разобраться и надеемся, что нам в этом поможет , – находящаяся в настоящий момент в альфе библиотека для создания виджетов с помощью инструментов ,.,Надеюсь, этот материал закроет многие белые пятна относительно виджетов и значительно упростит вам жизнь при их реализации. Спасибо за внимание!,Пользователь",Тёмная сторона Android App widgets / Хабр
[<200 https://habr.com/ru/company/ozontech/blog/662800/>],page2,"С нарастающими скоростями и распределёнными системами всё сложнее бывает создать приложение удобным для конечного пользователя. Программы обладают кучей фич. Но выполняют ли они то, что нужно юзерам? А скорость их выполнения достаточная? А производительность при выполнении не хромает? На эти вопросы помогает ответить нагрузочное тестирование (НТ).,Меня зовут Саша, я работаю в команде тестирования Ozon Fintech и расскажу про разнообразный спектр вариантов НТ: как именно мы его применяем и какие инструменты используем. Статья будет полезна тем, кто уже что-то слышал про НТ и хочет добавить его в свой проект, но пока страшновато. Давайте разбираться! ,Основные характеристики качества ПО описаны в стандарте ISO 9126: это функциональность, юзабилити, поддерживаемость, эффективность,  масштабируемость, надёжность. НТ относится к тестированию эффективности. Стандарт определяет эффективность как способность ПО обеспечивать достаточную производительность при наличии определённых ресурсов и под определённой нагрузкой.,Мне нравится подход Рекса Блэка, американского тестировщика, автора книг и учебников по тестированию. В своё время он был президентом , и соавтором программ подготовки к этой сертификации. Согласно его учебнику “Advanced Software Testing — Vol. 3”, неэффективность может выражаться по-разному:,в ,(slow responses times),,в крайне ,(inadequate throughput),,в ,(reliability failures under conditions of load),,в ,(excessive resource requirements). ,Под капотом неэффективности чаще всего прячутся , и ,, а в них сложно вносить изменения на поздних стадиях разработки. Поэтому лучше приступать к тестированию эффективности ещё на стадии дизайна и программирования — в этом вам поможет ревью и статический анализ.,Блэк напоминает, что существует множество ,: ,Представление, что, На самом деле большая часть нагрузочного тестирования не приводит к отказу. Конечно, существует НТ, которое отыскивает эту точку невозврата, но это всего лишь один из видов НТ., Это в корне неверно. Как и в случае с остальным тестированием, НТ должно быть всеобъемлющим на протяжении всего жизненного цикла ПО., На самом деле в НТ необходима крайне тщательная и обширная подготовка, поэтому одним инструментом тут не спастись. ,Так что же это за зверь, это ваше нагрузочное тестирование?,Тестирование эффективности и называют НТ. Согласно ,, НТ — вид тестирования производительности, проводимый с целью оценить поведение компонента или системы при различных нагрузках, обычно между ожидаемыми условиями низкой, типичной и пиковой нагрузки. Можно ещё сказать, что НТ — это тестирование с целью выяснить, выполняет ли система или компонент свои задачи в условиях ограничений за заданные временные интервалы и с определённой пропускной способностью.,Вот ,, которые перечисляет Блэк:, оценивает поведение компонента или системы в условиях увеличивающейся нагрузки, например при увеличении количества параллельных пользователей и/или количества транзакций, чтобы понять, выдержит ли этот компонент или система подобную нагрузку. ,Упор делается на ожидаемую и реалистичную нагрузку, хотя в основу этого НТ включают разнообразные сочетания запросов и их количество. Запросы создаются таким образом, чтобы смоделировать одновременную работу набора пользователей. Это позволяет оценить время ответа и пропускную способность. ,Иногда разделяют многопользовательское НТ с разумным/реалистичным количеством пользователей и объёмное НТ с огромным количеством пользователей.,Его как раз обычно представляют, когда говорят про НТ. Это разновидность тестирования с целью изучения поведения системы или компонента при пиковых объёмах нагрузки и в необычных условиях функционирования, например при нехватке таких ресурсов, как память или доступ к серверам. Стрессовое тестирование — это выкрученное на максимум НТ. Его цель — убедиться, что время ответа, надёжность и функциональность будут деградировать медленно и предсказуемо — и в конце концов отобразится сообщение типа «Я занят, перезвоните позже». Не должно быть асоциального поведения со стороны системы: повреждения данных, блокировки системы или её падения., позволяет обнаружить «бутылочные горлышки», а затем удостовериться, что увеличение мощностей поможет решить проблему. Например, если планируется добавить несколько процессоров для улучшения производительности, то тестирование масштабируемости позволит убедиться, что одних процессоров хватит. Также такое тестирование помогает определить пределы масштабируемости на проде., оценивает производительности процессора под нагрузкой и использование ОЗУ и дисковой памяти., ,исследует поведение системы при высоком уровне нагрузки в течение продолжительного промежутка времени. Обычно эта нагрузка в несколько раз превышает типичную на проде и позволяет обнаружить проблемы, которые могут возникнут после определённого количества транзакций, например утечку памяти. В отличие от обычного НТ тестирование стабильности позволяет найти такие проблемы благодаря своей продолжительности (при обычном НТ утечки могут попросту не начаться или не успеть себя проявить).,моделирует резкий импульсный рост количества параллельных пользователей или процессов внутри системы и позволяет оценить её стабильность при таких скачках и между ними, убедившись, что между скачками система полностью вернулась в норму. , проверяет способность системы выполнять свои функции в определённых условиях в течение заданного промежутка времени или при заданном количестве операций., помогает удостовериться, что увеличение нагрузки на систему никак не сказывается на юзабилити для конечного пользователя, например что в самый ажиотаж распродажи у конкретного Васи все страницы нормально грузятся и корзина работает.,нацелено на насыщение системы нагрузкой и нахождение точки и места отказа. То звено, которое не выдержало нагрузки, помечается как самое слабое в системе. Исходя из этого можно считать, что изменение дизайна этого звена может привести к улучшению производительности и времени ответа при больших нагрузках.,О других типах НТ можно почитать в , (пункт 1.2). ,НТ существует уже не первый год, и за это время накопилось много разных ,. ,Мы используем Яндекс.Танк — это удобный инструмент для тестирования бэка.,Про него написано немало, не буду повторяться:, танка,,,что , простыми словами, ,про , (слайд 54).,Мы в Ozon Fintech пишем на Go и работаем с gRPC, что надо учитывать при выборе пушек для Танка. Кроме того, наши пушки необходимо кастомизировать под свои нужды — выполнение сценариев, создание утилит для автогенерации пушки, автогенерация патронов — поэтому выбор пал именно на ,. ,Она сама ,, обновляется и поддерживается. Создать , на ней довольно просто:,В main() создаётся новый экземпляр пушки, подкладывается необходимый формат патронов, задаётся название.,В Bind() непосредственно настраивается пушка: создаётся conn для адреса тестируемого сервиса, подкладываются аргументы.,В shoot() прописывается сценарий стрельб.,В структуре патрона важную роль играет поле Tag, в зависимости от которого поведение пушки меняется по switch case’у. Пример есть ниже в практической части статьи.,Внутри выпавшего метода отправляем запрос по gRPC на ручку, получаем ответ, приводим код ответа на gRPC к HTTP-шному ответу, отдаём этот код агрегатору.,Создаём патроны для нашей пушки: в Tag кладём название ручки, а в тело — остальные изменяемые поля запросов, которые нужны для работы. Создаём побольше таких патронов, заполняем , для Pandora. А дальше локально или удалённо запускаем обстрел и смотрим на результаты.,Как рассчитать профиль нагрузки? Сколько патронов необходимо создать для проведения тестирования? Для этого нужны требования, которых часто нет, а авторам задачи нужно, «чтобы оно работало». ,Для каждого случая необходимо выяснять свои требования. В качестве основного можно взять такое: перед каждым релизом удостовериться, что «новая версия работает не хуже предыдущей». Другое типовое требование — проверять, что «сервис выдержит нагрузку в случае планируемого расширения системы, например, в десять раз». Для третьей задачи может понадобиться, чтобы «время ответа конечному пользователю не превышало шести секунд».,Остановимся на первом требовании. Как же понять, что «новая версия должна работать не хуже старой»? Для расчёта нагрузки идём в Grafana в описание работы интересующего нас сервиса на проде и смотрим, сколько раз была вызвана та или иная ручка.,Например, текущая нагрузка на ручку = 15 RPS, а тест, который я провожу, должен длиться десять минут. Соответственно, количество уникальных патронов, которые необходимо предгенерить, равно 15 RPS * 60s * 10m = 9000 патронов.,Что, если у меня нет возможности сделать каждый выстрел уникальным? Например, я в запросах обращаюсь к тестовым пользователям, а их в нужном количестве нет. Тогда патронов можно сделать меньше — и после первого прохода по файлу с ними Яндекс.Танк пойдёт на повтор. ,Получается, что вообще можно сгенерить по одному патрону на каждый тип запроса и успокоиться? Можно, но это будут нерепрезентативные запросы. Мы обычно рассчитываем минимальное количество запросов с учётом того, что нам необходимо, чтобы каждый запрос отправлялся на сервис не чаще, чем раз в пять секунд. Тогда с нашими условиями получится, что надо создать минимум 15 RPS * 5s = 75 патронов.,Что делать, если у нас не только простая нагрузка «один запрос — один ответ», а целый сценарий? Например, в запрос нужно добавить свежий токен. Или для подтверждения запроса в одной ручке надо дёрнуть вторую ручку.,Разберём такой тестовый сервис, обладающий методами MyRegistrationHandle, MyAccountHandle, UpdateBalance и ComplyBalance. На каждую ручку свой кейс:,MyRegistrationHandle — кейс с простой отправкой запроса.,MyAccountHandle — кейс с запросом к стороннему сервису авторизации.,UpdateBalance — кейс с последовательным выполнением запросов к одному и тому же сервису сначала на UpdateBalance, потом — на ComplyBalance.,Получается, что код shoot() в нашей пушке будет выглядеть так: ,Под капотом первого кейса всё просто: отправляем запрос к тестовому сервису, получаем ответ, убеждаемся, что ClientID вернулся не пустой.,Во втором кейсе сначала нужно выполнить все манипуляции для получения токена, значение которого потом подкладывается в конечный запрос. В моём случае для этого надо совершить только одно действие — получить его из GetToken. Но сколько бы ни требовалось действий, нужно проверять каждый ответ и возвращать уникальный код ошибки, чтобы точно определять место отказа, если что-то пойдёт не так.,В третьем случае мы последовательно отправляем запросы к одному и тому же сервису. Запросом к первой ручке создаётся запись в базе в статусе черновика, а вторым запросом эта запись подтверждается. ,Опять же, критично на каждом из шагов проверять коды ответа и приходящую информацию. Это важно, чтобы знать, на каком именно шаге что сломалось. Также необходимо все кастомные коды ошибок делать уникальными — чтобы было проще определять место поломки. Например, в третьем кейсе в обоих случаях проверяется сумма изменения баланса клиента, но при ошибке в работе ручки UpdateBalance вернётся errorCode4, а в ComplyBalance — errorCode5.,Дополнительно можно добавить логи на ошибки. Банальные фразы типа,резко повышают читаемость кода.,Pandora активно используется для проверки эффективности сервисов внутри Ozon Fintech. Сейчас мы активно развиваем качество пушек и увеличиваем их количество. Сервисов много, для каждого нужны уникальные запросы, патроны, поведение, сценарии. Для чего-то требуется предварительная авторизация, для чего-то — нет. Поведение из некоторых UI-тестов копируется в бэкендовых нагрузочных тестах и позволяет проверять пропускную способность сервиса на пользовательских сценариях. Нехитрыми способами, описанными выше, можно покрыть существенную часть тестов.,Сервисов много — сначала мы создавали пушки вручную, а теперь делаем это с помощью генератора. Параллельно наша команда работает над концепцией «Нагрузочное тестирование как сервис» — предоставлением полного цикла стрельбы от генерации и загрузки патронов на сервер и сборки пушки до проведения обстрела, сбора метрик и выведения результатов по нажатию на кнопочку в пайплайне CI.,Надеюсь, моя статья помогла понять возможности НТ и убедила вас в том, что это полезный и необходимый инструмент для любого проекта. Stay tuned!,Пользователь",50 оттенков нагрузочного тестирования / Хабр
[<200 https://habr.com/ru/post/662928/>],page2,"Цель статьи - описать создание инфраструктуры для парсинга на базе python, Django, Celery и Docker.,В студенческие годы я написал на заказ много парсеров магазинов и социальных сетей. Со временем парсеры усложнялись и из скриптов превращались в полноценные веб-приложения c базой данных и Rest API. В статье описан шаблон веб-приложения, который использую для создания парсеов.,Из статьи мы узнаем,Как создавать Rest API на базе Django Rest Framework,Как создавать асинхронные задачи с помощью Celery,Как деплоить веб-пиложение с использованием Docker-compose,Django для обработки HTTP запросов и хранения данных, Redis - транспорт (брокера сообщений),Celery - для создания очередей задач (задач парсинга),Пользователь делает HTTP POST запрос /task ,Если запрос содержит правильные данные, то приложение запускает задачу парсинга,В базу данных сохраняются результаты задачи,Для аккуратности  создадим отдельное приложение (директорию), код которого будет отвечать за логику приложения (парсинг),После выполнения команд у нас должен получиться такой проект:,Создаем .env файл с переменными окружения,Модифицируем файл с настройками,Описываем библиотеки, которые потребуются для запуска проект,Описываем инструкции для сборки докер-образа в Dockerfile,Конфигурируем nginx,Создаем docker-compose.yml на одном уровне с папкой project/,Запускаем приложение,Далее зайдем в докер-контейнер и создадим суперпользователя,После этого заходим на 127.0.0.1 Должен быть такой результат:,Добавляем в  docker-compose.yml файл новые сервисы,Добавляем настройки для celery,Создаем объект для работы с celery,Импортируем celery приложени, чтобы оно запускалось вместе с django,Мы будем парсить сайт , так как сайт предназначен для парсинга и не изменится. В HTTP POST запросе пользователь будет передавать название категории (напрмер, mystery_3),Программа будет сохранять названия книг из этой категории,Начнем с создания модели данных,Изменения в urls.py файлах. У нвс будет всего один запрос task/ -- постановка задачи на пасинг и мониторнг результатов,В файле views.py описываем обработчик запроса task/,GET -- возвращает статус задачи, POST -- ставит задачу на парсинг,Создаем файл tasks.py с описание логики работы задачи парсинга,Добавляем в админу данные,Перезапускаем приложение.,5.1 Запрос парсинга POST,5.2 Посмотрим результат задачи,5.3 Результаты работы в админке,Приложение готово!,Буду признателен за фидбек :-),Программист C++ / Python",Парсинг для взрослых или Инфраструктура для промышленного парсинга / Хабр
[<200 https://habr.com/ru/post/662940/>],page2,"У меня есть приложение на Go, в котором в одном из потоков работает простой HTTP сервер. К этому серверу обращаются по HTTPS. Запрос приходит на , - HTTP/HTTPS reverse-proxy and load-balancer и перенаправляется в приложение. TLS сертификат изготавливается и обновляется с помощью ,.,Простая и привычная схема. Правда, чаще в этой схеме бывает ,, но в этой статье мы не будем рассуждать, почему Pound, а не Nginx. Все очень хорошо, но меня последнее время начинает раздражать, когда к простому и понятному коду на Go нужно прикрутить небольшого динозаврика с пять-шестью скриптами на ,, закатать все это в деплой и радоваться тому, как это все славно улеглось в небольшой виртуалке. ,Я где-то там, в своем начале длительное время писал на ,-е. Размер откомпилированной программы больше одного Кб считался огромным. Видимо с тех пор во мне застряло желание экономить ресурсы компьютера, несмотря на то, что разработчики железа предоставляют их все больше и больше. Так же я ни как не могу привыкнуть к этим жутким папкам под названием ‘node_modules’ и всегда хочу стереть их содержимое.,И я задумался... ,Задумался над тем, как можно сократить количество участников в задаче «HTTP-сервер с сертификатом Let's Encrypt на Go». Год думал. Шучу. Мое приложение с динозавриками замечательно проработало год, и настало время сделать обновления.,И решение нашлось. Оказывается, все, что нужно для облегчения этой задачи, есть в , Итак, вот что нужно написать в программе на Go для того, что бы работал HTTPS сервер и автоматически создавался и обновлялся TLS сертификат по технологии Let’s Encrypt:,И все! Да, и все - те же самые стандартные три строчки, которые присутствуют во всех примерах создания HTTP-сервера. Ищем отличия, а я посмотрю на секундомер...,Ну конечно! - ‘autocert.NewListener(domain)’ в ‘http.Serve’.,Приложение, использующее этот код, должно быть запущено с правами администратора, и на хосте должен быть белый IP адрес. Без него Let’s Encrypt не сможет создать сертификат.,Для того что бы работала привычная нам переадресация с HTTP на HTTPS, добавим еще пару строк кода перед предыдущим кодом:,Здесь мы создали еще один HTTP сервер, работающий на 80-м порту и пересылающий все на наш основной сервер с помощью функции ‘redirectTLS’:,Приложение имеет параметр '–domain' и строка запуска выглядит так:,Конечно. Для запуска этого приложения у вас должен быть зарегистрированный домен и белый IP адрес.,Программа размещена на ,:,Вот ещё один адрес на ,, пока он запасной:,Программист (вечный)",HTTPs-сервер с сертификатом Let's Encrypt на Go / Хабр
[<200 https://habr.com/ru/company/unigine/blog/662782/>],page2,"Если вы планируете переходить с иностранного софта на отечественный и ищете полноценный аналог Unity или Unreal Engine, то одним из вариантов может стать продукция нашей компании, полностью готовая к импортозамещению. UNIGINE использует общепринятые интерфейсы и рабочие процессы, которые могут быть вам знакомы по работе с другими 3D-инструментами. По опыту наших клиентов, для перехода на UNIGINE с других платформ уходит не более 1–2 недель.,Одна из таких платформ — платформа разработки в реальном времени Unity. Далее в статье рассмотрим базовую информацию по переходу на UNIGINE.,Сначала разберемся с названиями различных сущностей и другими терминами. В таблице ниже приведены термины Unity и их точные или приблизительные эквиваленты в UNIGINE.,Hub,SDK Browser,Hierarchy Panel ,Окно World Nodes,Inspector,Окно Parameters,Project Browser ,Окно Asset Browser,Scene View,Editor Viewport,Scene,World,Component,Component System,GameObject ,Node,Prefab,NodeReference,Mesh Renderer,Static Mesh,Dynamic Mesh,Skinned Mesh Renderer ,Skinned Mesh,Blendshapes ,Morph Targets,Particle System,Particle System,Halo,Volumetric Objects,Lens Flares,Lens Flares,Billboard Renderer ,Billboards,Projector / Decal Projector (HDRP),Decals,Terrain,Terrain Systems,Trees / Grass,Mesh Clutter Grass,аддон Vegetation,Wind Zones,Animation Field,UI (,GUI (,Light Sources,Light Sources,Environment,Environment,Lightmapping,LightmappingVoxel GI,Reflection Probes ,Environment Probes,Shade,Base Material,Material,User Material,Кастомные шейдеры:,HLSL,Shader Graph,HLSL,GLSL,UUSL (,Визуальный редактор материалов ,Compute Shaders ,UUSL Compute Shaders,Rendering Paths,Rendering Sequence,Multi-Display Rendering ,Плагины для рендеринга на нескольких экранах (Multi-Monitor Rendering),Плагин Syncker для многоканальной визуализации,C#,C++,C#,UnigineScript,Scriptable Render Pipeline URP,HDRP,Rendering Sequence (при полном доступе из API),Scriptable Materials,Raycast,Intersections,Rigid Body,Rigid Body,Collider,Shape,Joint,Joint,Cloth,Cloth Body,Timeline,Tracker,NavMesh,NavMeshAgent,Off-Mesh Link ,NavMesh Obstacle ,Navigation Areas,Obstacles,Пользователи , используют , — приложение для поиска, загрузки и управления версиями движка и проектами.,В UNIGINE для этих целей служит ,. Помимо управления проектами и установленными SDK, браузер SDK предоставляет доступ к примерам (,), базе знаний (,) и дополнениям (,). В последнюю категорию входят различные 3D-модели и материалы, в том числе растительность, спецэффекты, погодные эффекты и другое.,Также заметным отличием UNIGINE является возможность создания нового (или редактирование старого) проекта с поддержкой одного из нескольких языков программирования: ,, , и ,. Для пользователей Unity рекомендуется использовать ,. Также возможно использование нескольких языков программирования в одном проекте. Например, для выполнения ресурсоемких задач часто используют ,.,Нажмите , в разделе ,.,Выберите тип проекта , в поле ,.,Если требуется поддержка VR-гарнитур, перейдите в раздел ,, отметьте необходимые плагины в секции , и нажмите , (больше о поддержке VR-устройств ,).,Нажмите ,.,После завершения загрузки нажмите ,, чтобы запустить UNIGINE Editor.,Элементы интерфейсов , и , близки по функционалу: на схеме ниже они окрашены в похожие цвета. Расположение элементов UNIGINE Editor можно настраивать, перетаскивая и изменяя их размер. В UNIGINE по умолчанию используется темная тема., Панель инструментов, которая обеспечивает доступ к инструментам позиционирования, а также элементам управления логикой приложения, воспроизведением звука, симуляцией физики, компиляцией шейдеров и запеканием света., Инструмент для работы с иерархией нод. Позволяет организовывать ноды в иерархию, а также добавлять, удалять, клонировать и переименовывать их., Просмотр трехмерной сцены. Позволяет визуально перемещаться и редактировать виртуальный мир., Окно параметров выбранного элемента виртуального мира. Позволяет просматривать и изменять параметры нод, материалов, свойств и ассетов., Инструмент для организации контента в проекте: создания, импорта, просмотра, переименования ассетов, а также перемещения и управления их иерархией.,Инструменты для просмотра виртуальной сцены , и , очень похожи между собой — это непосредственно само окно просмотра и панель инструментов.,Вы можете использовать столько окон ,, сколько вам необходимо., служит для переключения между камерами и настройки текущей камеры., требуется для отображения содержимого буферов рендеринга так же, как при использовании , в редакторе Unity., используется для быстрой настройки и переключения между пресетами скорости камеры, а также для изменения положения камеры., обеспечивает быстрый доступ к вспомогательным визуализаторам, таким как значки, гизмо и каркасы.,Навигация внутри , почти такая же, как и в , Unity. Подробнее ознакомиться с навигацией по сцене можно, просмотрев видео ниже (либо прочитав соответствующий раздел в ,):, (предварительная компиляция всех шейдеров) используется для ,;, (анимации);, (физики);, (звука);, для управления воспроизведением.,В режиме воспроизведения , редактора Unity рендерит финальную сцену с одной или нескольких камер.,В UNIGINE кнопка , используется для запуска экземпляра приложения в отдельном окне. Также есть возможность переключения между режимами воспроизведения для изменения его основных параметров. Так, например, можно включить режим VR, чтобы обеспечить совместимость с одной из поддерживаемых гарнитур виртуальной реальности., в UNIGINE (аналог ,), используемый для отладки и оценки производительности, считается избыточным для проектов, разрабатываемых на C# .NET 5. Однако, его можно использовать в других проектах.,Как в Unity, так и в UNIGINE есть консоль для стандартного ввода, вывода и регистрации ошибок. Также существует набор консольных команд, позволяющих совершать определенные операции.,Консоль доступна как в UNIGINE Editor, так и в работающем приложении. Чтобы открыть окно консоли в редакторе, перейдите в меню ,:,Во время работы приложения встроенная консоль запускается нажатием кнопки «,.,Во встроенную консоль можно выводить , из кода.,Так же как и Unity Editor, UNIGINE Editor позволяет выполнить подготовку финальной сборки проекта.,Проект в UNIGINE, как и проект на Unity, хранится в отдельной папке, настройки проекта хранятся в файле с расширением ,. В папке проекта есть различные подпапки с контентом и исходным кодом приложения. Также тут хранятся папки с файлами конфигурации и исполняемыми файлами. Наиболее важные подпапки: , (данных) и , (исходного кода).,Каждый проект UNIGINE обязательно включает в себя папку ,. Как и в папке , проекта на Unity, здесь хранятся ресурсы вашего проекта. Для импорта ассетов достаточно перетащить файлы в папку data — они автоматически импортируются и станут доступны в Asset Browser. Ассеты в UNIGINE Editor будут автоматически обновляться при внесении изменений в соответствующие файлы.,Соответствие содержимого папки data в корневой директории проекта и в Asset Browser,Unity поддерживает широкий спектр форматов файлов, в то время как UNIGINE поддерживает большинство наиболее популярных, а также ряд специфических:,                , Ассеты в формате , могут быть легко импортированы из Unity в UNIGINE без искажения масштаба.,Подробнее про импорт FBX-моделей читайте ,., Так же как и Unity, UNIGINE работает с PBR-материалами (,) и поддерживает , и ,. Материалы, созданные в Unity, можно воссоздать в UNIGINE, благодаря встроенной богатой библиотеке материалов, а также возможности визуально создавать и редактировать материалы в Visual Material Editor., Текстуры можно импортировать либо как часть модели, либо отдельно, а затем назначать их мешу. Но иногда может потребоваться предварительная подготовка. Например, Shading-текстура в UNIGINE хранит карты ,, ,, , и , в соответствующих каналах. Поэтому сначала нужно изменить Shading-текстуру с помощью GIMP или Photoshop, а затем импортировать ее в UNIGINE. А перед импортом Normal-текстуры нужно инвертировать канал , с помощью соответствующей настройки при импорте.,Модель со скелетной анимацией, которую вы использовали в проекте Unity, может быть импортирована в UNIGINE, если она хранится в формате FBX. При импорте такой модели, включите опцию , (импорт анимаций) и настройте дополнительные параметры.,Подробнее про импорт разных ассетов читайте ,.,Концепция сцены в обоих движках одинакова. Однако Unity и UNIGINE используют разные системы координат.                                  ,    , использует , систему координат, в которой вертикальное направление представлено осью ,.        1 юнит равен 1 метру.    , — вправо (+), влево (-), — вверх (+), вниз (-), — вперед (+), назад (-)Положительный угол поворота задает вращение по часовой стрелке.Формат файла: , использует , систему координат, в которой вертикальное направление представлено осью ,.        ,1 юнит равен 1 метру., — вправо (+), влево (-), — вперед (+), назад (-), — вверх (+), вниз (-),Положительный угол поворота задает вращение против часовой стрелки.,Формат файла: ,Как в Unity, так и в UNIGINE, сцена формируется из базовых объектов. С их кратким описанием, а также основными сходствами и различиями можно ознакомиться ниже.,Окно Hierarchy    ,Окно World Nodes,Базовый объект сцены — GameObject.,Базовый тип, от которого наследуются все типы объектов сцены — Node.,Некоторые имеют визуальное представление: Objects, Decals и Effects. У каждого из них есть поверхности для представления своей геометрии (меши). Остальные — Light Sources, Players и др. — невидимы.,GameObjects являются контейнерами для всех остальных компонентов.,Компоненты определяют функционал GameObject.,Базовая функциональность ноды определяется ее типом.,Дополнительные функции можно добавлять с помощью свойств и компонентной системы.,По умолчанию каждый GameObject имеет компонент Transform.,У каждой ноды есть матрица преобразования, которая задает ее положение, поворот и масштаб в пространстве.,GameObjects могут быть организованы в иерархию типа родитель-потомок.,Ноды могут быть организованы в иерархию типа родитель-потомок.,. Обычно вы собираете сложный объект из , с определенными компонентами и свойствами и создаете префаб из такого объекта. Затем префабы могут быть размещены в сцене посредством редактора или созданы во время выполнения приложения.,, которые очень похожи на префабы. Чтобы создать сложный объект, экземпляры которого затем будут многократно использоваться в сцене, достаточно построить нужную иерархию из нод, назначить им материалы и свойства, а затем сохранить ее как Node Reference. Так же, как и в случае с префабами, вы в любой момент сможете изменить Node Reference, просто изменив любой из ее экземпляров.,Подробнее про создание Node Reference и управление смотрите видео ниже (или читайте в ,):,В Unity Editor для разрешения конфликтов, возникающих при слиянии рабочих копий проекта, используется инструмент ,. Также редактор позволяет применять пользовательские инструменты для тех же целей. Для успешного объединения, сцены и другие файлы должны быть в формате ,.,В UNIGINE все исходные форматы файлов по умолчанию являются текстовыми, поэтому вы можете использовать любую привычную систему контроля версий и объединять миры, ноды и другие ассеты. Файловую систему можно расширять с помощью ,, которые позволяют добавлять в проект любые внешние ресурсы, находящиеся в совместном доступе. Кроме того, стандартный подход к работе над проектом заключается в разделении работы разных членов команды с помощью отдельных ,. Это позволяет избежать необходимости разрешения конфликтов при слиянии изменений.,Подробнее про совместную работу над проектом читайте ,.,Камеры в Unity и UNIGINE устроены немного по-разному., Все включенные камеры, присутствующие в сцене, визуализируются в окне просмотра (,) и могут перекрывать друг друга. Для переключения между камерами обычно нужно отключить текущую камеру и включить другую., Для упрощения создания наиболее часто используемых камер, управляемых с помощью устройств ввода (клавиатура, мышь, джойстик), предусмотрено несколько типов , с различным поведением:, — простая статическая камера, которая может быть усовершенствована с помощью пользовательских доработок., — камера свободного перемещения., — камера, которая следует за целевым объектом и может свободно вращаться вокруг него на заданном расстоянии. Это готовое решение для создания камеры от третьего лица., — камера с твердым физическим телом капсульной формы, которая может взаимодействовать с окружением. Это готовое решение для создания вида от первого лица, схожее с ,.,Одновременно Editor Viewport показывает вид только с одной камеры. Переключаться между камерами можно с помощью ,:,Чтобы в режиме воспроизведения (когда нажата кнопка ,) определенная камера использовалась по умолчанию, нужно установить флажок , в ее настройках.,Настройка общих параметров проекта в Unity Editor обычно выполняется через окно настроек проекта (меню: , -> ,). Аудио, графика, физика, уровни качества и другие настройки влияют на весь проект.,В UNIGINE общие настройки доступны вменю , -> , в разделе ,. Настройки мира задаются для каждого мира отдельно.,В Unity Editor асинхронная компиляция шейдеров включается и выключается в настройках редактора (меню: , -> , -> , -> ,).,В UNIGINE аналогичная функция редактора называется ,. Она доступна как через панель инструментов, так и через раздел , окна ,.,Вы используете пресеты в редакторе Unity, когда вам нужно повторно использовать настройки свойств, относящиеся к различным задачам, будь то настройки компонентов, настройки импорта или, в особенности, настройки проекта (,). Вы можете сохранить настройки для определенного раздела Project Settings в качестве , ассета и использовать его в процессе разработки.,UNIGINE позволяет сохранять и загружать пресеты для общих настроек физики, звука и рендеринга. Пресеты хранятся в виде ассетов с расширениями ,, , и , соответственно. Для загрузки и сохранения пресетов используются кнопки , и , в соответствующем разделе настроек окна ,.,Сохраненные ассеты отображаются в Asset Browser. Вы можете загрузить настройки рендеринга, дважды щелкнув необходимый ассет с расширением ,.,В UNIGINE пресеты доступны не только в редакторе. Вы можете использовать классы ,, , и , для управления пресетами соответствующих настроек — например, для переключения между уровнями качества во время выполнения приложения.,В Unity Editor настройки качества графики в основном собраны в следующих разделах:, Настройки уровня (,) обеспечивают платформенно-ориентированную настройку рендеринга и компиляции шейдеров. Уровень определяется автоматически в зависимости от используемой платформы.,, заданные для каждой платформы.,В UNIGINE настройки рендеринга мира можно найти в разделе , окна ,. Также есть возможность включать и выключать самые распространенные функции рендеринга с помощью соответствующего меню:,В UNIGINE нет платформенно-зависимых настроек качества, поэтому для управления уровнями качества необходимо написать свою собственную логику. Для этой цели можно использовать , (,).,Рассмотрим наиболее часто используемые настройки рендеринга в Unity и соответствующие им аналоги в UNIGINE:,                                                  , ,см. ниже, ,Preload at World Loading, ,Forward Per-Object Limits, , , , , , , , ,Меню: , , настройки ,настройки , ,устанавливается для каждого источника , , , настройки Video,В Unity существует два способа рендеринга: , (отложенный) и , (прямой) рендеринг. Они определяют точность шейдинга, а также потребление ресурсов при рендеринге и необходимое аппаратное обеспечение. Способ рендеринга можно выбрать в окне , для каждой камеры.,UNIGINE имеет фиксированную последовательность рендеринга, представленную комбинацией полного отложенного рендеринга с методами упреждающего рендеринга:,Вся непрозрачная геометрия отрисовывается в отложенном проходе (,).,Прозрачная геометрия отрисовывается во время прямого прохода (,).,Вы можете уменьшить вычислительную нагрузку, пропустив определенные этапы рендеринга. Посмотрите специальный видеоурок по использованию инструмента , для оптимизации рендеринга:,В Unity доступность эффектов постобработки определяется используемым конвейером рендеринга. В UNIGINE подобные эффекты не являются частью постобработки, а интегрированы в процесс рендеринга. Таким образом, , (HDRP) наиболее приближен к процессу рендеринга в UNIGINE по сравнению с другими конвейерами рендеринга.,В Unity для определения объемов, в которых параметры и эффекты постобработки локально (или глобально) переопределяются, используется фреймворк ,. В UNIGINE для плавной перехода между эффектами в различных областях необходимо написать собственную логику.,Методы сглаживания:, ,        ,Методы сглаживания:        , ,Эффекты камеры:            , , ,Цветокоррекция:, ,         ,Материалы постобработки:    , , , , , (,), (,),Материал получился объемный, но это лишь первый, обобщенный выпуск из запланированных трех по миграции с Unity. Следующий выпуск будет более специализированным и расскажет про миграцию на UNIGINE с точки зрения 3D-художника. А в последнем разберем все важные моменты по этому вопросу для программистов.,Чтобы получить доступ к бесплатной версии UNIGINE 2 Community заполните , на нашем сайте.,Все комплектации UNIGINE:, — базовая версия для любителей и независимых разработчиков. Достаточна для разработки видеоигр большинства популярных жанров (включая VR)., — расширенная, специализированная версия. Включает множество заготовок для инженерных задач., — максимальная версия платформы под масштабные проекты (размеров планеты и даже больше) с готовыми механизмами симуляции.,Пользователь",Быстрый переход на UNIGINE с Unity: первые шаги / Хабр
[<200 https://habr.com/ru/post/662942/>],page2,"Хотели когда-нибудь примерить на себя костюмчик успешного архитектора из мира больших бизнесов? Ну тех, кто зарабатывает на лекциях и подкастах больше, чем на основной работе. Рецепт то не особенно сложный: пара успешных проектов и кул стори в интернетах. , Иногда в комплекте к костюму идут одноцветные тапочки…,Успешный успех. Звучит как сказка! Вот только если вы прагматичный , архитектор, то , на фоне ,. Те, кто поднимают золотое знамя успеха, любят поднимать на вилы, тех осторожно предлагает подождать с конфети и фейерверками. Просто в свете и шуме салютов можно не услышать криков и пропустить тревожные сигнальные ракеты. Ведь об успехе заявляют не по окончании проекта. А зачастую даже не в точке первой прибыльности. Закончили ,и всё – успех! Вроде бы и обязательства выполнены, но, кажется, что-то забыли... Если софтина работает, то вы не поверите, с ней начинают работать! И как бы ни хотелось разработчикам, создание – относительно короткий срок в сравнении с использованием. Ход мысли довольно прост, но почему-то всегда встает на пол пути: ,Во время планирования и разработки системы вы воевали с адептами карго культа. Они верят в звучные имена технологий и призывают их во имя победы. Достаточно чтоб у нас было ""Облако"" или ""Микросервисы"" и охота будет удачной, а племя сытым. Вот охота на территориях клиента принесла первого буйвола. Теперь шаманов-культистов сменят каннибалы. У успеха много отцов и всех не прокормить с одной тушки. Вдруг окажется, что штат слишком раздут. Но проект же успешен и должен расти. Было 4 команды по 5 человек - станет 6 команд по 3. ,Энтерпрайз - всё же продуктовая история. Только очень богатые среди больших и богатых (ну или с особыми необходимостями, как сейчас правильно говорить) делают разработку системы только для себя и под себя (что-то типа гибридного аутсорса). Остальные адаптируют платформы (продукты одних корпораций для других корпораций). ,На этапе эксплуатации системы основной задачей архитектора станет уменьшение страданий. В первую очередь самого архитектора. При разработке платформы ваша компания скорее всего продавала ,. Продукта еще не было. Это понимают и клиенты. Так что первые заказы – наверняка в убыток. И как только уже оплаченные требования заканчиваются, начинает пахнуть деньгами. У компании с новым продуктом лишь одна задача – отбить деньги с разработки платформы. Для этого обычно требуют две вещи: история успеха (успешный клиент поможет продвинуть новый продукт на рынке) и сокращение расходов. Когда продукт вышел в свет, разработка не прекращается. Есть изменения стандартов, законов и рынка. Новые требования будут поступать постоянно. , – ,/, – , – , – ,. К чести компаний, я не видел случаев, когда клиента просто начинают доить, завышая цену за CR. Есть какой-то минимум маржи, который спускают сверху вниз, но и всё. Никаких в «в два-три раза», как обычно думают заказчики.,Вот, собственно, и весь секрет профессии. Архитектура, как и сам архитектор, нужны чтоб сэкономить деньги. Очень классно, когда можно избежать ошибок и трат на первых этапах – во время разработки. Но это лишь годика два из всего жизненного цикла. А вот уменьшать затраты на протяжении десятка летка лет – это мечта, обеспечивающая нам ,. И если в первые и критичные этапы вам приходилось спорить с продуктовыми менеджерами, начальниками разработки и кучей директоров ради сокращения времени и усилий, то теперь война будет за увеличение прибыли. Я говорил, что клиентов не дурят? Ну там сложная история. Знаете, как бывает – левая рука не ведает, что делает правая. В начале всё идет как по книге: получают запрос на изменение, продакт и архитектор дают первичный дизайн, отдел разработки дает , и на этом основании выставляют счёт клиенту (с учётом рисков). Но как только всё подписали, появляются управленцы оптимизирующие расходы и ,. Они пытаются «улучшить» решение и увеличить маржу.,Самая полезная штука этого этапа – документация прошлых шагов. Схемы, диаграммы и записи, сделанные во время разработки. В начале разработки вам дали картинку (видение продукта), потом порезали на пазл (все эти эпики и стори), и вы все вместе собрали этот пазл и передали заказчику. Теперь же необходимо будет постоянно дополнять эту головоломку новыми кусочками. Держать всю картину в голове, конечно, круто, но не практично. Так иметь перед глазами карту, пусть и не детальную – сильно упростит вам повседневную работу. В идеале вы даете форму кусочка и палитру, продакт готовит рисунок, а дальше уже девелопер сам. Но если у вас есть хорошо документированный кусок архитектуры, то вашу работу можно делегировать сразу разработчику, оставив на архитектора лишь ревью. ,Идея делегировать решения вам может показаться странной, так как вам всё равно его нужно будет утвердить. Значит ответственность остаётся на ваших плечах. Как будто пашите вы, а зарплату тратит кто-то другая…. Однако на самом деле всё просто – у вас будет мало времени. Код велик как баобаб. И баобаб будет только расти. До начала эксплуатации у вас был один бэклог, хоть там и были периодические изменения и версионность. Теперь же у вас их стало как минимум два. Один, как и прежде – проектальный. Та часть, которая принадлежит клиенту и обслуживает конкретно его нужды и оборудование. Второй – продуктовый. Та часть, которая служит (или будет) платформой для других клиентов. Ванилька. И новые блоки будут достраиваться в оба потока параллельно с частичной миграцией и вымиранием/замещением между ними. И если ваша компания амбициозна, а клиенты крупные (сеть предприятий), то велик шанс, что потоков намного больше. В глобальном мире у вас будет слоёв 6–7, что обычно выливается в 4 и больше независимых стрима (инфра, глобальный бизнес, локальный бизнес, кастомизация клиента). ,Компания А изначально планировала разработку платформы отдельно, а кастомизации отдельно. Год занимались только бекэндом своего будущего продукта, чётко имплементируя все внешние связи и интерфейсы. В это время начали продажи и нашли трёх крупных клиентов. На второй год, уже основываясь на программных контрактах, началась разработка и кастомизаций. Получилось 4 разные команды, 3 из которых основываются на результатах четвёртой. Сырой продукт требовал доработок и рефакторинга, что в свою очередь приводило к изменениям и во всех трёх проектах. Позже выяснилось, что не учли геополитику и одна из трёх команд требовала всё больше правок в платформе и более сложных адаптаций. Платформу «временно» разделили на две. Временность продлилась 4 года. Всё это сказалось на сроках и вместо планированных 3-х лет на всё, получилось почти в два раза больше.,Компания Б получила заказ на разработку с большущим списком требований и малым бюджетом. Компромиссом стало соглашение с клиентом, что большую часть кода можно продавать другим. Так что менеджмент решил, что такое партнёрство даст шанс выйти из ниши производителя оборудования и стать полусофтверной продуктовой компанией. План был пилить год под того самого заказчика, а потом отрефакторить всё это дело и зажить на широкую ногу. Вот всё так и сделали – завернули 90% проекта в красивую обертку и назвали продуктом. Оказалось, что вот этот продукт в таком виде никому не нужен. И каждый потенциальный клиент хочет изменить как минимум половину, а потом еще и добавить свой бизнес. Первая же попытка продать другому клиенту привела к провалу с расходом на год разработки и выплатой неустоек. Понадобилось еще пару лет для перезапуска.,В основе лежат базовые вещи типа логирования, метрик, безопасности, протоколы и всё такое. Следующим слоем ляжет основной дистиллят бизнес-логики (допустим у нас банк). Над ним глобальные вещи, общие для многих территорий, как например язык, системы мер и тому подобное (григорианский календарь, метры, английский, глобальные стандарты). Дальше будет слой местных особенностей, таких как законы (валюта, налоги, местные регуляции). Последние слои уже не платформа, а клиентский проект – кастомизация под основной бизнес, а за ним возможно франшизы и побочные бизнесы. Слоёв может быть больше и разделение немного другим. Допустим в Европе есть общие законы для всего ЕС, но языки разные. Валюта тоже не ложится один к одному, да еще и законы в каждой стране свои. У клиента может быть несколько подвидов бизнеса и разного рода партнеры/дилеры. А так как клиентов у вас несколько, то уровень абстракции платформы должен удовлетворять потребности каждого из них.,Почему нас это волнует только сейчас, а не на этапе первичного проектирования? Строить изначально глобальный продукт очень дорого. Исходя из опыта – года 2-3 чистых убытков на создание платформы и лишь потом первые клиенты. Так как любой клиент будет требовать доработку, интеграции и адаптацию, а потом и поддержку, то лишь часть денег пойдёт в счёт покрытия расходов самой платформы. Которую, кстати, надо будет тоже развивать, чтоб соответствовать меняющемуся рынку. Поэтому менеджмент смотрит в цифры и планирует всего на пару лет вперёд. Ну а архитектор? Архитектор работает с требованиями, а как вы догадались никаких далеко идущих требований ни клиент, ни продуктовый отдел попросту не предъявляли. И если сам архитектор не специалист в предметной области и не имеет подходящего опыта и рефернсов – мы имеем того, ну в общем… Вот и большой сюрприз – после того как вы потратили годы на проектирование и разработку, надо переделывать, перестраивать и ломать.,Без ,гильдия архитекторов не может указать, какие вещи в бизнес-процессах клиента будут меняться, а какие нет. Так что соломки на всё не напасёшься, а куда стелить не ясно. К чему можно и нужно готовиться – к интеграции. Со временем количество внешних сервисов и устройств только растёт. То, что ещё вчера делали вручную – будут делать машины под управлением контроллеров. Ну и постоянно растёт количество датчиков и всяких IOT. Которые даже не связанны с производством/делом. Допустим подключение дверей к умным магнитным замкам или камера распознавания номеров машин, недавно появившаяся на парковке завода. Кому-нибудь точно придёт в голову идея «вот тут потрекать, тут связать данные и вот в такой отчётик». Если у вас есть четкая картина API, то разработчики и сами справятся. Только надо проследить, чтоб решение этой задачи не закрыло будущее для смежных заданий. Не всегда разработчику известно, где и как еще используются необходимые ему сервисы. ,Одну легаси систему целиком запихали в образ и ,. Вполне обычная потребность - знать сколько копий в работе и распределять запросы, требовала сервис для проверки состояния. Документация была и найдена команда вполне квалифицированных людей для этой задачи. Все решили за пол спринта и отправили в прод. Через месяц вдруг заметили, что места система стала жрать немерено. Начали рыть, что изменилось и пришли к этому самому сервису работоспособности. Система была монолитом и возможностей что-то вызвать было не много. Программист решил просто делать логин и проверять ответ. А вот чего он не знал, так что для этой системы сам логин – критичное действие. И это действие требовало занесения в несколько журналов и запускало всякую эвристику. Так как логин одного и того же пользователя без логаута каждые 5 секунд включал кучу алертов и писал еще в десяток журналов, то всё это снежным комом росло и в базе, и в файловом хранилище. Ну и для полного счастья самого юзера нахардкодили не зная о политиках безопасности – через месяца два всё бы встало из-за автоматической блокировки. ,Еще один частый кейс – это попытки интегрировать сторонние продукты. Умелые продавцы расскажут вашему клиенту, как удобно работать с «вот этим новым умным» терминалом/планшетом/мобильным приложением. И у них уже всё готово, вот и REST API с JWT и HTTPS – просто и надёжно, только бери и запускай. Основная проблема в том, что те, кто продают, не понимают ограничений реального производства и существующих процессов. А те, кто покупают, не понимают реальных возможностей продукта и ограничений информационных систем. Основные просчёты: сертификации, безопасность, анонимность, необратимость. В E-com можно отменить заказ, вернуть деньги клиенту и ничего не потерять. А вот приготовленный капучино обратно в машинку не засунешь, так же, как и не сделать перерасчёт ушедшему клиенту оплатившему наличкой. Банки давно сами стали IT, а вот розничная торговля, рестораны, отели, заправки и другие крупные компании редко имеют свой отдел разработки и людей способных оценить трудозатратность и сложность предлагаемых им чудо-решений. Так как применить инновационные гаджеты можно, но для этого необходимо перестроить все операции вокруг них.,Вот примерно так владелец АЗС приобрел терминал для самообслуживания своего кафе-пристройки, чтоб организовать ,. Он знал, что интернета на станции нет, но решил, что поставить точку доступа - не проблема. В чём-то он, конечно, прав. Вот только сертификация интегрированных систем управления и ,не допускала наличия публичной сети. И из-за легковоспламеняющихся материалов, все устройства должны быть с определённым стандартом пожаробезопасности, иначе инспекции и страховщики вам быстро помогут ликвидировать бизнес. Так что, вместо «легкой» интеграции, пришлось выносить терминал за пределы пожароопасного периметра и пилить много чего автономно весящего в облаке с оффлайн синхронизацией (утром вносили меню и цены в терминал, вечером вбивали данные о продажах в основную систему управления). Плюс надо было докупать всякую мелочь типа планшета для кассира, отдельного кухонного принтера, на который шли заказы с терминала и т. д. А через полгода работы все это убрали – не пошла бизнес-идея, так как вместо заказа «по ходу заправки» получилось, что надо проезжать дальше в специальное место самообслуживания и забирать заказ в другой точке. Да и работники кафе не очень любили ручную автоматизацию, которая к тому же слегка обесценила несколько продуктовых метрик. За пару лет пандемии подобных историй было много.,Всё, что происходит в рамках разработки для конкретного заказчика, так же отражается и на вашем основном продукте. Помимо продуктового менеджера и его виденья будущего платформы, используют опыт и фидбек заказчиков. Так иногда платформа привносит инновации в бизнес клиента, а иногда клиенты в платформу. Основным ограничением тут служит периодичность обновлений. Цикл разработки платформы обычно медленный для реалий рынка. Об этом можно судить по особенностям ,и ,. Клиенту же необходима динамика и ,. Это создаёт много шума и излишней работы. Проектальный деплой идёт по желанию клиента, а платформа по плану. И на вопрос «подождать ли полгода-год и получить функицонал за Х денег или получить его за 10Х через месяц?» обычно отвечают «получить через пару месяцев за 5Х цены». Клиент разрабатывает функционал в своём проекте при поддержке разработчиков платформы. Эта фича станет доступной клиенту в короткий срок, а платформа сможет забрать её к себе на следующий релиз. Но получается, что в следующем релизе одна и та же функциональность существует на разных уровнях, что приведет к конфликтам. Значит перед выкатом релиза надо убрать её из проекта, возможно необходимо разработать/отконфигурировать кастомизацию, пересобрать и проверить. Таким образом внесение изменений в слой заказчика может просачиваться в платформу, а обновления платформы требовать изменений у клиента. Вот у нас и появилась двухсторонняя зависимость параллельных стримов, о которой при проектировании платформы скорее всего не думали. Ведь до выхода в эксплуатацию такого не происходит (на стадии разработки это легко решается перестановкой и пополнением в основном бэклоге). Такой вот слоёный и распределённый монолит.  ,О чём еще забыли упомянуть в начале? Конечно, о том, что жизнь – не всегда рост. Да, будут новые модули и новые функции. А еще будет угасание и отмирание старого. Об этом тоже почему-то не думают. Сложность состоит не в том, чтоб удалить участки кода – для этого есть стандартные процедуры: объявление клиента в релиз плане, пометка ,в кодовой базе и через пару релизов выжигаем напалмом. В теории. На практике же - есть куча документов, отчётов и регуляций, которые требуют доступа к данным из мёртвого функционала. Зачастую, чтоб удалить что-то старое, приходится разрабатывать что-то новое. Как бы раньше у вас был «редактор» и его надо заменить «просмоторщиком». Если вам «повезло», то данные могут быть защищены от удаления законом на срок в десяток лет (на моей практике в некоторых странах фискальные данные хранят 15 лет – по сроку устаревания финансовых преступлений).,Не продуманными на уровне бизнеса остаются и процедуры закрытия, замораживания, переноса. Клиент может продать свой побочный бизнес или перевезти его в другой регион. Та же проблема с данными. Нужно сохранить старые и пересчитать новую иерархию, возможно валюту, налоги и так далее. Пандемия тоже подкинула кейсов, когда бизнес не закрывается, но засыпает. Нужно уменьшить операционные расходы, оставив доступность данных. Это проще, когда весь бизнес затих, но в основном попадались требования притушить внешний рынок и сосредоточиться на внутреннем. Необходимостью стала разработка инструментов для автоматизации инфраструктуры. Так чтоб в пару кликов снизить частоту синхронизации, бэкапов, остановить операционные процедуры, подвешенные на таймер (открытие смены, закрытие операционных и финансовых периодов), автоматизация ручных (by design) процессов – выдача разрешений, остановка оборудования и т.д. (в новых реалях никого на месте просто нет). ,При проектировании ритейл системы было обговорено, что это нечто маленькое (для одиночных магазинчиков) и как аксиома – один часовой пояс и одна валюта. Потом был первый «успех» и продажи поскакали по миру. И сюрприз – в некоторых странах принимают больше, чем одну валюту. А некоторые магазинчики перерастают в мини сети. Даже с разными часовыми поясами. К счастью, у меня уже был подобный опыт, поэтому минимум абстракций был задан с самого начала: своя реализация для времени (особенно важно для Now), которая в начале просто обернула системные библиотеки и структура для цены (сумма, валюта), которая исключала какой-то параметр конфигурации или валюту по умолчанию. Без этого переработка всех отчётов и аудиторских записей, стала бы сложной не только в разработке, но и в миграции исторических данных. И нет, менеджмент эти абстракции не одобрил, я просто их ввёл в код напрямую. Спасибо тоже никто не сказал, но пожурили прошлое начальство за недальновидность.,Всё это к тому, что проектировать нужно будет еще много чего и то, что софт пошёл в стадию обслуживания, не значит, что он перестаёт развиваться или, что интересной работы для архитектора не будет. Более того вам предстоит честь представлять и защищать своё детище перед клиентом в реальных продакшн кризисах! Всё что вы слышали и к чему готовились, всё равно случится. В моём личном архиве: сгорел датацентр, повредили подводный кабель, наводнение уничтожило инфраструктуру телеком провайдера и конечно рейд массив умер целиком. Хотите смешного? Это всё может случиться одновременно! ,Клиент тестировал DR сайт (стандартная процедура – они переключались с основного на резервный сервер раз в месяц). В очередной тест, центр с резервным сервером сгорел, а основной админы вырубили, чтоб спровоцировать переход на резерв. Ну и конечно, рассчитывать на бесперебойную работу с одним резервом было бы глупо, вот только на дополнительных двух шёл апгрейд. Много разных команд – дитя без глаза.,Вместо того, чтоб разрешать кризисные ситуации, лучше их предотвращать. Поэтому стоит подумать об инструментах мониторинга. Что-то уже существует на рынке. Можно с легкостью найти инструмент для резервного копирования и автоматического тестирования целостности бэкапов. Вот только проверить согласованность данных можно только зная конкретную систему. Ведь утилиты мониторинга логов тоже надо настраивать под себя. А с данными еще сложней. Желательно иметь вариант быстрой проверки целостности данных – какой-нибудь хэш только последних и только критичных данный. Эту проверку будут гонять часто. И вариант глубокой проверки с выявлением конкретных проблем – то, что админы будут запускать редко, на бэкапах и когда первая проверка выявила ошибку. Неплохо продумать экстренные/дополнительные каналы распределения и обновления данных. Во время разработки, обычно на таком экономят. Потом получается, что у вас сервер работает по принципу ,, а системный юзер заблокирован. Если вы просто обновите юзера в центре, то на локальных серверах это сообщение попадёт в конец очереди. Пользователь блокируется, а сообщение о разблокировки забрать не получится (,). В распределённых системах, особенно после восстановления, сообщения могут приходить в случайном порядке. Часто упускают необходимость постепенного восстановления. Это особенно критично в коммуникациях. Если после долго оффлайна клиентский сервер выходит на связь, то отдав ему все данные за раз можно заблокировать его работу на долго. А если таких серверов много, то и загрузить центр. Вообще большой объём данных – как прорыв плотины - те, кто выживут будут плавать в грязи.,Через год после запуска у клиента начались проблемы с производительностью. Бутылочным горлышком стала централизованная очередь сообщений (есть много всяких , реализаций, и я хочу избежать product placement). Проблема переросла в кризис после инцидента с подтормаживаниями сервера даже после перезагрузки. Причина тормозов выяснилась не сразу и заключалась она в том, что очередь хранила сообщения вечно (by design), а клиент добавил в систему виртуальных получателей, которые физически не забирали сообщения. Очередь росла и после перезагрузки сервер поднимал всё то же безумное количество сообщений, что и приводило к проседаниям отзывчивости и пропускной способности. Нельзя терять данные, но никто не подумал, что делать если количество данных превышает способность сервера их обработать. Тут стоит упомянуть, что я только недавно получил повышения до должности архитектора в связи с тем, что прошлый уволился. Для меня это был первый опыт кризиса, но для клиента нет. Отношения были напряженны со всех 4 сторон: моя компания, IT отдел клиента, подрядчик интегратор и консалтинг, представлявший клиента в качестве аудитора. После первого инцидента и до выяснения причины пройдет 2 дня и работоспособность еще не восстановлена. В первые же часы мне выдали билет на самолет и встретил меня на той стороне не коллега, а адвокат. Адвокат нашей конторы. Он провёл инструктаж (с кем и о чём можно говорить без него), ознакомил с ситуацией и выдал задание. Пока я был в воздухе, архитектор клиента при поддержке консалтинга выдвинул следующий тезис: «выбранный нами продукт для сервера очереди является причиной низкой продуктивности и нам следует сменить его, оплатить изменения, развёртку и неустойку из своего кармана». Это еще не был иск, но было уже передано через адвокатов и включало сумму, сроки и список «правильных» очередей. От меня требовалось обосновать «наш выбор» хоть как-то.  С прошлым архитектором мне поработать не довелось – он был на релокейшине на сайте клиента и не сильно часто общался с разработчиками. Я лично узнал его имя лишь когда сообщили об его уходе. И пока команда копалась с сервером и выясняла причины ситуации, я сидел в поте лица и выяснял всё, что мог про все популярные очереди. К полуночи я составил табличку из десятка технологий и функциональных особенностей проекта. А потом, смотря на характеристики этих очередей в версии доступной в начале разработки и на данный момент, ставил крестики там, где возможности не соответствовали требованиям. И когда к утру в этой таблице я получил крестики во всех строках (включая и новые версии этих продуктов), кроме того бренда с которым работали мы – я понял, что мой предшественник работал на совесть. На этом, правда, приключения не закончились. Когда прояснились причины, решение стало очевидным – надо очистить очередь. Вот только клиент боялся одобрить необратимое действие и требовал найти другой подход. Через давление нашего адвоката сместили директора их IT отдела, который возглавлял сопротивление (за нарушение политик эксплуатации продукта - не должно было быть не работающих адресатов), а операцию по фильтрации и удалению сообщений произвела консалтинговая компания по нашим инструкциям.,Ну и, наверное, главное, что можно сказать об этом этапе – смена кадров. Люди будут сменяться и у вас, и у заказчика. Так как количество разработки уменьшается лишь ближе к концу жизни, а критичность и сложность изменений растёт, то в команде всегда будут спецы и новобранцы. Мидл практически исчезает как класс. В роли архитектора, при правильной передаче знаний и работы, у вас могут появиться протеже. Официально или нет, но эта стадия идеально подходит для новичков. Есть кому обучить, есть уже готовое решение, есть много легких задач (те, что вписываются в текущую парадигму), но и сложные не исчезают (те, что с архитектурой не согласуются). Я вот так и попал на должность. Как опытный разработчик я стал техлидом и отвечал за дизайн простых частей пазла. Со временем на меня стали кидать больше задач по проектированию новых модулей и исследования критических багов (обычно сложная часть – понять, где и в чём проблема). Когда встал вопрос кем заткнуть дыру, образовавшуюся после ухода текущего архитектора, то выдвинули меня. Правда не официально – меня просто пихали на все совещания как «ведущего технического специалиста». Получить звание без личной беседы с менеджментом (при этом пришлось перепрыгнуть через прямого начальника) не получилось. ,На фоне вялой текучки персонала и того, что вы уже не одну собаку съели в этом бизнес домейне - пора дойти до осознания, что архитектура — это не только в код, но и в бизнес. Обычно продуктовым менеджерам нужна помощь в определении четких метрики поведения пользователей и использования системы. Ваши знания связей оболочки и внутренностей дают фору в определении быстрых и дешевых решений. Какие ошибки указывают на проблемы системы, а какие на неумение или не желание пользователей с ней работать. Какие превентивные меры можно реализовать, дабы защитить систему от недоброкачественного использования. Что можно проверить прототипированием, а что не опасно вынести в А/Б тестирование. Тут, кстати, целое непаханое поле для сбора и обработки информации с помощью машинного обучения. Просто бюджет и ресурсы на эту, хоть и хайповую тему, вам не выделят. Но если вы понимаете, что ищет клиент и как не напугать расходами, то сможете продать идею и своим и клиенту. Если, конечно, и вам и разработчикам хочется запустить свою нейросеть.  ,А таки зачем это вам? Когда проект оживает, архитектура тоже не может оставаться статичной. Эволюционирует ли она или будет просто гнить и разваливаться – вот критерий, который определяет - хорошо ли вы работали на первых этапах и насколько хорошо усвоили уроки прошлых «успехов». Но определяет успех архитектуры и лично ваш, не устойчивость и не гибкость. А то, насколько хорошо об этом знают и используют! И никто, кроме вас, не знает систему лучше и не заинтересован в максимальной утилизации возможностей. Я уверен, что есть тысячи уникальных решений и сложных систем, о которых мы даже и не слышали. И очень жаль. С другой стороны, тех кого пиарят и приглашают на всякие вебинары, забивают медийные потоки однообразной информацией («как и почему» в десятке топовых самодостаточных компаний). Очень интересно послушать про технологии и решения каких-нибудь поисковиков или социальных сетей. Но в мире материальных продуктов вам не нужен еще один поисковик или соц.сеть. И конкурировать с каким-нибудь , или , сервисом в энтерпрайзе тоже вряд ли захотят (по экономическим причинам). Получается, что есть куча данных по проектированию систем и инструментов, которые вам не придётся разрабатывать. И минимум доступного опыта/советов, которые пригодятся в реальной работе., , ,Вы находитесь здесь,Enterprise Architect",Успех на каждый день / Хабр
[<200 https://habr.com/ru/post/662936/>],page2,"Из ,: культовая демо-сцена Sponza получила обновление, Wolfire  Games выложила в открытый доступ исходный код Overgrowth, официально  запустился SketchUp для iPad, Steam возобновил выплаты для разработчиков  из России, вышел Mudbox 2023.,Из интересностей: как создавалась игра Among Us, диздок игры Crankin  для Playdate от Кейты Такахаши, про дизайн окружения и уровней Dying  Light 2.,Впервые выпущенная CryTek ещё в 2010 году, Sponza долгое время  использовался для демонстрации графических возможностей игровых движков.,Спустя более десяти лет Sponza ,.  Новая сцена включает в себя физически корректные материалы с  4K-текстурами и геометрией высокого разрешения, фотограмметрически  соответствующие реальному Атриуму в Дубровнике, Хорватия.,Исходники приключенческого 3D-экшна выпущены под лицензией Apache 2.0. В ,  нету ассетов, поэтому, если хотите запустить игру, вам всё равно  необходимо приобрести копию. Overgrowth работает на Windows, Mac и  Linux.,В рамках процесса «улучшения магазина приложений».,Подобные , весьма неприятны для обычных одиночных игр, которым не нужны обновления или для которых уже не выпустить обновления.,Период бета-тестирования , и теперь приложение получило официальный релиз.,Понадобится ,  о зарубежном банке-посреднике. Естественно, не получится перевести на  счета банков, которые попали под санкции, но лучше так, чем ничего.,Что интересно, разработчикам из Беларуси выплаты по-прежнему не перечисляются.,Начиная с ,  Radeon Memory Visualizer (RMV) обеспечивает поддержку Smart Access  Memory (SAM). SAM — это функция AMD, которая может повысить  производительность графически интенсивных приложений, предоставляя ЦП  прямой доступ ко всей доступной видеопамяти.,В новой версии , непрерывность отслеживания, взаимодействие между руками и распознавание жестов.,В обновлении , поддержка Rocky Linux и новый установщик под Linux.,Разработчики , на рекламную поддержку со стороны издательства и на 67% от выручки с продаж.,Можно получить бесплатно ,.,Скачать можно с , разработчика.,Кейта Такахаши выложил ,,  по которому можно оценить творческий процесс. Если интересно узнать как  создатель довольно популярной в своё время Katamari размышляет при  работе над проектами, почитайте.,Ещё можно почитать , с разработчиком.,Разработчики рассказали о первой версии игры, о том, как она стала популярной и что студия собирается делать в будущем.,Подробнее в ,.,Некоторые начинающие художники скачивают кисти в надежде, что они помогут создать шедевр. Но не в кистях дело. В , рассказывается, с чего нужно начинать, если вы действительно хотите добиться успеха в цифровом рисунке.,Ведущий концепт-художник по окружению Кася Зелиньска ,,  как создавался мир Dying Light 2, поведала о создании окружения,  поддерживающего игровой процесс, и объяснила, как были спроектированы  различные области карты.,Мобин Фикри, разработчик Hyper Meteor, ,, как они создали аркадный шутер для Playdate.,Набор UI Toolkit, доступный в последней версии Unity, поддерживает  создание пользовательского интерфейса в рантайме с помощью знакомого и  интуитивно понятного процесса разработки, вдохновлённого  веб-технологиями. В этой , рассматриваются особенности его возможностей и делятся советами по началу работы с UI Toolkit в качестве альтернативы UGUI.,Автор на пошаговых примерах , с правилами построения разных видов перспектив. В конце статьи — краткий гайд в карточках.,Yumi’s Cells использует человеческие эмоции и то, как они  трансформируются в поведение, исследуя чудеса человеческого мозга.  Команда Epic Games ,  с художниками, создавшими сериал, чтобы узнать, как они подошли к  задаче изобразить человеческое мышление таким уникальным способом.,Клара Кокс ,  о работе над проектом Mountain Side Living, поделилась рабочим  процессом создания ассетов и дала несколько советов начинающим  3D-художникам.,В , подкаста Джеймс Тан из Digital Confectioners , о том, чем цепляет проект, его неожиданном успехе в Китае и пути к 1 миллиону проданных единиц.,Работа в локализации заставляет кричать. Иногда от счастья, иногда от ужаса. Менеджер проектов Allcorrect Валерий Тимченко , впечатлениями от работы и рассказал о самых важных её аспектах.,В ,  рассказывается о реализации deferred texturing. Техника разработана для  снижения стоимости рендеринга растительности. Представлен подробный  взгляд на различные этапы реализации, рассмотрено то, как интегрируется в  пайплайн рендеринга Decima.,Дополнительно показано, как реализовать решение с переменной скоростью на оборудовании без нативной поддержки.,В , представлено краткое изложение различных способов выражения поворотов с интерактивной визуализацией методов.,Показано поведение линейной интерполяции, рассмотрены ограничения методов.,Из ,.,Найдено в ,.,В , автор разбиваем меши внутри Houdini и использует сгенерированное в Niagara для анимации частей на GPU. ,Полностью ,  с помощью геометрических нод и Cycles. Никаких ключевых кадров, все  процедурно. 7 отдельных networks/modifiers, которые взаимодействуют друг  с другом.,Backend (node.js/ts), немного gamedev (unity)","Недельный геймдев: #67 — 24 апреля, 2022 / Хабр"
[<200 https://habr.com/ru/company/hh/blog/662920/>],page2,"Мы постоянно работаем над улучшением нашего сервиса. И стараемся делать так, чтобы дата-сайентистам не предлагали вакансии сантехников. С каждым годом мы все ближе к космическому идеалу. В 2020 году мы получили патент на изобретение автоматизированного поиска релевантных резюме и вакансий для рекомендательной системы ,Умный поиск hh.ru, с использованием машинного обучения и понижением размерности многомерных данных. ,Компонент Умного поиска hh.ru был изобретен нашими профи по анализу данных и машинному обучению – Георгием Даньщиным, Виктором Реушкиным и Александром Сидоровым. Наше изобретение позволяет за сотни миллисекунд выбирать из миллионов резюме и сотен тысяч вакансий правильные, и соотносить друг с другом те, у которых похож не только текст, но и смысл. Наша фича проделывает это куда лучше и эффективнее, чем подобранные вручную правила.  ,Благодаря этому улучшается качество рекомендаций резюме и вакансий и требуется меньше вычислительных ресурсов – мы экономим память и процессоры серверов. ,Чтобы рекомендовать вакансии в последние два года мы применяем более сложные ML-модели с большим количеством признаков. Запатентованные изобретения позволяют ощутимо уменьшать объем данных, сохраняя существенную часть их смысла. Затем эти данные используются для очень быстрого предварительного отбора вакансий и резюме, а затем модели выбирают наиболее подходящие. ,Из больших полноразмерных данных мы получаем сжатые. Однако они должны сохранять определенный смысл, чтобы без потери качества и скорости обработки мы могли рекомендовать подходящие резюме для вакансий и наоборот. Наше изобретение используется и для обработки запросов работодателей при поиске по резюме, и для обработки запросов соискателей при поиске вакансий. ,Благодаря hh.ru сотни тысяч работодателей и соискателей находят друг друга в России и за её пределами. Чтобы это происходило быстрее и оптимальнее, наши разработчики используют прикладной искусственный интеллект. Наши решения сильно отличаются от применяемых, например, в веб-поиске. Всё из-за очень большого разнообразия и специфики предметной области наёма и работы, и очень высоких требований к качеству выдачи, которую видят пользователи.  ,В 2018 году нам потребовалось сделать рекомендации резюме для вакансий, с учётом географического местоположения и интересов. На тот момент в нашей базе уже было больше 40 млн резюме и 600 тыс. вакансий. Посчитать вероятность, что каждое резюме пригласят на каждую вакансию с помощью сложных математических моделей, подобранных посредством машинного обучения – невозможно. Тем более онлайн, за 50 мс, пока пользователь ждёт отрисовки веб-страницы или экранного мобильного приложения. ,Для решения этой проблемы можно попытаться придумать эвристики и классификаторы резюме и вакансий. Например, оцифровать резюме и вакансию, представить их в виде векторов чисел. Затем снизить размерность этих векторов, представив их в виде хешей. Использовать для вычисления этих хешей нужно такую функцию, которая делала бы их похожими с точки зрения последовательностей отдельных битов информации, если работодатель позвал кандидата на собеседование. Дальше нужно сложить эти хеши в поля базы, и когда приходит работодатель с вакансией – делать запрос «выдать все резюме, хеш которых отличается не более чем на N бит». И уже получив несколько десятков тысяч резюме, применять к ним более сложные и ресурсоёмкие модели. Технические подробности – в ,, который подготавливали для участия в конкурсе Юридический департамент – Юрий Донников и Дарья Першенкова. ,В результате мы смогли запустить рекомендации резюме на вакансии. С тех пор их использование выросло настолько, что работодатели приглашают большинство соискателей не благодаря поиску по базе, а из резюме, которые рекомендуются на их вакансии. Это позволяет тратить на поиски подходящих кандидатов примерно в 6 раз меньше времени, а значит делает наем и поиск работы проще и быстрее. ,Мы решили запатентовать это решение не только для его защиты, но и чтобы поделиться им с вами. Мы верим, что подобный подход может пригодиться и в других компаниях, которые применяют искусственный интеллект на практике, чтобы стать удобнее, полезнее и эффективнее для своих клиентов. Особенно это касается многосторонних маркетплейсов. Это сделает жизнь людей немного лучше. ,Мы очень рады и горды, что Российское патентное ведомство удостоило нашу разработку в сфере искусственного интеллекта такого внимания и признания. Надеемся, это изобретение поможет и вам. ,Пользователь",Успешное изобретение для Умного поиска hh.ru / Хабр
[<200 https://habr.com/ru/post/662678/>],page2,"Сегодня мы разберем хитроумный и нетривиальный алгоритм поиска подстроки в строке. Он основан не на сравнении символов, а на сравнении чисел. Я уже писал, что основная моя цель это не написать простой разбор алгоритмов, а посмотреть их эффективность, какие-то интересные места и сравнить их производительность между собой.,
И сегодня есть что посмотреть.,
,Вообще в классической реализации используется полиномиальный хеш, но подойдет в общем-то любая кольцевая хеш-функция. Но мы возьмем полиномиальную версию.,Алгоритм крайне простой: берем хеш паттерна, берем хеш куска текста равного по длине, сравниваем их. Если равны, то проверяем их посимвольно, в противном случае — идем дальше.,Все очень просто.,У нас есть двоичный алфавит, где ,.,Вычислим хеш паттерна:,Вычислим хеш подстроки ,.,Они не совпадают, значит двигаем паттерн на символ правее.,Вычислим хеш подстроки ,.,Хеши подстроки и паттерна снова не совпадают, значит двигаем паттерн еще на символ правее.,Вычислим хеш подстроки ,.,Хеши совпали, значит проверяем этот участок посимвольно.,Чтобы сопоставить две строки, мы по факту превращаем их в числа простым переводом в произвольную систему счисления, где позиция символа это разряд, его код это значение разряда, а мощность алфавита это количество символов в кодировке (вообще это не обязательно, за мощность алфавита можно брать любое число больше единицы, но тогда коллизии будут встречаться чаще. Чем больше число, тем лучше, но главное не упереться в переполнение,).,В общем виде расчет хеша будет выглядеть примерно вот так:, — хеш паттерна,
, — некоторое натуральное число, например мощность алфавита.,Теперь давайте посчитаем хеш реальной строки. Возьмем строку , и для наглядности за , возьмем мощность семибитной таблицы ASCII — ,.,Результат: ,Выглядит многовато. Чтобы записать такое число в двоичном виде понадобится 49 разрядов, а у нас из исходных данных всего лишь паттерн длинной 7 символов и урезанная кодировка ASCII. Если мы возьмем восьмибитную версию, то будет уже переполнение даже на 64-х битных системах. Так что решить этот вопрос нам помогут , и ,.,Ее задача помочь нам избавиться от возведения в степень, так как возведение в степень относительно прожорливая арифметическая операция. Кроме того, без схемы Горнера алгоритм хеширования станет менее оптимальным. Так как у нас будет большое количество дублирующихся вычислений.,Да и никакая модульная арифметика нам не поможет, если у нас паттерн длиной 128 ASCII-символов. В таком случае, при обычном возведении в степень, у нас для хеширования первого символа в паттерне будет использоваться число ,, а это, как вы понимаете, много.,Для того чтобы избежать переполнения, но при этом получать корректные результаты хеширования мы для всех промежуточных результатов вычислений будем брать остаток от деления.,Таким образом, наш конечный алгоритм должен выглядеть примерно вот так:,На всех промежуточных этапах мы получим значения не превышающие,
,.,Но тут есть один нюанс:,Если начать забуриваться в изыскания на тему коллизий в кольцевых хеш-функциях, то натуральным образом можно оттуда не выбраться. Если кратко, то число , должно быть большим и простым. А если поподробнее то вот:,У нас есть текст , длиной ,. Если мы возьмем ,, где ,, то вероятность коллизии будет менее, чем ,Если пробежаться по первой тысяче простых чисел и построить график ,, где , это количество коллизий, то будет вот:,Здесь я провел проверку на отрывке про дуб из Войны и мира, где искал строку ,. Да паттерн не очень большой, числа тоже не самые огромные, но зато мы можем посмотреть динамику количества коллизий. И как это не тривиально — мы получили обычную асимптоту. Ну и как мы можем заметить после 7000 самое худшее, что мы получаем это 1 коллизию. Конечно не факт, что дальше не будет какого-нибудь всплеска. Но начиная с 2000 самое худшее, что у нас было это 4 коллизии на текст 1671 символа.,Все логично, чем больше простое число, тем меньше мы скукоживаем хеш, тем меньше коллизий. Ведь когда мы берем диапазон чисел ,, (а примерно такой хеш мы можем получить без модульной арифметики), и пытаемся его ужать в диапазон ,, то очевидно, что мы должны за это чем-то заплатить. И цена этому коллизии.,Конкурсы все те же:,Замеры, где и текст и паттерн состоят только из буквы ""а"".,Текст: строка длинной 1024 символа,
Образец: строка длинной 32 символа,Алгоритм Рабина-Карпа по сложности в худшем случае имеет сложность ,, по времени работы ничем сильно не выделяется, но при этом делает больше сравнений. Ведь перед тем как сверять строку мы сначала должны сверить хеш. Так что здесь наш подопытный аутсайдер.,Строка состоящая из 1024 символов алфавита [TGAC]. ,GTAGTGTGTCTACGTCTTTCTTTGACAGTACCGCGTAATTTGCAGTGCTATAAATCATCTCTAACGCTGGCTGTGCACCGCCACCGTAGTGTGTCTACGTCTTTCTTTGACAGTACCGCGTAATTTGCAGTGCTATAAATCATCTCTAACGCTGGCTGTGCACCGCCACCCCAGCGGGAAGCCCATTTTTCCACTACCTCTGTTCCTGGTATAGTGCACTATATCGCCCGTAACCGATGTCTCGCCAAGATTTTGGCAACTTCCCGAGCAATCAGGTGGAGTCAGACCGATAGCTCTAATGGTTTACGTGAATGCATGGCGCCTATAGCTATGGGCAGAAACCAGCGGGAAGCCCATTTTTCCACTACCTCTGTTCCTGGTATAGTGCACTATATCGCCCGGTAGTGTGTCTACGTCTTTCTTTGACAGTACCGCGTAATTTGCAGTGCTATAAATCATCTCTAACGCTGGCTGTGCACCGCCACCCCAGCGGGAAGCCCATTTTTCCACTACCTCTGTTCCTGGTATAGTGCACTATATCGCCCGTAACCGATGTCTCGCCAAGATTTTGGCAACTTCCCGAGCAATCAGGTGGAGTCAGACCGATAGCTCTAATGGTTTACGTGAATGCATGGCGCCTATAGCTATGGGCAGAAATAACCGATGTCTCGCCAAGATTTTGGCAACGTAGTGTGTCTACGTCTTTCTTTGACAGTACCGCGTAATTTGCAGTGCTATAAATCATCTCTAACGCTGGCTGTGCACCGCCACCCCAGCGGGAAGCCCATTTTTCCACTACCTCTGTTCCTGGTATAGTGCACTATATCGCCCGTAACCGATGTCTCGCCAAGATTTTGGCAACTTCCCGAGCAATCAGGTGGAGTCAGACCGATAGCTCTAATGGTTTACGTGAATGCATGGCGCCTATAGCTATGGGCAGAAATTCCCGAGCAATCAGGTGGAGTCAGACCGATAGCTCTAATGGTTTACGTGAATGCATGGCGCCTATAGCTATGGGCAGAAA,Образец: ,. (37 символов),getSubstringNaive x 6,349,
getSubstringKMP x 156,780,
getSubstringNotSoNaive x 189,694,
getSubstringBMBadCharacter x 199,476,
getSubstringRK x 229,361,Несмотря на то, что алгоритм Рабина-Карпа на втором месте, по количеству сравнений, он опережает Бойера-Мура. Ну в общем-то БМ с эвристикой плохого символа не так эффективен на сгенерированных строках.,Текст:,На краю дороги стоял дуб. Вероятно, в десять раз старше берез, составлявших лес, он был в десять раз толще, и в два раза выше каждой березы. Это был огромный, в два обхвата дуб, с обломанными, давно, видно, суками и с обломанной корой, заросшей старыми болячками. С огромными своими неуклюже, несимметрично растопыренными корявыми руками и пальцами, он старым, сердитым и презрительным уродом стоял между улыбающимися березами. Только он один не хотел подчиняться обаянию весны и не хотел видеть ни весны, ни солнца.,«Весна, и любовь, и счастие! — как будто говорил этот дуб. — И как не надоест вам все один и тот же глупый бессмысленный обман! Все одно и то же, и все обман! Нет ни весны, ни солнца, ни счастья. Вон смотрите, сидят задавленные мертвые ели, всегда одинакие, и вон и я растопырил свои обломанные, ободранные пальцы, где ни выросли они — из спины, из боков. Как выросли — так и стою, и не верю вашим надеждам и обманам» .,Князь Андрей несколько раз оглянулся на этот дуб, проезжая по лесу, как будто он чего-то ждал от него. Цветы и трава были и под дубом, но он все так же, хмурясь, неподвижно, уродливо и упорно, стоял посреди их.,«Да, он прав, тысячу раз прав этот дуб, — думал князь Андрей, — пускай другие, молодые, вновь поддаются на этот обман, а мы знаем жизнь, — наша жизнь кончена! » Целый новый ряд мыслей безнадежных, но грустно-приятных в связи с этим дубом возник в душе князя Андрея. Во время этого путешествия он как будто вновь обдумал всю свою жизнь и пришел к тому же прежнему, успокоительному и безнадежному, заключению, что ему начинать ничего было не надо, что он должен доживать свою жизнь, не делая зла, не тревожась и ничего не желая.,Паттерны:,На реальном тексте алгоритм держит планку по ,, его производительность примерно как у КМП алгоритма и ± такое же количество сравнений.,Это далеко не самый шустрый алгоритм чтения, но у него есть одна крутая фишка: он может в неточный поиск. Так как он не делает сравнения символов напрямую, то он может шустро искать в потоковом режиме примеры вхождений определенного паттерна. Значит мы можем сделать систему, которая будет удалять все знаки препинания, переводить текст в нижний регистр и искать вхождение абзаца в текст. Да, если поизголяться, то это можно сделать и без РК алгоритма, но тогда это выйдет не так оптимально по времени и памяти.,Разработчик",Строковые алгоритмы на практике. Часть 3 — Алгоритм Рабина — Карпа / Хабр
[<200 https://habr.com/ru/company/kaspersky/blog/662850/>],page2,Пользователь,Security Week 2217: расшифрованный шифровальщик / Хабр
[<200 https://habr.com/ru/company/timeweb/blog/662740/>],page2,Пользователь,«Чернобыль». 23 года эпидемии Win.CIH / Хабр
[<200 https://habr.com/ru/company/auriga/blog/662958/>],page2,Пользователь,GraphQL и почти все-все-все. Часть 1:) / Хабр
[<200 https://habr.com/ru/company/otus/blog/662970/>],page2,"В MongoDB мы стремимся предоставить разработчикам возможность внедрять инновации в работу с данными. Временные ряды — самая быстрорастущая рабочая нагрузка с интенсивным использованием данных. Наши нативные возможности при работе с временными рядами позволяют быстрее создавать приложения и получать больше инсайтов из данных этих рядов с меньшей когнитивной нагрузкой.,Обычно в данных временных рядов бывают пропуски, например, когда IoT-датчик отключается от сети. Но для проведения анализа и получения правильных результатов данные временных рядов должны быть непрерывными. Вы также можете захотеть создать гистограммы или скоррелировать датасеты, чтобы обеспечить более сложную оперативную аналитику в контексте разработки приложений. ,, теперь доступное в ,, в сочетании с уплотнением, которое мы представили в MongoDB 5.1, помогает лучше обрабатывать недостающие данные, чтобы легко создавать и вытаскивать на поверхность ценные инсайты.,Два новых этапа агрегирования создают простой, оптимизированный способ работы с отсутствующими данными во временных рядах и регулярных коллекциях, обеспечивая аналитику для любого сценария использования. Этап $densify создает новые документы для устранения пробелов во временной или числовой области на требуемом уровне детализации, а $fill устанавливает значения для полей, когда величина равна нулю или отсутствует. Заполнение отсутствующих значений может быть выполнено с помощью константы или с использованием линейной интерполяции: путем переноса последнего наблюдения или возврата от следующего.,Эта функция предоставляет почасовое отображение метрик для каждого складского помещения. Если данные о температуре отсутствуют, они должны быть интерполированы линейно, движение по умолчанию должно равняться 0, а количество запасов должно быть перенесено из последней известной точки.,Выходные данные: Второй документ был создан на основе двух соседних документов.,Ранее такие сложные виды аналитики были возможны только в специализированных системах, таких как специальные базы данных временных рядов или хранилища данных. С точки зрения архитектуры, специалисты-технологи должны были найти беспроигрышный компромисс между нишевой (узкоспециальной), часто незрелой технологией, предназначенной исключительно для рабочих нагрузок временных рядов и не связанной с системами учета, содержащими полный набор корпоративных данных, и экспортом данных временных рядов в хранилища, затрудняя тем самым практическую операционализацию инсайтов. Оба подхода предполагают управление многочисленными хранилищами данных и хрупкими ETL-пайплайнами, что увеличивает сложность и стоимость. Обходные пути для этих подходов часто предполагают создание разработчиками сложных дата-пайплайнов для заполнения пробелов, в том числе на уровне приложений, что приводит к низкой производительности запросов или ограничивает аналитику небольшими датасетами. С помощью MongoDB 5.3 разработчики могут создавать насыщенную аналитику на основе данных временных рядов в полете и предоставлять оперативную информацию своим пользователям в рамках работы приложения.,MongoDB 5.3 доступна уже сейчас. Если вы используете серверные инстансы Atlas Serverless или выбрали получение Rapid Releases в выделенном кластере Atlas, то ваш деплоймент будет автоматически обновлен до версии 5.3 начиная с сегодняшнего дня. MongoDB 5.3 также доступна в виде релиза для разработчиков только для ознакомительных целей в Центре загрузки MongoDB. В соответствии с ,, объявленным в прошлом году, функциональность, доступная в версии 5.3 и последующих Rapid Releases, будет перенесена в MongoDB 6.0, наш следующий основной релиз, запланированный к выпуску в конце этого года.,Более подробное объяснение заполнения пробелов для временных рядов можно найти в нашей ,.,Скоро в OTUS состоится открытое занятие «Варианты установки MongoDB». На бесплатном вебинаре мы:,развернем MongoDB различными способами;,обсудим возможности и ограничения каждого способа;,выберем наиболее оптимальный способ в зависимости от задачи.,Регистрация для всех желающих доступна ,Пользователь",Представление заполнения пробелов для данных временных рядов в MongoDB 5.3 / Хабр
[<200 https://habr.com/ru/company/avito/blog/653417/>],page2,"Когда ковид ещё не стукнул, и мы все работали из офиса, жизнь была проста и безмятежна. Все были на виду, сидели рядом, всегда могли быстро переговорить, узнать прогресс или что-то напомнить друг другу. Командные активности были офлайн, легко было собраться у доски или пойти вместе в переговорку, никого не забыв. На командном телевизоре крутилась статистика спринта, метрики продукта, результаты OKR-ов.,С приходом ковида мы перешли на постоянную удалёнку, и жизнь стала другой. Она стала сложнее. Причём не только для менеджера, которому надо было перестроить всю свою работу с командой, но и для самих ребят. Появилось очень много разной рутины:,Постоянно собирать прогресс по разным доскам.,Напоминать о стендапах, задачах и PR-ах.,Собирать всех в Зуме на разные встречи.,Процессы и работа в целом начали терять прозрачность. Стало понятно, что надо с этим что-то делать.,С приходом удалёнки, Slack стал чуть ли не единственным местом помимо Зума, где мы постоянно общались. Нам пришла идея написать бота, который будет регулярно собирать различную статистику и выкладывать её в общий чат команды.,Сказано — сделано. За выходные был состряпан простенький бот, который был гордо назван Sheldon в честь персонажа «Теории большого взрыва» и награждён соответствующей аватаркой.,На тот момент Шелдон умел лишь ежедневно присылать уведомление в чат с сообщением, содержащим список того:,какие задачи подвисли;,какие PR-ы сейчас открыты и под какие задачи;,какие critical-баги присутствуют в бэклоге — у нас есть регламент исправления подобных задач.,Прелесть была также в том, что каждое утро в таком сообщении был тег автора или того, кому надо обратить на него внимание. Так нужные люди получали нотификации, а я избавился от рутинной необходимости напоминать самому.,Позже в сообщение Шелдона мы добавили информацию о тех, кто в ближайшее время уходит в отпуск и с какого числа, или тех, кто уже отдыхает и когда вернётся. На этот момент его пост в чате выглядел так:,Шелдон берёт информацию из Jira, Stash и нашего внутреннего портала Avito People.,Эксперимент удался, и мы начали развивать Шелдона. Дальше он начал присылать раз в итерацию общий прогресс целям и статистику по задачам. Для этого пришлось научить его генерировать прогресс-бар и графики.,К сожалению, Slack не умеет отображать много графики. Надо было найти хоть какой-то вариант, который бы помог отобразить прогресс по OKR. Идея пришла ,: отображать прогресс-бар в виде 10 символов ⬜ и 🟦, где первый означает сам прогресс-бар, а второй — его выполнение. ,Таким образом, если у нас эпик на OKR, в котором 100 задач, и 20 из них выполнено, рисуем последовательность 🟦 🟦 ⬜ ⬜ ⬜ ⬜ ⬜ ⬜ ⬜ ⬜ (20/100). Подправленные под наш юзкейс исходники можно ,.,Зная, какие эпики у нас в квартал отвечают за OKR, можно легко узнать и вывести прогресс по ним в процентах выполненных работ. Знаю, это не то, как считается прогресс по целям в реальном мире, но таким был единственный автоматизированный вариант, который получилось сделать на тот момент.,Смотреть постоянно в кумулятивную диаграмму, высчитывая статистику, — довольно муторная штука. К тому же хотелось дать доступ к таким данным всей команде. Поэтому было решено добавить графики по новым/выполненным задачам и багам в сообщение бота.,С этим было сложнее: это уже графика, а единственный более-менее рабочий вариант прикрепить картинку к сообщению бота — ссылка на файл, доступный по http. Нормального варианта складывать картинку на персистентный сторадж на тот момент у нас не было, и пришлось выкручиваться.,По итогу был сделан http-хендлер, который генерирует график в png по массиву точек, полученных в get-запросе. Саму ссылку генерируем прямо в момент отправки сообщения после сбора этих самых точек из статистики. Таким образом мы избавляемся от стораджа и можем генерировать разные виды графиков в рантайме. Единственное ограничение — количеством точек, которые мы можем передать, ведь get лимитирован 2048 символами. Исходники можно ,По итогу получилось сообщение, которое приходило каждый понедельник:,Просто показывать статус и отправлять напоминалки о зависших задачах — это хорошо, но что если добавить сразу кнопку на то, чтобы их закрыть? Сказано — сделано. Шелдон научился сам закрывать PR, в которых прошли тесты и есть апрув, а также таски, в которых все PR смерджены.,Постепенно Шелдон стал выходить наружу. Его добавили в общий чат поддержки продуктов команды, чтобы систематизировать запросы. Бот научился менять дежурных и звать всех заинтересованных на наши демо.,А ещё Шелдон:,Автоматизирует процесс дежурств. Он назначает дежурного, напоминает, что нужно ответить на вопрос в канале и напоминает, когда ты дежуришь.,Напоминает закрыть дела перед отпуском.,Напоминает команде о демо, чтобы не забыть подготовиться.,Постепенно о боте узнали в других командах, и из pet-project он стал массовым. Пришлось его рефакторить. Код был разделен на модули, модули на блоки, был добавлен простой механизм сетапа. В итоге, после массированной рекламы в общих каналах, его подключили себе ещё 10 команд. И Шелдон стал полноценным членом семьи.,А какие боты используются у вас?,Пользователь",Sheldon: бот для автоматизации командной рутины / Хабр
[<200 https://habr.com/ru/post/662976/>],page2,"Если мы не используем EF (такое случается), то нам нужно как-то устроить загрузку объектов из базы данных. Вариант: берём ,, делаем ему ,, а из него берём данные для строительства нужных объектов. При этом класс, который умеет заполнять ,, не знает, для объектов какого класса он это делает. Абстракция, низкая связанность, всё хорошо. ,Однако, мы ждём, пока заполнится ,, только после этого можем начать отправку ответа клиенту.,Ранее в публикации  , мы рассмотрели способ, как отправлять данные по мере поступления с помощью , (например, ,). Там был приведён пример:,И так делать, конечно, некрасиво. Потому что в таком случае у нас класс, который умеет работать с базой данных зависит от интерфейса , и класса ,, относящихся к бизнес-модели.,Можно попробовать получить абстрактный DbDataReader и крутить его уже на более высоком уровне:,Оказывается, это не работает:,Это из-за ,При выходе из метода соединение закрывается, так как вызывается ,. Придётся , убрать:,Теперь котики выводятся, но у нас висит незакрытое соединение, что также некрасиво.,И тут нас посещает идея: а давайте возвращать тот же ,, но не целиком, а на каждой строке данных через ,! Это означает, что мы останемся внутри метода GetCatsDataReader() до конца данных, соединение закроется правильно и наша первоначальная цель также будет достигнута. Итак:,Проверено, работает!,Пользователь",Инверсия зависимости и System.Data.Common.DbDataReader / Хабр
[<200 https://habr.com/ru/company/yandex/blog/662826/>],page2,Frontend Team/tech lead,"Гайд по написанию и рефакторингу компонент, которые хочется переиспользовать / Хабр"
[<200 https://habr.com/ru/company/lanit/blog/662720/>],page2,"Российский рекламно-аналитический рынок меняется с невероятной скоростью, и эти изменения довольно серьезные. На них нужно как-то реагировать и крупным компаниям-производителям товаров и услуг, и рекламным агентствам. Привычные инструменты аналитики перестают быть доступными. Существует ли альтернатива, например, для сбора событий в условиях недоступности сервисов Google? Рассказываем про нашу разработку  CleverDATA Tag Manager, которая является частью ,.,Несколько слов, собственно, о CDP. Это платформа управления клиентскими данными, которая помогает компаниями собирать, анализировать, обогащать и сегментировать свою аудиторию для дальнейшей аналитики и/или персонализации маркетинговых сообщений. CDP CleverDATA  интегрируется с необходимыми клиенту источниками онлайн- и офлайн-данных и целевыми каналами коммуникаций.  ,Теперь перейдем к модулю, которому посвящена статья. Основная задача, решаемая Tag Manager, – это сбор любых типов событий с сайта в привязке к конкретному пользователю.,Также Tag Manager помогает реализовывать маркетинговые активности на сайте, не зависимо от загруженности команды веб-разработчиков, позволяет быстрее проводить эксперименты, добавлять отслеживание новых показателей и даже проводить подстройку контента на сайте под пользователя, хотя это и не является основным сценарием использования Tag Manager.,При помощи CleverDATA Tag Manager можно собирать такие события, как нажатие на кнопку,посещение раздела сайта или определенных страниц, заполнение форм, время сессии, процент просмотра страницы, можно отправлять события по факту выхода курсора за границы окна или отслеживать активность вкладки. В целом в CDP можно собирать любые события, которые можно отследить через JavaScript на сайте в браузере. Это позволяет реализовать ,.,CleverDATA Tag Manager (сокращенно CTM) использует хорошо известную веб-аналитикам сущность, называемую маркетинговый тэг. Тэги — это небольшие фрагменты кода, как правило, на языке JavaScript, которые выполняются после загрузки страницы, собирают информацию о посетителях сайта для их дальнейшего анализа, забирая данные из html-кода страниц (например, значение конкретного поля в онлайн-форме) и из браузеров (например, cookies), и отправляют данные в CDP.,А сам CleverDATA Tag Manager — инструмент, который позволяет легко управлять тегами, размещенными на сайте, через свой интерфейс, ,. Т.е. на сайте необходимо один раз добавить небольшой кусочек кода, так называемый сниппет, а все последующие изменения настроек по сбору событий можно делать в интерфейсе CleverDATA Tag Manager.,С помощью СTM администратор может указать для каждого тэга, какие атрибуты пользователя необходимо передать и после какого события. Например, если нужно отследить, что пользователь нажал на кнопку «Свяжитесь со мной», то по нему можно передать набор cookies пользователя и факт нажатия данной кнопки. ,В CleverDATA Tag Manager есть два типа тэгов: стандартный тип для передачи данных о событиях в CDP и тип, позволяющий внедрять в код сайта дополнительную логику, написанную на JavaScript (например, вызов сниппета Яндекс.Метрики (см. пример ниже) или пикселя VK). Во втором случае можно использовать Tag Manager как «оркестратор» для отправки данных в различные сторонние системы. Ниже в статье разобран пример настройки отправки событий в Яндекс.Метрику и дальнейшей проливки данных в ClickHouse.,В целом, возможности CleverDATA Tag Manager аналогичны возможностям широко известного Google Tag Manager.,Но есть и различия – т.к. наше решение является частью CDP CleverDATA, то ,, пользующейся CDP, и их можно будет хранить в собственном хранилище и использовать как угодно. CDP сохранит данные, собранные тэгом, в атрибутах пользователя, и по этим данным можно потом: ,строить , внутри CDP, ,настраивать по данным сегментам , на различных рекламных площадках (к которым уже есть готовые коннекторы), ,выгружать данные во внешние системы для дальнейшего ,.,При корректном применении CTM , загрузки страниц сайта.,CleverDATA CDP развернута в Яндекс.Облаке, также доступна возможность развертывания решения в выделенном контуре в облаке.,А теперь посмотрим, как можно при помощи CleverDATA Tag Manager решить проблему ,Задача: на основе поведения пользователя передать события в Я.Метрику, построить дашборды в DataLens и передать визиты в ClickHouse. Будем считать, что сниппет CTM уже размещен на сайте в разделе <head> веб-страницы (требования к размещению такие же, как к размещению сниппета GTM).,В первую очередь надо разместить счетчик Яндекс.Метрики на сайте через CTM. Для этого в CTM создаём тег с типом Custom HTML и в него помещаем сниппет. Активацию тега включаем на событие загрузки страницы. Как выглядит тег со сниппетом, показали выше, ниже приведен пример настройки триггера. Тип события, на которое происходит срабатывание, в данном случае – «Загрузка страницы». Можно дополнительно ограничить срабатывание различными условиями, например, определённый URL или – для события клика – название элемента, на который был выполнен клик. ,Можно еще добавить метку, чтобы было легче ориентироваться, когда будут сотни триггеров. После сохранения тега необходимо опубликовать изменения в контейнере, чтобы новый тег начал срабатывать на сайте. ,Но одного счётчика не достаточно, надо настроить активацию целей Яндекс.Метрики. Для этого опять создаем тег Custom HTML, но вставляем в него код цели для сайта, созданный в кабинете Яндекс.Метрики (JavaScript-событие). ,Сам тег настраивается вот так:,И триггер на клик по кнопке «GetStarted» с классом «btn_start». ,Далее всё сохраняем и публикуем контейнер. Теперь можно пойти на сайт и посмотреть, что событие клика срабатывает и выполняется передача данных в Я.Метрику. В консоли разработчика переходим на вкладку Network, в фильтре набираем yandex, чтобы остались только вызовы сервисов Яндекс. Среди этих вызовов смотрим, где есть goal, и убеждаемся, что есть идентификатор нашей цели – get_started.,Спустя какое-то время в кабинете Яндекс.Метрики можем увидеть наше достижение цели. ,Но когда у нас много целей, то не очень удобно делать дашборды под каждую цель. Тут нам поможет DataLens. Создаем собственный чарт, перетягиваем «Измерения» и «Показатели» – «Достигнутая цель» и тут же видим результат:,Можем вывести в виде диаграммы или таблицы и добавить на дашборд: ,Подробнее можно почитать тут: ,.,Рассмотрим, как решить схожую, но немного иную задачу: активировать событие для Яндекс.Метрики, но теперь не по клику или действию пользователя, а по решению сайта. К такому подходу приходится прибегать, когда нужно отследить что-то более сложное, чем просмотр страницы или клик на кнопку, например, трекать факт просмотра страницы для Single Page Applications (SPA) с бесконечным скроллом.,Для этого надо отправить активацию события на сайте в DataLayer (DL) CTM. Синтаксис DataLayer СTM аналогичен синтаксису DL в Google Tag Manager. Код для активации триггера: ,.,И сам триггер для активации тега:,Выбираем тип «Пользовательское событие» и в поле вставляем ключ события, в нашем случае «backend_event» (для примера). Создаём тег, в котором будет активироваться цель, и сохраняем его. После публикации контейнера можем проверить на сайте, как срабатывает триггер.,Чтобы проверить корректность настройки, в консоли выполняем код , и убеждаемся, что произошёл вызов Яндекс.Метрики. ,Данные из Яндекс.Метрики можно перенести в ClickHouse. Непосредственно в этой задаче CTM не поможет, но вот ,.,Также, если брать CleverDATA Tag Manager в составе CDP  CleverDATA, то события с сайта, объединенные с событиями из других источников, можно заливать в ClickHouse напрямую из CDP.,Стоит добавить, что в составе CDP Tag Manager позволяет решать много других интересных задач, например, запрашивать по ID пользователя данные из внешних систем (например, из той же CDP) и на основе ответа выполнить вызов пикселей площадок. Если будет интересно, пишите в комментариях, и мы расскажем про такой кейс в следующей статье.,Мы показали, как решить пару несложных задач с помощью CTM. Эти же задачи можно решить с помощью Google Tag Manager, но многие компании сейчас готовятся к возможному отключению этого сервиса. Мы как компания изначально не ставили перед собой задачу заменить GTM.  Модуль CTM создавался как must-have часть CDP, но сейчас мы сделали его доступным и в виде самостоятельного модуля. Будем рады, если наш продукт поможет компаниям не потерять знания о своей аудитории. ,Статья подготовлена в соавторстве с ,CPO в CleverDATA",CleverDATA Tag Manager – есть ли жизнь без Google? / Хабр
[<200 https://habr.com/ru/post/662986/>],page2,"Доброго времени суток Дамы и Господа! Многим приходилось сталкиваться с необходимостью анализа большого количества данных при помощи Python по запросам начальства или коллег. Однотипные запросы поступают с определенной периодичностью, и не составляет труда подставить новые данные в свой код и провести анализ. Но иногда из-за определенной нагрузки не всегда хочется заниматься таким анализом. Намного проще сделать скрипт с графическим интерфейсом, чтобы сам заказчик для анализа данных мог нажать пару кнопок и получить желаемый результат. Тем более, можно изначально вложить в интерфейс столько «хотелок» заказчика для анализа, сколько будет душе угодно.,Покажу вам, как достичь желаемого на примере библиотеки для Python ,., разработчиков размещены примеры по функционалу библиотеки, а также простые скрипты.,Пример скрипта вывода окна для ознакомления с функционалом библиотеки:,Код понятен, легко можно адаптировать под свои задачи и вкусы. Мы можем менять в всплывающих окнах как цветовую схему окна, шрифта, так и цвет выделения текста (заднего фона текста). Поменяем настройки предыдущего окна:,И фраза «давай поиграем со шрифтами и цветом» звучит не так страшно.,Библиотека постоянно обновляется, так, в версии 4.6. количество тем было значительно увеличено и теперь их более 100.,Достаточно добавить в код ,, чтобы выбрать наиболее подходящую тему для задачи. Чтобы увидеть тему и список доступных тем библиотеки нужно вызвать функцию ,Библиотека позволяет работать с большим количеством работающих окон, которые работают независимо с несколькими вкладками в каждом окне и счетчиками прогресса, которые обновляются одновременно.,У библиотеки достаточно широкие возможности, вплоть до использования в написании игр (шахматы и пинг-понг описаны на сайте разработчиков). ,Перейдем к простой визуализации кода для анализа данных дата-сета.,Для анализа данных нам на помощь приходят библиотеки ,, воспользуемся , для анализа данных от разработчиков. ,Для примера возьмем данные из файла sp500.csv в котором находится моментальный снимок индекса S&P500. В первой строке файла содержатся имена столбцов, а остальные 500 строк содержат информацию о пятистам крупнейших компаний США.  ,После запуска скрипта работа происходит только в оконном режиме. ,Открываем необходимый нам файл для анализа.,После загрузки файла поочередно будут открываться окна согласно скрипту с вопросами для уточнения по данным и необходимому анализу.,При ответе на вопрос «Данный файл содержит имена столбцов?» если указать «No» то согласно прописанному скрипту столбцам будут присвоены имена по типу 'Столбец0', 'Столбец1', и т.д. Далее выбираем необходимые параметры по имеющемуся датасету.,При просмотре загруженных данных мы видим данные в виде таблицы по нашему файлу.,Библиотека позволяет работать как с загруженными в код алгоритмами расчета статистики, так и сделать диалоговое окно с выбором нужных расчётов.,Окно со статистикой включает расчет метрик, прописанных нами в скрипте, в примере включены такие данные как квартили, средняя, минимум, количество и медиану. ,Статистика в примере представлена по всем данным датасета. ,Выбор анализа можно представить по-разному:,- в виде выпадающего списка:,- обычного списка:,Библиотека также позволяет комбинировать различные элементы.,Вариантов визуализации достаточно, чтобы угодить самым искушенным аналитикам, можно вставить анимированные GIF изображения (с помощью , ) или просто картинки.,После закрытия окна всплывает следующий вопрос – Хотим ли мы увидеть график по нашим данным.,Ну и ответ на данный вопрос вполне очевиден. ,При визуализации сразу видно наиболее прибыльные акции. График строится на основе загруженной библиотеки Matplotlib.pyplot. Для визуализации можно построить любой график, который заложен в данной библиотеке, так же можно использовать любую другую.,Не во всех организациях приветствуется использование и создание exe-файлов, но если разрешено, то это намного проще для конечного пользователя, чем работать с файлом скрипта и «тетрадкой».,Для создания exe-файла для нашего скрипта анализа, нам нужно установить PyInstaller или cx_freeze и использовать инструмент PySimpleGUI EXE Maker, его можно найти на GitHub в учетной записи PySimpleGUI. Это простой интерфейс для pyinstaller,  , с инструкцией по работы с инструментом.,В итоге у нас получится файл, который заказчик анализа может запускать самостоятельно и производить свой анализ без отвлечения вас от других задач. PySimpleGUI одинаково хорошо работает на Mac, Linux и Windows, также разработчики библиотеки отвечают на вопросы и проблемы на GitHub.,Успехов в решении задач!,Пользователь, Профессиональное сообщество","Сделай то, сделай это, сделай сам / Хабр"
[<200 https://habr.com/ru/company/angarasecurity/blog/661341/>],page2,"Здравствуйте, хабролюди!,Меня зовут ,, и я работаю в отделе анализа защищенности компании Angara Security. Отвечаю я, значится, за инфраструктурный пентест, и в этой статье я хотел бы поговорить об одном из самых эффективных методов добычи учетных данных на «внутряке» — извлечении секретов из памяти процесса lsass.exe (MITRE ATT&CK ,) — и, в частности, об особенностях реализации этого метода в ру-сегменте тестирования на проникновение.,За два года работы пентестером мои нервы были изрядно потрепаны нашим любимым отечественным антивирусным решением Kaspersky Endpoint Security (далее — KES), который установлен у каждого , второго нашего клиента, и который, в отличие от других средств антивирусной защиты, наглухо блокирует все попытки потенциального злоумышленника получить доступ к lsass.exe (не реклама!).,Далее я расскажу свой опыт использования и кастомизации публично доступных инструментов, которые в разные промежутки времени позволяли мне сдампить память LSASS при активном «Касперском». Погнали!,Если не сильно углубляться в теорию, то Local Security Authority Subsystem Service (он же LSASS) — это процесс (исполняемый файл ,), ответственный за управление разными подсистемами аутентификации ОС Windows. Среди его задач: проверка «кред» локальных и доменных аккаунтов в ходе различных сценариев запроса доступа к системе, генерация токенов безопасности для активных сессий пользователей, работа с провайдерами поддержки безопасности (Security Support Provider, SSP) и др.,Для нас, как для этичных хакеров, ключевым значением обладает тот факт, что в домене Active Directory правит концепция единого входа Single Sign-On (SSO), благодаря которой процесс lsass.exe хранит в себе разные материалы аутентфикации залогиненных пользователей, например, NT-хеши и билеты Kerberos, чтобы «пользаку» не приходилось печатать свой паролЪ в вылезающем на экране окошке каждые 5 минут. В «лучшие» времена из LSASS можно было потащить , в силу активности протокола WDigest (HTTP дайджест-аутентификация), но начиная с версии ОС Windows Server 2008 R2 вендор решил не включать этот механизм по умолчанию.,Несмотря на то, что в 2к22 при успешном дампе LSASS злоумышленнику чаще всего остается довольствоваться NT-хешами и билетами Kerberos, это все равно с большой вероятностью позволит ему повысить свои привилегии в доменной среде AD за короткий промежуток времени. Реализуя схемы ,, , и ,, злоумышленник может быстро распространиться по сети горизонтально, собирая по пути все больше хешей и «тикетов», что в конечном итоге дарует ему «ключи от Королевства» в виде данных аутентификации администратора домена.,Рассмотрим первопроходцев в ремесле извлечения данных аутентификации из памяти LSASS.,Было бы преступлением не начать повествование с такого мастодонта в области потрошения подсистем аутентификации Windows как ,, которым хоть раз пользовался любой пентестер.,Модуль , позволяет «налету» парсить память lsass.exe с целью поиска секретиков без сохранения соответствующего дампа на диск. Этот инструмент поистине произвел революцию в наступательных операциях и положил начало многим другим исследованием в области извлечения чувствительной информации с хостов под управлением Windows.,К сожалению для пентестеров, вендоры , / ,быстро «просекли фишку» и стали относиться к «Мимику» ,как к самому опасному ПО, созданному за всю историю человечества,, поэтому на сегодняшний момент он пригоден лишь как пособие для изучения реализованных в нем техник — для их переосмысления и переизобретения в собственных инструментах.,На заметку: официальная , Mimikatz покрывает далеко не все его возможности, поэтому энтузиасты InfoSec-комьюнити создали вот , замечательный ресурс, которым я рекомендую пользоваться в случае возникновения вопросов, что делает та или иная команда этого замечательного инструмента.,Другим фаворитом внутренних пентестов долгое время был метод создания снимка памяти LSASS с помощью служебной программы , из состава ,. Этот инструмент позволяет создавать дампы процессов с целью их дальнейшего анализа, и процесс lsass.exe тому не исключение (если права позволяют, разумеется, хе-хе).,Теперь можно притащить слепленный дамп к себе на тачку и распарсить его с помощью того же Mimikatz.,Или его аналога для Linux – ,.,Прелесть этого метода заключается в том, что все необходимые операции по созданию слепка памяти выполняет ProcDump, подписанный Microsoft, и этичному взломщику не требуется тащить на хост никакой малвари. Однако разработчики корпоративных антивирусных решений тоже долго не стояли в стороне и оперативно прикрыли возможность делать дампы LSASS с помощью ProcDump, включив его в разряд ,.,Безусловно, интересной находкой стало обнаружение экспорта функции , в системной библиотеке ,, которая дергает вызов Win32 API , и позволяет делать слепки процессов в рамках концепции , (LOLBAS), когда злоумышленнику не нужно приносить ничего лишнего на атакуемую машину.,Эта библиотека легла в основу первых версий замечательной утилиты ,, позволяющей делать слепки LSASS и удаленно читать необходимые области памяти созданного дампа, а не перенаправлять его целиком на машину атакующего (подробнее о принципе работы можно почитать , автора утилиты).,Если взглянуть ,, можно найти суперские «однострочники» для Cmd и PowerShell, которые автоматически позволяют получить идентификатор процесса lsass.exe и сдампить его память по заданному пути.,Примечание: лучше пользоваться PowerShell-версией команды, так как для оболочки PowerShell в отличии от Cmd по дефолту включена привилегия , для привилегированной сессии шелла, которая понадобится для доступа к памяти lsass.exe.,Стоит ли говорить, что создание дампа по такой простой технике, разумеется, будет предотвращено хотя бы мало-мальски неравнодушным антивирусом?,Еще один древний как мир способ — позаимствовать , функции MiniDumpWriteDump из класса , сборки ,, как это делается в скрипте , из арсенала PowerSploit.,Результат работы скрипта аналогичен вызову функции MiniDump из предыдущего метода, поэтому оставлю это в качестве упражнения для читателя. Ну и, соответственно, антивирусы так же негативно к нему относятся.,Итак, перейдем к самому интересному: как же можно «угодить» антивирусным средствам защиты и сделать дамп памяти процесса lsass.exe в стиле Operational Security?,Запреты AV на создание слепков памяти LSASS условно можно разделить на 3 части:,Запрет на получение дескриптора процесса lsass.exe.,Запрет на чтение виртуальной памяти процесса lsass.exe.,Запрет на сохранение результирующего дампа на диск.,Ниже мы рассмотрим 3 проекта, каждый из которых в свое время помогал мне извлечь чувствительную информацию из памяти сетевых узлов при активном средстве KES на внутренних пентестах или операциях Red Team.,Первым обнаруженным мною проектом, который на удивление мог обходить защиту KES, был , от исследователя ,.,Его ключевые особенности:,Написан на C#, что позволяет запускать его из памяти сессии , или с помощью механизма .NET ,.,Применяет магию , и плагина , для генерации «на лету» псевдопровайдера аутентификации LSA SSP и его загрузки в память LSASS для получения дескриптора процесса lsass.exe вместо использования API ,Использует проекты , и , для установки userland-хуков на вызовы внутренних API , для перенаправления потока байт результирующего слепка памяти lsass.exe в память исполняющего процесса. Таким образом у оператора появляется возможность отправить дамп памяти по сети и не сохранять его на диск скомпрометированного хоста.,В минусы этого способа безусловно входит то, что библиотека DLL псевдопровайдера аутентификации LSA , быть сохранена на диск скомпрометированного хоста для возможности ее использования в API ,, и которая, ко всему прочему, не может быть удалена после создания дампа без перезагрузки ПК.,Данный проект существует как Proof-of-Concept, который «из коробки» в конечном итоге все равно сохраняет дамп памяти на диск даже с учетом того, что генерация такого дампа проходит столь необычным образом. Поэтому я решил сделать ,, добавив две новые фичи:,Парсинг слепка прямо в памяти с помощью библиотеки , (работает не на всех версиях ОС Windows).,Возможность сжатия и отправки байт слепка памяти по TCP-каналу на машину атакующего, где парсинг может быть произведен силами сторонних инструментов (Mimikatz / Pypykatz).,Для первой фичи был добавлен , ,, при наличии которого байты слепка передаются на , MiniDump.,Для второй фичи был написан вспомогательный , на Python, содержащий тривиальный сокет-сервер, ожидающий «зиппованный» дамп. Скрипт также автоматически распакует прилетевший дамп, по желанию проверит контрольную сумму и распрасит его с помощью Pypykatz.,Отправка запакованного дампа также легко реализуется на нативном C# через метод ,.,Также метод создания слепков lsass.exe с помощью MirrorDump , мной для использования вместе с lsassy.,К сожалению, недолго музыка играла и примерно полгода спустя «Касперский» начал блокировать создание дампов LSASS через данную технику на уровне поведенческого анализа, что заставило нас искать другой «непалящийся» способ извлечения кред на внутряках.,Нашим следующим «спасителем» стал инструмент , от компании-разработчика Cobalt Strike, который я без преувеличений считаю просто произведением искусства.,Его ключевые особенности:,Использование системных вызовов (с их динамическим резолвом) с помощью ,, что позволяет обходить userland-хуки Win32 API, которые вешает антивирусное ПО.,Собственная реализация MiniDumpWriteDump через чтение памяти lsass.exe с помощью ZwReadVirtualMemory, что избавляет оператора от необходимости дергать потенциально подозрительную ручку API.,Поддержка разных трюков и техник создания дампа (перечислены не все):,поиск уже открытых дескрипторов lsass.exe в других процессах [,],,использование утекающего хэндла lsass.exe при вызове функции , [,],,загрузка NanoDump в виртуальную память lsass.exe в виде провайдера SSP [,],,возможность снятия защиты PPL [,].,Намеренное повреждение сигнатуры дампа памяти с целью избегания детекта от AV на этапе его записи на диск.,Компиляция в Beacon Object File (BOF) для выполнения NanoDump из памяти в случае, когда моделируемый злоумышленник обладает сессией «Кобальта» на скомпрометированном сетевом узле.,Для нас, как для пентестеров компаний преимущественно из ру-сегмента, наибольший интерес представляет техника загрузки NanoDump, скомпилированного в виде DLL, прямо в LSASS как SSP, то есть в виде псевдопровайдера аутентификации LSA. Исходя из нашего опыта, на данный момент это и есть слабое место «Касперского».,Для того, чтобы воспользоваться этой техникой без сессии Cobalt Strike, моделируемый злоумышленник должен принести на скомпрометированный узел 2 бинаря: загрузчик библиотеки SSP и, собственно, саму библиотеку SSP. Полагаю, что в скором времени оба они начнут детектиться по крайней мере на уровне сигнатурного анализа, поэтому воспользовавшись примером , от , мы напилили свой загрузчик NanoDump SSP из памяти с помощью кредла на PowerShell.,Намеренно не раскрываю исходник кредла (тем более, что в приведенной выше статье все есть), ибо надеюсь, что этот метод проживет хотя бы еще немного. Ну а в общем, смиренно ждем, когда и эта техника начнет «палиться» KES, чтобы начать искать новые ухищрения для дампа памяти LSASS...,Последним творением, которое мы сегодня рассмотрим, будет проект , от F-Secure LABS. Его подход к дампу LSASS отличается от остальных тем, что вместо того, чтобы сосредотачиваться на методах уклонения от хуков AV / EDR в userland, он использует , WinPmem (часть форензик-проекта ,) для получения доступа ко всей физической памяти целевого узла и ищет там область, соответствующую памяти процесса lsass.exe, через монтирование виртуальной ФС ,.,Покажем в действии, как заставить это чудо работать:,Для начала клонируем репозиторий проекта, рекурсивно разрешая зависимости в виде git-подмодулей.,Далее исправим версии библиотек , и , в зависимостях ,, чтобы они дружили с актуальным Python 3.,Теперь можно запустить инсталлер, который накатит питонячую виртуальную среду и поставит все, что ему нужно.,Следуя рекомендациям ,, я скачал крайний , WinPmem (нам понадобится только файл ,) и обновил , для изменившегося , взаимодействия с драйвером. Внесенные , можно посмотреть в моем форке проекта.,Также среди внесенных изменений — захардкоженный файл драйвера, который автоматически кладется в файловую систему «жертвы» перед установкой соответствующей службы и стирается после ее остановки и удаления:,Смотрим, как всем этим пользоваться:,Чтобы не упускать преимуществ C#, на котором написана серверная часть, продемонстрируем возможность загрузки и выполнения сборки из памяти.,Вуаля, хеши из LSASS получены!,Вместо заключения приведу несколько рекомендаций, которые помогут свести к минимуму возможности для потенциального злоумышленника сдампить LSASS или извлечь из сделанного слепка значительную выгоду:,Свести к минимуму доступ к любым сетевым узлам в домене с учетными данными пользователей, входящих в привилегированные доменные группы (Domain Admins, Enterprise Admins, Administrators и др.), а для администрирования серверов и рабочих станций использовать выделенные для данных целей УЗ с минимально необходимым набором привилегий (смотрим концепцию ,).,Настроить механизм безопасности , для предотвращения сохранения аутентификационных данных пользователей при подключении к удаленным сетевым узлам по протоколу RDP для привилегированных УЗ.,Использовать механизм , (PPL) для предотвращения потенциальной возможности доступа к памяти процесса lsass.exe.,Использовать группу безопасности Windows «,» (Protected Users Security Group) и добавить в нее УЗ критически важных пользователей, например, администраторов домена (эта фича требует тестирования перед внедрением в прод, поэтому аккуратнее).,Следовать , производителя ОС для снижения риска проведения атак типа Pass-the-Hash.,Ну а пока извечная игра в кошки-мышки между пентестерами и вендорами антивирусного ПО продолжается, Happy hacking!,OSCP|OSEP|CRTO :: Pentest / Red Team","Дампы LSASS для всех, даром, и пусть никто не уйдет обиженный / Хабр"

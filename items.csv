link,next,text,title
[<200 https://habr.com/ru/post/662608/>],page2,"Я так давно пользуюсь услугами Github, что уже начал забывать, как это страшно потерять код, который целый день сочинял и отлаживал. Раньше для сохранения кода я использовал дискетки, потом cd-rom и переносной жесткий диск, потом пришли флешки. Все это для того, чтобы перенести код с рабочего компьютера на домашний и не потерять. И все эти,устройства постоянно ломались, терялись, у них заканчивался срок службы и т.п. ,Потом я завел свои ""облака"" и хранил код на своем железе и рабочих компьютерах. И наконец появился Github. По началу что-то ещё дублировалось на своих серверах и внешних дисках, но к сегодняшнему дню я настолько привык к сервису Github, все настолько удобно и надёжно, что страх того, что ""дискетка"" может сломаться, постепенно улетучился.,И тут на тебе! Оказывается, в любой момент, по не зависящим от меня причинам, меня могут отключить от этого технологического чуда! ,И ладно бы, если б я ходил на работу и писал неизвестно что, неизвестно зачем. Но у меня-то код весь свой, и кроме него ничего нет. Оказывается, что за время существования Github там у меня завелись сотни репозиториев и просто так - вручную - их не скопируешь.,Тем более, что работаем мы сегодня не на одном компьютере, и понять, где что лежит, сразу невозможно - все лежит в Github-е.,Короче. Слепил я программку для быстрого копирования всех своих репозиториев из Github, из всех своих пользователей и организаций, к которым у меня есть доступ. Программа на Go, потому что я последнее время использую только Go (ну, не считая vue и javascript для webapp).,Программа использует 'gh' (github-cli) и 'git'. С помощью gh получаем список репозиториев, а с помощью git клонируем репозиторий со всей историей коммитов, со всеми ветками и тегами. Это все можно сделать из командной строки, но если у вас много репозиториев, то легче с помощью программы.,Вот эти команды:,Как уже говорилось выше, gh выдает список репозиториев, а git клонирует репозиторий со всеми потрохами.,Перед использование программы в gh нужно залогиниться, т.е. выполнить комманду 'gh login' и убедиться, что у вас настроен доступ к Github по ключу ssh.,Далее, полученные архивы можно перенести на любой git-хостинг простыми командами:,Программа имеет параметры:,Пример запуска программы:,В этом случае программа будет искать репозитории у пользователя myuser и в организации myorg и загрузит только репозиторий github-backup пользователя myuser, как указано в пареметре -limit, если этот параметр убрать, то будут загружены все репозитории пользователя myuser и организации myorg. Если права для 'gh' и 'git' позволяют вам пользоваться приватными репозиториями, то будут загружены публичные и приватные репозитории, если не позволяют, то только публичные.,Программа размещена в привычном для меня ,, доступ пока еще есть:,Всё же надеюсь, что доступ к Github у нас останется, но, оказывается, в этом мире бэкап нужен всегда! ,Вот ещё один адресс на ,, пока он запасной: ,С уважением,,Kirill Scherba,Программист (вечный)",Быстрый бэкап всех ваших репозиториев Github / Хабр
[<200 https://habr.com/ru/company/severstal/blog/662271/>],page2,"Цифровая металлургия: результаты хакатона от «Северстали»,С 22 февраля по 20 марта «Северсталь» при поддержке Russian Hackers провела онлайн-хакатон для аналитиков и разработчиков в сфере поиска и анализа данных с общим призовым фондом 700 000 рублей. Хакатон прошел отлично и даже превысил ожидания. Самое время подвести итоги.,На хакатон мы пригласили профессиональных разработчиков и аналитиков и предложили участникам четыре актуальных бизнес-задачи: , Провести исследование данных по затратам на энергоресурсы за несколько лет, проанализировать возможности обогащения внешними данными, определить значимые факторы влияния, построить прогнозную модель с пошаговым разъяснением принципов построения., Разработать инструмент, помогающий специалисту планировать цену на закупку запасных частей к спец. технике., Разработать алгоритм парсинга товаров, исследовать данные во внешних источниках, разработать программы подготовки информации и требуемом разрезе., Провести исследование имеющихся данных и попробовать спрогнозировать просрочку по контрагенту, предложить, как обогатить модель иными данными. ,На участие в хакатоне было подано свыше 1000 заявок. Более 60 команд из 40 городов России загрузили решения на отборе. ,После рассмотрения и оценки загруженных решений наше жюри отобрало 27 команд для участия в финальном туре. Ну и наконец, в финале победило в треках и номинациях 5 команд. Их решения представляем ниже.,Состав команды:,•	,, капитан, специалист по EDA и DS;,•	,, дополнительные и внешние данные;,•	,, презентация / видео.,Команде помогала Анна Баранова, которой отдельная благодарность от Oops I did it again. Особенность именно этой команды в том, что все участники собрались вместе прямо перед хакатоном, причем для большинства из них это был первый опыт совместной работы в рамках соревнования. ,Описание решения команды: Задача хакатона заключалась в прогнозировании транзакций за оплату электроэнергии по нескольким объектам «Северстали» на наборе временных рядов. Ряды нерегулярные и ресемплинг делать нельзя, т. к. бизнес-смысл всей задачи от этого поменяется. Решили композицией из двух моделей. Первая предсказывает даты совершения платежей, а вторая — величину транзакции.,Оказалось, что в каждом ряду присутствуют свои закономерности совершения платежей по дням недели/месяца. Мы сделали реиндексирование датасета по дням, чтобы получить метки дней, когда совершались транзакции. Затем на них обучили модель предсказания дня транзакций по рядам. Также мы смогли значительно увеличить точность за счет того, что мы трансформировали таргет в кумуляту по месяцам и предсказывали именно кумуляту. Затем путем дифференцирования кумуляты-прогноза возвращались к прогнозу, который нужен бизнес-заказчику, а именно казначейству «Северстали».,Приз участникам команды — 300 тысяч рублей и мерч от организаторов.,Отзыв команды о хакатоне: «Нам очень понравилось, что было несколько треков, плюс достаточно длительный отборочный период для генерации и проверки гипотез. Спасибо организаторам за помощь, оперативную связь и предельную четкость условий. Спасибо команде! Здорово, что нам всем повезло встретиться и сделать такую крутую работу». ,Состав команды:,•	Анастасия Батхина, PhD, академический директор в НИУ ВШЭ, основатель и СЕО международного технологического стартапа InMind, Нью-Йорк;,•	Виталий Мальцев, Data Scientist.,Описание решения команды: Мы скачали данные биржевых котировок, затем провели анализ возможности их использования в качестве признаков модели. Разделили товары на 3 группы, сгенерили большое количество данных на котировках и построили модели для части товаров. Сделали пример пользовательского меню. Важный момент — в нашем примере предсказание цены только на сегодняшний день. Для генерации признаков на другие дни просто не хватило времени.,Приз участникам команды — 100 тысяч рублей и мерч от организаторов.,Отзыв команды о хакатоне: «Понравился хакатон тем, что было сразу несколько интересных и нетривиальных задач, у каждой из которых мы видели сразу несколько альтернативных решений. Благодаря этому хакатон получился аналитическим, нетипичным соревнованием, где все блендят решения ради тысячной доли метрики на лидерборде». ,Состав команды:,Александр Сергеев, капитан; ,Алексей Шкиль, бэкенд;,Никита Гетьман, фронтенд;,Дмитрий Меренков, CJE;,Евгений Васильев, Data Scientist.,Описание решения команды: мы фактически с нуля разработали комплексное решение для автоматизации поиска и скоринга поставщиков на надежность. Решение состоит из 7 модулей, каждый из которых обладает своими метриками качества и может развиваться отдельно.,В рамках решения разработаны:,алгоритм парсера,адаптивный механизм парсинга на основе ML модели классификации, позволяющий определять, какой тип URL парсится — портал/маркетплейс или индивидуальный сайт поставщика.,предложена ML модель определения типа поставщика (прямой/ посредник),предложена ML модель скоринга поставщиков,фронт,разработаны схемы процессов + прочая информация.,Приз участникам команды — 100 тысяч рублей и мерч от организаторов.,Отзыв команды о хакатоне: «Наша команда взяла для решения задачу поиска поставщиков. Ключевая идея — разработать алгоритм, который по номенклатуре автоматически найдет поставщиков необходимых товаров. Важно, чтобы это была не просто табличка, но и дополнительная информация о поставщиках. Например, индекс надёжности. Причем алгоритм должен был отсортировать поставщиков по этому индексу. Это позволит автоматизировать процесс и оптимизировать время работы человека за компьютером». ,Состав команды:,Делёв Александр, Data Scientist;,Захаров Алексей, Data Scientist;,Юрченко Петр, Data Scientist.,Описание решения команды: мы решали задачу «Анализ контрагентов», в которой было необходимо построить модель предсказания возможности задержки платежа в случае работы с контрагентом по постоплате. Бизнес-цель — повышение точности прогнозирования просрочек (для снижения рисков компании) и возможное увеличение объема портфеля на условиях постоплаты. Мы построили 2 модели: для контрагентов, с которыми компания «Северсталь» еще не работала и, соответственно, никакой истории взаимодействия с ними нет, и для контрагентов, с которыми сотрудничали хотя бы 1 год, и в этом случае была возможность добавить в модель важные признаки по работе с контрагентом в прошлом. В обоих случаях использовались модели градиентного бустинга, для которых подбирались гиперпараметры с помощью библиотеки hyperopt и трешхолд по метрике Юдена. Модели позволят сократить количество просроченных платежей минимум в 3 раза.,Приз участникам команды — 100 тысяч рублей и мерч от организаторов.,Состав команды:,Круть Дарья, аналитик;,Обидина Екатерина, аналитик;,Феоктистов Дмитрий, Data Scientist;,Гурьянов Евгений, фронтенд;,Спицын Николай, аналитик, машинное обучение.,Описание решения команды: мы разработали решение, которое поможет сотрудникам компании:,парсить данные с различных сайтов и собирать их в одном месте,собирает и самостоятельно подсчитывает различные метрики, в том числе и классические финансовые показатели,использует метрики из различных областей для систематической оценки контрагента,на основе полученной оценки формирует рейтинг контрагентов, который отображается не только в списке, но и на личной странице каждого из контрагентов,Приз участникам команды — 100 тысяч рублей и мерч от организаторов., «Что найдет Сервер». ,В целом, как наши ожидания, так и ожидания участников оправдались, все остались довольны результатами. Что касается самих решений, то вполне возможно, что некоторые из них будут опробованы «в полевых условиях» и внедрены в нашей компании.,Это мероприятие было «пробным шаром» в сторону создания профессионального комьюнити в области цифровизации корпоративных процессов. Мы, «Северсталь», хотим развивать это направление и дальше расширять области потенциального взаимодействия с участниками рынка — фрилансерами, любителями хакатонов и аутсорсерами. Летом мы планируем провести митап, в т.ч. с участниками хакатона, чтобы рассказать о прогрессе тех кейсов, которые мы прорабатывали вместе, а также анонсировать наш дальнейший трек работы в этом направлении. Расскажем об этом чуть позже — следи за анонсами нашего блога.,Пользователь",Цифровая металлургия: результаты хакатона от «Северстали» / Хабр
[<200 https://habr.com/ru/company/selectel/blog/661715/>],page2,Пользователь,Лучшие одноплатники и парочка проектов на базе RP2040 весны 2022: что предлагают производители / Хабр
[<200 https://habr.com/ru/company/globalsign/blog/662489/>],page2,информационная безопасность,Reticulum — радиопротокол для mesh-сети. Зашифрованная пиринговая связь без интернета / Хабр
[<200 https://habr.com/ru/post/662521/>],page2,"Недавно на github был , этот вопрос, вызванный , новой платформы javascript, близкой к вершине промежуточного рейтинга ,. Эта платформа Just(js) является тем, над чем я работаю в качестве пет-проекта уже некоторое время. Здесь я попытаюсь дать краткий ответ на вопрос, а в ближайшие недели дам дополнительные подробности об этом и о самой платформе.,Если вы обнаружите, читая это, что вам становится жарко, пожалуйста, ознакомьтесь с предостережениями ниже, прежде чем двигаться дальше.,Рейтинги Techempower проводятся с 2013 года и стали очень полезным инструментом по ряду причин.,Это отличное место, чтобы получить хороший обзор различных фреймворков, доступных на разных языках и на разных платформах.,Он охватывает довольно хороший спектр основных задач, которые должен выполнять любой веб-сервис. Определенно есть большие пробелы, но у людей из Techempower есть планы по их дальнейшему развитию.,Он дает некоторую полезную информацию о том, в чем заключаются относительные сильные и слабые стороны различных платформ и языков.,Он оказывает влияние на выявление слабых сторон и направляет усилия во многих из этих сред для повышения производительности. Он показывает постоянное улучшение средней производительности конкурирующих фреймворков. Хотя все такие соревнования следует воспринимать с большим недоверием, они действительно влияют на ,. Совокупный балл пяти лучших фреймворков улучшился на 62% с 24 тыс. в , до 39 тыс. в ,, и в настоящее время он составляет 48 тыс. в , промежуточном раунде., — это небольшой, простой и, надеюсь, эффективный серверный javascript-фреймворк, построенный поверх движка javascript ,. В настоящее время он поддерживает только Linux x86_64. Он всё ещё довольно далёк от стабильной бета-версии, но я не ожидаю каких-либо больших изменений в функциональности, а кодовая база очень мала (примерно 5 тысяч строк C++ и javascript), поэтому не должно быть огромного объёма работы, чтобы довести его до какого-то стабильного состояния.,В будущих постах я более подробно расскажу о причинах и мотивах, стоящих за разработкой, но сейчас я могу сказать, что основные цели заключаются в следующем:,иметь небольшую кодовую базу и как можно меньше исходных файлов (т. е. уменьшить когнитивную нагрузку),отдавать предпочтение вызовам функций в стиле C, а не сложным объектно-ориентированным структурам,быть эффективным при выполнении и использовании памяти,быть только на одной платформе, что означает, что код легче понять без множества макросов и вездесущих #ifdef-ов,быть простым для понимания и модификации для любого, у кого есть базовые знания C / C ++ и системного API Linux.,быть полезной платформой для получения дополнительной информации о движке v8 и системного API Linux.,быть легко расширяемым и настраиваемым для встраивания,продемонстрировать, что javascript может быть правильным выбором для создания высокопроизводительного программного обеспечения системного уровня и сетевых серверов.,Хорошо, хорошо! Вот 10 лучших фреймворков из последнего промежуточного запуска Techempower. Пожалуйста, прочитайте предостережения, чтобы понять, почему эти результаты следует воспринимать с небольшим скептицизмом.,Вы можете посмотреть полные результаты ,.,Есть несколько вещей, которые мы можем отметить:,Как и следовало ожидать, в верхних строчках рейтинга преобладают фреймворки C++ и Rust.,Just(js) отстает от самой производительной ,, набрав 95 % общего балла и на 4 % опережая следующую по производительности ,.,Just(js) на 10% и 13% лучше, чем самые производительные ,.,По сравнению с другими языками «высокого уровня» оценка Just(js) на 32% выше, чем у самых эффективных фреймворков , и ,.,Just(js) оценивается на 40% выше, чем самый эффективный ,.,При сравнении с другими средами Javascript разница становится довольно существенной.,Just(js) оценивается в 2 раза выше, чем следующая по производительности платформа Javascript, ,, основанная на Vert.x и JVM.,Его оценка в 5-6 раз выше, чем у , , Javascript на основе Node.js.,Ну, это было , тяжелой работы. Мои первоначальные попытки, которые я сравнивал на своем ноутбуке с лучшими фреймворками C++, Rust и Javascript, были так себе.,Я видел только 50% от лучшей производительности с особенно плохими оценками в тестах с несколькими запросами, обновлениями и фортуной, которые являются наиболее соответствующими реальным сценариям.,Я не был слишком разочарован этим, у меня все еще было много возможностей для оптимизации и сильное чутьё на то, что можно сделать большие улучшения, поэтому я приступил к работе.,Моя первоначальная попытка заключалась в использовании ,, взаимодействующего с официальным C API ,. Я много раз тестировал и анализировал трафик и столкнулся с узкими местами, которые не мог объяснить, поэтому решил отказаться от этого и вместо этого попытаться написать библиотеку Javascript для взаимодействия с Postgres. Позже я обнаружил (изучив исходный код других фреймворков), что существует , для libpq, который решает эти проблемы, поэтому в будущем я могу вернуться к привязке C++.,После нескольких дней изучения документации и тестирования postgres я был доволен удивительно хорошими результатами, которые я увидел в ,, который я собрал. Мне пришлось усердно работать, чтобы обеспечить как можно меньше выделений из кучи, и внедрить множество оптимизаций, таких как возможность , запросов в ArrayBuffers и использование подготовленных операторов в Postgres, что намного быстрее, чем выполнение необработанных запросов Sql.,Вот небольшой фрагмент того, как работают предварительно скомпилированные запросы:,и как это вызывается:,Я также добавил некоторые дополнительные оптимизации, которые позволили мне выжать еще несколько падений производительности:,Использование ,, который использует инструкции SSE4 для молниеносного синтаксического анализа http.,Использование , вместо нативного JSON.stringify дало небольшую, но заметную разницу по всем направлениям.,Выполнение экранирования HTML , с чтением строк из ArrayBuffers, возвращаемых из Postgres, имело большое значение в тесте Fortunes. Строки размещаются в куче v8, и если они недолговечны, их приходится собирать мусор, что сильно снижает производительность. Я признаю, что экранирование строки на уровне драйвера базы данных - это небольшой обман, поэтому я, вероятно, вернусь к этому, когда у меня будет время.,Я заметил, что другие фреймворки используют одиночные запросы для выполнения обновлений, а не запрос для каждого обновления, и также принял этот подход. Вы можете увидеть, как это выглядит ,.,Мне не удалось найти достаточно быструю библиотеку HTML-шаблонов для теста Fortunes, поэтому мне нужно проделать дополнительную работу, чтобы найти для этого оптимальное решение. На данный момент html ,, что опять-таки немного обманывает, но я уверен, что смогу найти что-то, что работает и обеспечивает эквивалентную производительность.,Поскольку наборы результатов для теста Fortunes невелики, я обнаружил, что простая , работает быстрее, чем стандартная Array.sort в Javascript/v8.,Компания Techempower запускает , на 3 серверах Dell R440, каждый из которых оснащён процессором Intel Xeon Gold 5120 с 28 потоками и выделенным сетевым адаптером Cisco 10 Гбит/с.,Тестируемый фреймворк работает на одной машине, база данных — на другой, а инструмент бенчмаркинга (использующий ,) — на третьей. Всё работает внутри докера с использованием сети хоста, поэтому накладные расходы от этого должны быть минимальными.,Проделав всю эту работу, я теперь увидел, что Just(js) соответствует самым производительным фреймворкам в моих локальных тестах, поэтому был достаточно уверен, что смогу хорошо набрать в производственной среде techempower.,Я отправил свой пул-реквест, он был рассмотрен и принят, а затем с нетерпением ждал, пока серверы techempower будут проходить тесты. Общий запуск занимает около 5 дней каждый раз, поэтому я был очень разочарован, когда моя первоначальная заявка не удалась во время производственного запуска. Это было связано с тем, что DNS в производственной среде работал иначе, чем в локальной системе techempower, и мой код не обрабатывал это правильно.,Я внес , в код DNS, чтобы использовать /etc/resolv.conf и /etc/hosts для определения IP-адреса базы данных, как только я понял, что производственная среда использует параметр докера --add-host для ввода IP-адреса базы данных в файл контейнера /etc/hosts.,К счастью, после некоторого беспокойного ожидания тесты в следующий раз прошли успешно, но я был немного разочарован, увидев, что Just(js) занял лишь , в общем зачёте, набрав примерно 70% от производительности лучших фреймворков.,После моего первоначального разочарования я решил, что это на самом деле довольно респектабельный результат, поскольку только фреймворки более низкого уровня Rust и C++ получают более высокие баллы, поэтому я сделал перерыв на несколько дней и обдумал, что может быть причиной несоответствия между моими локальными тестами и тестами в производственной среде.,После перерыва, я решил арендовать некоторое время на сервере с аналогичными характеристиками от , (теперь Equinix Metal), чтобы посмотреть, смогу ли я воспроизвести проблему в аналогичной среде. После долгих исследований производительности и стратегии я заметил, что существует огромное количество накладных расходов на системные вызовы. При этом преобладали вызовы pthread_mutex_lock и pthread_mutex_unlock, поэтому я решил покопаться в коде v8, чтобы посмотреть, что может быть причиной этого.,В конце концов я отследил проблему до ,. Кажется, что каждый раз, когда мы вызываем ArrayBuffer->GetBackingStore(), v8 использует мьютекс, чтобы избежать гонок при чтении резервной памяти. Я предполагаю, что это связано с тем, что сборщик мусора работает в отдельном потоке, но это нужно исследовать дальше. Я создал тикет в группе v8-users google и надеюсь, что смогу составить воспроизводимый отчёт о проблеме для просмотра командой v8.,В этот момент я действительно ломал голову над тем, что я мог бы сделать, но во время тестирования на сервере пакетов я заметил, что проблема усугублялась по мере того, как я использовал больше потоков. Первоначальная отправка , для каждого экземпляра сервера, поэтому я решил попробовать использовать процессы вместо потоков.,Это оказалось прорывом, на который я надеялся, и я увидел огромное улучшение на сервере пакетов по сравнению с многопоточным подходом. Я до сих пор не совсем уверен, почему это так, но я предполагаю, что куча v8 и/или GC распределяются между потоками при использовании потока для каждого изолята v8, что означает много конфликтов при чтении этих мьютексов из ArrayBuffers.,После внесения моих изменений , показал огромное улучшение. Just(js) теперь занимает 2-е место в общем зачете и находится в пределах 5% от самой производительной среды и сохраняет свою позицию в последующих запусках.,Вы можете увидеть огромное сокращение накладных расходов на системные вызовы на этих двух графиках для теста с несколькими запросами:,Это показывает падение общего времени ЦП на 80% только из-за переключения с потоков на процессы. Надеюсь, я смогу понять, почему эта проблема возникает при использовании многопоточного подхода, поскольку он потребляет намного меньше памяти, чем отдельный процесс для каждого изолятора.,Теперь давайте более подробно рассмотрим отдельные тесты и то, как Just(js) сравнивается с другими фреймворками на них. Я использовал этот действительно ,, который позволяет нам увидеть больше деталей, чем веб-сайт techempower. Я также внёс некоторые изменения в него локально, чтобы видеть количество запросов в секунду на поток, что даёт нам лучшее представление об относительной производительности, чем необработанные числа RPS.,Я выбрал подмножество фреймворков для сравнения здесь:,Два самых эффективных фреймворка C++ — lithium и drogon,Два самых эффективных фреймворка Rust — ntex и may-minihttp,Два самых эффективных фреймворка Go — fiber и fasthttp,Два самых эффективных Java-фреймворка — jooby и wizardo-http,Самая производительная среда C#/.Net — aspcore,Самый эффективный PHP-фреймворк — php-ngx,Пять самых эффективных фреймворков Javascript — Just(js), es4x, polkadot, nodejs и fastify.,Ниже я обсуждаю только необработанную пропускную способность и использование ЦП. Я надеюсь, что смогу более подробно рассказать о результатах памяти и задержки в следующем посте.,Подробнее о требованиях к тесту можно прочитать ,.,Тест простого текста является самым простым и, вероятно, наименее полезным, но он даёт нам некоторое представление о производительности парсера HTTP, цикла обработки событий и сетевого API. Результаты для этого также приходят с большой оговоркой, что все самые производительные фреймворки оцениваются примерно одинаково. Это связано с тем, что сеть 10 Гбит/с насыщена 7 миллионами запросов в секунду.,Мы можем видеть из числа RPS на поток, что между наиболее эффективными фреймворками существует значительная разница, даже несмотря на то, что их общие показатели RPS схожи.,Just(js) неожиданно оказался лучшим в этом тесте, опередив lithium (C++) на 3% по количеству запросов в секунду на поток. Он превосходит самую производительную среду Rust на 17 %, C# на 27 %, Java на 38 %, Go на 50 %, Javascript/Vert.x на 84 % и Javascript/Node.js на 93 %.,Вы можете видеть, что 5 лучших фреймворков на этом графике не используют максимальную нагрузку на ЦП. Это связано с тем, что сеть является узким местом в этом тесте. Надеемся, что в какой-то момент techempower сможет запустить тесты в более быстрой сети или перепроектировать тесты, чтобы устранить этот недостаток.,Just(js) использует только 50% доступного ЦП (65% пользователей, 35% системы) для обслуживания 7 млн запросов / передачи 10 Гб данных в секунду, что указывает на то, что теоретически он может достичь около 14 млн запросов в секунду или 20 Гб/с если 28 ядра будут использоваться полностью. Это довольно удивительные цифры для языка, который, как правило, получает много критики за то, что он не соответствует стандартам производительности, по крайней мере, на стороне сервера.,Я удивлен, что Go и Java получили такие низкие оценки в этом тесте, поэтому, возможно, есть место для улучшения реализации тестов для этих языков. Я также полагаю, что в реализации Node.js и PHP могут быть внесены некоторые оптимизации.,Тест JSON немного более реалистичен, чем простой текст, и даёт нам некоторое представление об относительной производительности сериализации JSON в тестируемых фреймворках.,В этом тесте Just(js) занимает второе место в RPS/Thread, на 9% отставая от самой производительной среды Lithium/C++. Мы также видим, что ЦП не исчерпан для Just (js) и Lithium, а Lithium использует только 85% доступного ЦП. Вполне вероятно, что в этом случае клиент бенчмаркинга максимально загружает ЦП, но techempower в настоящее время не предоставляет данные для клиента или сервера Postgres. Было бы неплохо, если бы мы могли включить эти цифры в наборы данных techempower в какой-то момент в будущем.,Для RPS/Thread оценка Just(js) на 9% ниже, чем у лучшего фреймворка C++, на 2% выше, чем у лучшего фреймворка Rust, на 5% выше, чем у лучшего фреймворка Java, на 20% выше, чем у лучших фреймворков Go и PHP, на 25% выше, чем у лучшего фреймворка C#, на 41 % выше, чем у Javascript/Vert.x, и на 64 % выше, чем у Node.js.,Этот тест является первым, включающим базу данных, и запускает один запрос на запрос, сериализуя результаты в JSON.,В этом тесте Just(js) уступает Lithium с большим отрывом — 27%. Я не совсем понял, почему он так сильно отстаёт, поскольку он превосходит Lithium в тестах Multi-Query и Update, поэтому мне придётся провести дополнительное исследование, чтобы увидеть, смогу ли я немного улучшить результаты здесь.,Just(js) на 27% ниже лучшей среды C++, на 9% выше лучшей среды Rust, на 11% выше Java, на 21% выше Javascript/Vert.x, на 23% выше PHP, на 30% выше Go, на 32% выше C# и На 60% выше Node.js.,Вы также можете заметить, что многие фреймворки, включая Just(js), здесь не используют максимальную нагрузку на ЦП. У лучших исполнителей это, вероятно, связано с тем, что база данных заполнена до максимума, но опять же это невозможно определить, поскольку мы не получаем эти данные из наборов данных techempower. Для фреймворков с более низкой оценкой это, вероятно, связано с задержкой, вызванной драйверами libpq, используемыми для связи с Postgres.,Тест Fortunes, вероятно, является наиболее реалистичным и включает в себя выбор ряда строк из базы данных, динамическую вставку новой строки в набор результатов, а затем сортировку и HTML-экранирование результатов перед сериализацией в JSON.,Это тест, в котором мы видим худшую производительность Just(js) по сравнению с лучшими фреймворками C++ и Rust. Мне пришлось изрядно потрудиться, чтобы получить хорошую производительность в этом тесте, и я не уверен, что его можно еще оптимизировать. Самым большим узким местом для Just (js) здесь является необходимость экранировать каждое поле в наборе результатов по отдельности, что означает создание большого количества строк, выделенных в куче v8, которые должны быть потом очищены. Я всё ещё надеюсь, что смогу придумать лучший подход, который позволил бы экранировать все строки за один вызов в среде выполнения C++.,Just(js) занимает здесь 5-е место, на 26% отставая от самых эффективных фреймворков C++, на 16% отставая от Rust, на 13% выше Java, на 17% выше C#, на 19% выше Go, на 25% выше PHP, на 40% выше Javascript/Vert. .x и на 57% выше Node.js.,Опять же, похоже, что ряд фреймворков здесь (в частности, Lithium и Wizardo-Http) сталкиваются с узким местом. На данный момент невозможно определить, что это такое, но вряд ли это будет база данных и, скорее всего, что-то внутреннее для используемых клиентских библиотек postgres.,Тест с несколькими запросами требует, чтобы фреймворки извлекали 20 случайно выбранных строк из базы данных в виде 20 отдельных запросов, объединяли результаты и сериализовали их в JSON.,Это первый тест, в котором Just(js) лидирует. Вероятно, это связано с тем, что он использует собственный клиент postgres, написанный на Javascript, и в полной мере использует конвейерную обработку запросов. Это также позволяет избежать отправки Sync/Commit при каждом запросе. Насколько мне известно, это соответствует правилам, но я буду рад внести изменения для синхронизации при каждом запросе, если это не так.,Мы также можем видеть из этих результатов, что база данных теперь стала узким местом. Just(js) использует только 15% доступного ЦП, что означает, что он тратит 85% своего времени на бездействие, ожидая возврата результатов из базы данных. Было бы неплохо, если бы techempower могла изменить тесты в какой-то момент, чтобы предоставить больше ресурсов серверу базы данных, чтобы мы могли максимально использовать ЦП на сервере инфраструктуры и посмотреть, как тогда будут выглядеть результаты.,В этом тесте Just(js) выполняет 16 тыс. запросов в секунду на поток или 66 тыс. запросов в секунду в целом при 16% доступной вычислительной мощности.,Just(js) занимает здесь первое место, на 23% выше лучшей среды C++, на 84% выше лучшей среды Rust, на 86% выше PHP, на 87% выше Java, на 90% выше Javascript/Vert.x, на 91% выше Go, На 94 % больше, чем C#, и на 96 % больше, чем Node.js.,Это довольно поразительные цифры!,Тест обновлений требует извлечения 20 случайно выбранных строк из базы данных в виде 20 отдельных запросов, а затем обновления каждой из этих строк новым случайным значением. Techempower позволяет группировать обновления, но не запросы, и Just(js) использует это преимущество.,Опять же, в этом тесте Just(js) имеет большое преимущество, и это, вероятно, связано с возможностью конвейерной обработки пользовательской клиентской библиотеки postgres.,Мы снова видим, что узким местом здесь является база данных, даже в большей степени, чем тест Multi-Query. Это имеет смысл, поскольку база данных должна выполнять гораздо больше работы при обновлении строк, чем при выборе, и я наблюдал в своих собственных тестах значительную нагрузку на , Postgres.,В этом тесте Just(js) обрабатывает 14 тыс. запросов в секунду на поток или 36 тыс. запросов в секунду в целом при 9,4% доступной вычислительной мощности.,Just(js) занимает здесь первое место, на 42% выше лучшей среды C++, на 80% выше Rust, на 82% выше Java, на 85% выше Javascript/Vert.x, на 90% выше Go, на 91% выше PHP и на 98% выше Node.js.,Подводя итоги, мы видим, что:,Just(js) занимает 1-е место в простом тексте, 2-е место в JSON, 2-е место в одиночном запросе, 5-е место в рейтинге Fortunes, 1-е место в мультизапросе и 1-е место в обновлениях для запросов в секунду на поток.,По общему составному баллу techempower он занимает 2-е место, ноздря в ноздрю с лучшими низкоуровневыми фреймворками C++ и Rust.,Он на 6% ниже самой производительной среды C++ в целом.,Он на 11% выше, чем самый эффективный фреймворк Rust в целом.,По сводным оценкам он опережает все другие языковые фреймворки более высокого уровня, в 1,5 раза превосходит Java и C#, в 1,7 раза превосходит Go, в 2 раза превышает Javascript/Vert.x и PHP и более чем в 5 раз превосходит Node.js.,По сравнению с другими высокоуровневыми языковыми фреймворками он более чем в 5 раз превосходит (Java) по обновлениям и более чем в 5 раз по множественным запросам.,Это довольно впечатляющие цифры, если не сказать больше, и они намного превзошли мои ожидания, когда я начинал это приключение. Даже принимая во внимание множество предостережений, изложенных ниже, и недостатки в текущем процессе сравнительного анализа, я уверен, что Javascript способен выстоять против самых эффективных фреймворков для сценариев, охватываемых этими тестами. Будем надеяться, что люди из techempower смогут продолжить разработку и улучшение своего процесса, и мы увидим, что Javascript продолжит сиять в будущем.,В последние месяцы я проделал огромную работу, но не хочу приписывать себе эти результаты. Невероятная производительность обусловлена потрясающей работой, проделанной командой v8, постоянно расширяющей границы того, что может сделать Javascript.,Я хотел бы сказать большое спасибо людям из Techempower за предоставление этого ресурса сообществу, а также за помощь и поддержку, которые они оказали во время моих расследований.,Надеюсь, вам было интересно читать и выводы были интересны. Если вы хотите обсудить фреймворк Just(js) или принять участие, свяжитесь с DM в , или оставьте комментарий ниже.,Пожалуйста, отнеситесь к этим результатам и моим размышлениям выше со здоровой долей скептицизма. Есть ряд моментов, которые я хочу прояснить, чтобы избежать каких-либо споров:,В общем, бенчмарки — это что-то вроде ,. Каждый эталонный тест будет иметь свои недостатки и погрешности, и помимо пиковой производительности существует целый ряд других факторов, которые необходимо оценивать при выборе платформы или фреймворка для создания веб-сервисов.,Обсуждаемые здесь результаты techempower получены из промежуточного прогона и не являются официальными. Текущие официальные рейтинги датируются , года, и я не уверен, когда запланирован следующий официальный раунд, но я надеюсь, что Just(js) сможет сохранить свои позиции, когда они будут опубликованы.,В результатах также используется набор , из 19-го раунда, которые не были скорректированы с учётом последних промежуточных результатов. Я не ожидаю, что это сильно изменит выводы, сделанные здесь.,Я не утверждаю, что Just-JS «лучше» любой другой платформы. Он всё ещё находится на очень ранних стадиях разработки, и предстоит ещё много работы, чтобы сделать платформу более надёжной и функциональной. Его сравнивают со зрелыми и широко используемыми фреймворками, которые будут гораздо более надежными и будут иметь гораздо более продвинутые функции, чем Just (js).,Just (js) Javascript-платформа: ,Введение в Techempower: ,Тестовая среда Techempower: ,Последние промежуточные результаты Techempower: ,TFBVis — инструмент визуализации Techempower: ,Правда о традиционных тестах Javascript: ,Блог разработки V8: ,С++ фреймворк lithium: ,Ntex Rust фреймворк: ,Asp.Net: ,Jooby Java/Kotlin фреймворк: ,Fiber Go фреймворк: ,Polkadot Node.js фреймворк: ,ES4X Javascript/Vert.x фреймворк: ,Fastify Node.js фреймворк: ,Патч конвейера Postgres: ,Форматы сообщений протокола Postgres: ,PicoHTTPParser: ,Equinix Metal (ранее Packet): ,Что нужно и чего нельзя делать при сравнительном анализе стека: ,Программист",О производительности Javascript. Рейтинг Techempower / Хабр
[<200 https://habr.com/ru/company/audiomania/blog/662315/>],page2,"Расскажем о проекте, который используют орнитологи-любители и ученые, и библиотеках с записями тысяч песен пернатых — в том числе для коммерческого использования.,Есть множество исследовательских проектов, посвящённых анализу птичьих голосов. Таким образом ученые стремятся оценить популяцию и качество жизни животных в отдельных регионах. Нередко орнитологи записывают длинные аудиозаписи со звуками природы, а затем , классификацией на слух.,Чтобы автоматизировать этот процесс, специалисты из Лаборатории орнитологии при Корнеллском университете в США , нейросеть BirdNET. Она идентифицирует более 3 тыс. птичьих видов по спектрограмме их пения. Система помещает результаты ,, которая помогает орнитологам следить за популяцией животных.,Работу алгоритма можно оценить на официальном , — он анализирует живое аудио с микрофона, установленного на природе. В аудиопотоке встречаются голоса ,, ,, , и других пернатых. Также есть демо, куда , собственную аудиозапись для анализа.,Поработать с нейросетью можно не только в браузере. Приложение запускается как на Raspberry Pi, так и домашнем ПК — в частности, под Ubuntu и Windows. Инструкция по настройке для открытой операционной системы приведена в ,. Что касается Windows, то достаточно скачать и распаковать , и начать работу ,.,Еще разработчики выпустили ,. Оно позволяет записать птичье пение на микрофон, выделить наиболее отчетливый фрагмент и отправить на распознавание. Но поскольку алгоритм восприимчив к посторонним шумам, лучше записывать пернатых рано утром или поздно вечером.,Нейросеть BirdNET уже легла в основу нескольких исследовательских проектов. Например, в рамках инициативы , энтузиасты строят карту с местами обитания птиц. По всему миру установлены десятки акустических станций, которые мониторят природные зоны и идентифицируют птичьи голоса.,Другой проект — , — позволяет превратить компактный компьютер Raspberry Pi в мобильную орнитологическую станцию. Анализ акустической среды происходит с помощью звуковой карты, подключенной по USB. Аналогичную систему развивают авторы , при поддержке Федерального министерства окружающей среды Германии. Это — сеть автономных систем мониторинга птичьих голосов в лесах по стране.,Один из резидентов Hacker News в тематическом треде ,, что масштабная система на базе нейросети BirdNET в перспективе позволит визуализировать миграцию пернатых. Однако подобные проекты уже существуют — например, , система , для мониторинга так называемых , (NFC). Это — звуки, которые издают птицы во время ночных полетов, чтобы координировать свои движения.,Проект достаточно молодой и не готов к масштабным внедрениям, но в будущем его смогут использовать как орнитологи-любители, так и ученые-биологи.,Другой проект — ,. Это — специальное устройство, предназначенное для установки на заднем дворе загородного дома. Оно мониторит акустический фон, распознает пение обитающих рядом птиц и фиксирует животных с радиометками. Далее, информация поступает в общую базу данных для дальнейших исследований.,Есть проекты, которые стремятся не визуализировать перемещение птиц, но сохранить их голоса для потомков. О некоторых таких проектах мы , в одном из наших прошлых материалов. Так, Служба национальных парков США формирует акустическую библиотеку ,, в каталоге которой уже более двухсот аудиозаписей птиц и диких животных. К слову, все звуки можно скачивать и свободно использовать в коммерческих целях.,Аналогичный свод данных формируют в Корнеллской лаборатории орнитологии. В их , сотни тысяч записей звуков, издаваемых птицами, рептилиями и насекомыми. Все они применяются для исследовательских и образовательных задач.,Пользователь",Сам себе орнитолог — нейросеть поможет распознать птиц по их пению / Хабр
[<200 https://habr.com/ru/post/662519/>],page2," 💡 Вы узнаете, что такое MLOps, зачем вам нужны MLOps для ваших проектов машинного обучения, почему MLOps должен быть open source и примеры существующих инструментов MLOps. ,Автор: Yuqi Li ,Оригинал: ,Большинство компаний используют инструменты MLOps для автоматизации конвейеров, мониторинга и управления различными проектами машинного обучения, которые решают их бизнес-задачи. Инструменты MLOps обеспечивают систематический подход к работе в проектах машинного обучения.,В этой статье вы узнаете:,Что такое MLOps?,Зачем вам MLOps?,Почему MLOps должны быть open source?,Пример инструментов MLOps open-source,Давайте начнем! 🚀,MLOps происходит от двух терминов «машинное обучение» (machine learning) и «операции» (operations), которые относятся к набору процедур, направленных на автоматизацию жизненного цикла проекта машинного обучения. MLOps дает группе специалистов по data science и команде IT-специалистов руководство по совместной работе и созданию наиболее эффективного решения в области машинного обучения для конкретной проблемы.,MLOps происходит от двух терминов «машинное обучение» (machine learning) и «операции» (operations), которые относятся к набору процедур, направленных на автоматизацию жизненного цикла проекта машинного обучения. MLOps дает группе специалистов по data science и команде IT-специалистов руководство по совместной работе и созданию наиболее эффективного решения в области машинного обучения для конкретной проблемы.,MLOps похож на DevOps, но исполнение отличается. Поскольку машинное обучение носит более экспериментальный характер, MLOps рассматривает другие практики, такие как валидация данных, реинженеринг признаков (features reengineering), анализ и оценка моделей, в условиях изменения данных с течением времени. Цель состоит в том, чтобы создать непрерывный пайплайн (конвейер) для всех важных этапов вашего проекта машинного обучения.,Ключевые этапы MLOps:,Сбор данных,Анализ данных,Преобразование/подготовка данных,Обучение и развитие модели,Проверка модели,Подача модели,Мониторинг модели,Переобучение модели,Инструменты MLOps могут помочь вам сделать сквозные (end-to-end) проекты машинного обучения гибкими, управляемыми и воспроизводимыми со всеми важными этапами жизненного цикла машинного обучения.,Р,Работа в ML проекте может вызвать множество проблем, особенно когда вам приходится иметь дело с такими важными моментами, как:,Сбор и очистка больших объемов данных,Анализ и визуализация ваших данных,Отслеживание конфигурации и результатов различных экспериментов во время обучения модели машинного обучения,Развертывание вашей модели в production,Мониторинг качества вашей модели машинного обучения,С MLOps вы будете выполнять все необходимые шаги для создания ваших решений машинного обучения в эффективном рабочем процессе. Это также поможет вам сэкономить время и снизить затраты на операции. Руководители проектов, специалисты по данным и другие специалисты в вашей команде будут более гибкими и будут иметь стратегии, позволяющие легко принимать решения для достижения своих целей.,MLOps предлагает множество преимуществ для команд, которые хотят интегрировать модели машинного обучения в свои приложения. Это упростит процесс развертывания, поможет улучшить операции и обслуживание, а также даст вашим моделям возможность масштабироваться когда требуется.,Компаниям в этих сценариях рекомендуется рассмотреть возможность использования инструментов MLOps для достижения своих бизнес-целей.,Масштабирование существующих моделей машинного обучения,Планирование сложных моделей машинного обучения,Проекты машинного обучения, в которых участвуют несколько команд, таких как команда Data Science и IT-команда.,Open-source ,Open-source MLOps инструменты дают вам возможность свободно и гибко использовать, читать и изменять исходный код, не тратя целое состояние.,Вот несколько веских причин, по которым MLOps должны быть open source.,Большинство проприетарных платформ MLOps являются более дорогостоящими по сравнению с open-source инструментами. Проприетарные MLOps постоянно просят существующих клиентов платить за обновления или новые функции для использования на их платформах.,Иногда вы можете заплатить за план подписки, но вы не собираетесь использовать все функции, представленные в этом плане, что является пустой тратой денег. С open source MLOps инструментами у вас есть возможность выбрать лучшие инструменты для своих нужд и свободно модифицировать их в соответствии со своими конкретными потребностями.,Open source MLOps инструменты дают вам возможность владеть и управлять ими с полным контролем. Вы можете установить и запустить open-source MLOps инструмент в своей собственной инфраструктуре для запуска своих проектов машинного обучения.,Большинство проприетарных MLOps инструментов принадлежат компаниям, которые не позволяют вам, как клиенту, иметь полный контроль над своим инструментом для запуска ваших проектов. Иногда вам нужно будет запросить разрешение или отправить запрос на выполнение конкретной задачи на их платформах. Очевидно, что это может занять больше времени, чем ожидалось.,Они сказали, что данные — это новая нефть, поэтому вы не хотите, чтобы все имели доступ к вашим активам. По данным ,, компании ежедневно генерируют около , данных в различных отраслях. К , году сгенерированные данные будут стоить около 77 миллиардов долларов.,Данные обычно очень чувствительны. Использование проприетарных MLOps может привести к проблемам с конфиденциальностью данных, поскольку вы не полностью контролируете платформу. Open source MLOps могут помочь вам избежать любых проблем с конфиденциальностью за счет:,Больше контроля над вашими данными.,Адаптация вашего MLOps инструмента в соответствии с вашей политикой конфиденциальности данных.,Полный аудит вашего MLOps инструмента для проверки манипуляций с данными и внесения необходимых изменений.,Open-source MLOps инструменты создаются и поддерживаются сообществом пользователей. Любой участник сообщества может предлагать новые идеи или новые функции для платформы. Вы также можете помочь кому-то еще решить проблему на платформе.,Коды инструмента всегда доступны, что дает членам сообщества возможность внедрять инновации и эффективно сотрудничать. Например, репозиторий MLflow GitHub в настоящее время содержит , , чтобы предложить новые изменения в инструменте. Эти запросы либо добавляют новые функции, либо устраняют ошибки.,Проприетарные MLOps не предлагают тех же преимуществ по сравнению с open-source MLOps. Устранение ошибок или получение помощи в проприетарном инструменте MLOps может занять больше времени, поскольку у них есть специальная группа людей, которые отвечают на ваши запросы.,Существует ,Существует множество open-source MLOps инструментов, которые вы можете начать запускать и управлять своими проектами машинного обучения. Большинство этих инструментов предлагают различные услуги, поэтому вам придется выбрать один из них в соответствии с вашими требованиями. Вот некоторые рекомендуемые популярные open-source инструменты.,Это дружественная к Python и code-based система MLOps, но она также поддерживает язык R. Вы можете использовать этот инструмент MLOps в различных проектах машинного обучения с большими командами. Он предоставляет различные сервисы машинного обучения, интегрированные в AWS.,Metaflow open-source с декабря 2019 года и изначально был разработан в Netflix, чтобы помочь специалистам по данным и инженерам по машинному обучению создавать и управлять реальными данными научных проектов.,Open-source оснащен встроенными функциями для:,Управления внешними зависимостями,Управления вычислительными ресурсами,Воспроизведения и возобновления выполнения рабочего процесса,Перемещайтесь назад и вперед между локальным и удаленным режимами выполнения,Выполнения контейнерных запусков,Metaflow доступен в виде пакета Python для пользователей macOS и Linux.  Вы можете загрузить его, посетив их , или получить последнюю версию из ,:, pip установить metaflow,Это еще один open-source MLOps инструмент, который может помочь вам управлять жизненным циклом проекта машинного обучения. В настоящее время MLflow предлагает четыре компонента:,MLflow Tracking,Этот компонент помогает вам использовать API или UI для регистрации параметров, метрик, версий кода и выходных файлов при проведении экспериментов машинного обучения и, наконец, визуализировать результаты. Вы можете использовать Python API, R API и Java API для регистрации и запроса экспериментов.,MLflow Projects,Этот компонент помогает вам упаковать код машинного обучения в повторно используемую и воспроизводимую форму, чтобы вы могли развернуть его в работе или поделиться им с другими специалистами по данным.,MLflow Models,Этот компонент помогает вам управлять моделями машинного обучения, разработанными с использованием различных библиотек машинного обучения, и развертывать их на различных платформах обслуживания моделей и вывода.,Model Registry,Вы можете использовать этот компонент для совместного управления полным жизненным циклом модели MLflow путем аннотирования, хранения и управления моделями в центральном репозитории., Примечание : Он также интегрирован с несколькими популярными решениями MLOps, такими как Amazon SageMaker, Google Cloud, Docker, ,, Kubernetes и Databricks.,Это система управления версиями open-source для проектов Data Science и машинного обучения.  Он может контролировать наборы данных, модели машинного обучения и другие файлы в вашем проекте.,Он также связывает их с помощью кода и использует Amazon S3, Microsoft Azure Blob Storage, Aliyun OSS, SSH/SFTP, Google Drive, Google Cloud Storage, HDFS, HTTP, сетевое хранилище или диск для хранения содержимого файлов.,Как гибкий инструмент MLOps, DVC может помочь группе специалистов по обработке и анализу данных сотрудничать и разрабатывать общие и воспроизводимые проекты машинного обучения.  Система управления версиями данных имитирует любой стандартный репозиторий, сервер или поставщик Git, поэтому вы можете легко внедрить DVC в свой проект машинного обучения. ,Open-source MLOps ,Open-source MLOps инструменты обладают большим потенциалом и преимуществами по сравнению с проприетарными инструментами MLOps.  Вот почему все больше и больше проектов машинного обучения используют MLOps с открытым исходным кодом.,Проприетарные инструменты MLOps рекомендуется использовать только в том случае, если у вашей команды нет большого инженерного опыта для создания и управления инфраструктурой для запуска ваших проектов машинного обучения.,В этой статье вы узнали, что такое MLOps, зачем вам нужны MLOps для ваших проектов машинного обучения, почему MLOps должен быть open source и примеры существующих инструментов MLOps.,Если вы узнали что-то новое или вам понравилось читать эту статью, поделитесь ею, чтобы другие могли ее прочитать.,Machine Learning | MLOps | Robotics",Почему инструменты MLOps должны быть с открытым исходным кодом? / Хабр
[<200 https://habr.com/ru/post/662533/>],page2,"Много лет назад я занимался созданием маленьких Flash игр и публиковал их на сайте Newgrounds. Сейчас я делаю полноценные игры для ПК.,На сегодняшний день у меня 4 законченные коммерческие игры в Steam, и самая последняя из них — выпущенная в 2021 году Pilie Pals, о процессе создания которой я расскажу в этой статье. Я работал над игрой всего примерно 6 месяцев, по вечерам после работы и на выходных.,Я занимаюсь дизайном, программированием, графикой, звуками и музыкой в одиночку. Мне это нравится, и таким образом я могу часто переключаться с одного вида деятельности на другой, благодаря чему не теряю интерес к разработке игры.,Я написал собственный 3D игровой движок ,, используя Haxe, C++ и OpenGL, и на данный момент он используется тремя моими играми. Подробности и причины создания собственного движка приведены ,. Такой подход меня вполне удовлетворяет, и я не планирую его менять.,Трейлер к игре. Доступна ,, а также есть ,.  ,В начале разработки Pilie Pals я скопировал папку проекта своей предыдущей игры , и удалил все ресурсы и файлы кода, связанные с игрой. Остался ""чистый"" движок, с которым можно экспериментировать, чтобы создать прототип следующей игры.,Перед тем, как описывать создание Pilie Pals, я объясню несколько главных принципов работы моего движка.,Мой движок почти полностью основывается на внешних данных (data-driven). Это значит, что я создаю набор файлов с данными, а движок их читает и обрабатывает.,Я стараюсь избегать жёсткого прописывания чего-либо в исходном коде игры. По возможности, вся информация хранится в отдельных файлах: игровые объекты, уровни, переводы текстов, визуальные и звуковые эффекты, и даже некоторая игровая логика, написанная ,. Большая часть данных хранится в формате JSON.,Самое большое преимущество такого подхода заключается в том, что я могу создавать и править игру, не выходя из неё. При изменении какого-либо файла с данными движок автоматически перегружает ту часть данных, которая изменилась, без необходимости перезапускать или пересобирать что-то вручную. То же самое происходит с другими ресурсами, например, с текстурами, 3D моделями и звуками.,Ещё один плюс — это потенциальная поддержка пользовательских модификаций игры. Недавно я выложил новое обновление для игры, которое добавляет русскую локализацию, ,.,Два главных компонента сцены игры в моём движке — это сущности и карты.,Сущность — игровой объект, который я описываю в JSON файле, чтобы впредь создавать экземпляры объектов такого вида. Например, персонаж игрока — это сущность.,Файл описания сущности содержит информацию:,Какие 3D модели содержит данная сущность, и как их отображать,Какие анимации могут совершать модели этой сущности, и как осуществляются переходы из одного состояния в другое,Какие эффекты может запускать данная сущность,Какие звуки может воспроизводить данная сущность,Какие области соприкосновения есть у данной сущности,Какие у сущности могут быть состояния, и каково поведение сущности в разных состояниях,Другие данные для использования в игровой логике: специальные тэги, группы, и т.д.,Стоит отметить, что у каждого экземпляра сущности есть собственная машина состояний, и у каждого состояния есть последовательность действий, которую сущность может проигрывать.,Например, можно задать состояние ""walk"" (ходьба) для сущности персонажа игры, и последовательность действий этого состояния может содержать команды, которые запускают нужную анимацию 3D модели, проигрывают звуки шагов и показывают эффект поднимающийся с земли пыли. Всё это описывается в текстовом файле, который можно редактировать и сразу тестировать, и это сильно ускоряет процесс разработки.,Карта — это файл данных, который содержит описание расположения сущностей. В этом файле также есть многоуровневая сетка ""плиток"", из которой можно построить ландшафт. Я не меняю файлы карт вручную — карты создаются с помощью встроенного редактора.,Так как движок знает всю информацию о сущностях, которые располагаются на карте, он в состоянии автоматически оптимизировать некоторые вещи. Например, если сущность является статичной и никогда не двигается (например, дерево или ландшафт), то движок автоматически объединяет её вместе с другими статичными сущностями, которые используют одинаковую текстуру, и создаёт одну общую 3D модель. Это значительно сокращает количество отображаемых видеокартой объектов, что сильно улучшает  производительность игры.,Некоторая часть игровой логики может быть описана в текстовых файлах, но она используется только для создания игровых сценариев, а не основной функциональности игры. Такая логика, как система соприкосновений, поведения искусственного интеллекта, правила игрового процесса и т.д. —  программируется в исходном коде Haxe, который превращается в C++ при компиляции.,Есть 2 категории файлов кода, связанных с логикой:,Процессоры логики сущностей — могут быть присоединены к отдельным сущностям, используются для обработки индивидуальной логики объектов (например, искусственного интеллекта). Не каждой сущности нужен процессор логики.,Ядро — одиночный класс, который описывает общую логику правил игрового процесса.,У меня была идея о пошаговой игре-головоломке, в которой игрок может управлять сразу несколькими персонажами, способными поднимать и переносить всякие предметы. Логика игры должна была быть пошаговой, потому что я хотел записывать каждый игровой шаг, чтобы дать игроку возможность отменить свои шаги, т.е. вернуться в прошлое.,Сначала я написал ядро логики и реализовал пошаговую систему. Для каждой сущности, которая является частью головоломки, ядро выставляет состояние.,Игрок может выделить персонажа и выполнить действие (например, переместиться, или поднять предмет), которое меняет состояние игрового мира. Игра потом вычисляет, является ли возможным новое состояние мира (например, не столкнулся ли игрок с препятствием), и проверяет, нужна ли какая-либо реакция на это изменение, со стороны других элементов  головоломки (например, если игрок положил какой-то предмет на кнопку, то у кнопки должно измениться состояние на ""нажатая""). Если все проверки пройдены, то изменения применяются и добавляются в историю шагов.,Игрок может отменить последний шаг, просто вернувшись в предыдущее состояние мира.,В этом заключается основная функциональность игрового ядра. На самом деле, всё немного сложнее, потому что мне нужно обеспечить плавные, анимированные переходы между состояниями, позволить элементам быть переносимыми другими элементами, и делать другие интересные вещи — но всё это добавляется к базовому ""фундаменту"" логики игры.,Реализация такой системы и всех крайностей заняла у меня примерно неделю. Оставшееся время заняли размышления о том, какую игру я хочу сделать. В это время у меня появилась идея о том, что персонажи могли бы переносить других персонажей, и даже создавать стопки из друг друга.,Я создал 4 уровня игры, используя мой существующий редактор карт, чтобы убедиться, что игровой процесс действительно интересный. Результат мне понравился, и я продолжил разработку.,Теперь у меня был рабочий прототип игры, и можно было начинать экспериментировать с художественными стилями. Я остановился на мультяшном визуальном стиле, и весь месяц занимался созданием 3D моделей, анимаций, эффектов, интерфейса, переходами, и т.д. Игра разбита на 5 тематических миров.,Я использую Blender для создания 3D моделей и анимаций, и GIMP для создания текстур.,Пластиковый вид игры достигнут с помощью написанного мною шейдера, который применяет заранее приготовленные данные об освещении к моделям. Работает это так: берётся заготовленная картинка освещённой сферы, применяется к некоторым частям модели, смешивая цветовые данные текстуры на основе нормалей модели в пространстве экрана.,Идея такого эффекта появилась у меня после работы с программами 3D моделирования и скульптуры. Подобный эффект в таких программах иногда называется MatCap. Мой подход заключается в том, что я смешиваю такой приём с традиционными алгоритмами освещения в реальном времени. Получается интересный эффект, который обрабатывается видеокартой очень быстро.,Весь месяц я улучшал User Experience игры: добивался плавности анимаций, хорошей чувствительности управления, чистоты графических элементов, удобности интерфейса.,Было реализовано меню паузы, меню выбора уровня, система последовательности уровней и системы сохранения — практически весь этот функционал у меня уже был создан для предыдущих игр, поэтому можно было повторно использовать часть этого кода, откорректировав некоторые визуальные вещи.,Работать над интерфейсами довольно утомительно, но плохой UX раздражает игроков, поэтому важно сделать всё правильно с самого начала.,Я решил сделать отполированный, полноценный ""вертикальный срез"" как можно быстрее. Таким образом я смог бы проверить, как бы выглядел готовый проект. Так как в игре на тот момент было мало контента, можно было свободно вносить глобальные изменения без особых проблем.,Я пишу музыку и создаю звуки в виртуальном модульном синтезаторе SunVox.  Я — самоучка, и создание музыки у меня занимает довольно много времени.,В моём движке есть система динамического звукового окружения, которая может генерировать окружающий шум в реальном времени, используя подготовленные звуки (например, шум волн или крики птиц), меняя их частоту и воспроизводя в разных направлениях и в разных комбинациях. Эта информация описана в отдельном JSON файле.,На тот момент у меня была полностью ""отполированная"" игра, в которой было всего 4 уровня. Я создал ещё 6, и выпустил демо версию игры.,Так я стал собирать отзывы от игроков на ранней стадии разработки, и смог на их основе улучшить User Experience игры.,Примерно в это время я добавил систему подсказок в игру. О ней я подробно написал ,.,Следующие два месяца я делал новые уровни, добавлял новые игровые элементы, используя ту систему сущностей, которую я описал выше. В исходном коде игры уже практически не было никаких изменений, потому что ядро игры уже было полностью готово.,Эти два месяца я в основном работал в редакторе карт.,В целом я удовлетворён результатом.,В начале разработки игры у меня уже было довольно чёткое представление о том, какую игру я хочу сделать. Поэтому процесс разработки прошёл намного плавнее и быстрее, чем обычно. Так я осознал важность наличия чёткой цели с самого начала.,Также оказалось хорошей идеей сделать отполированный вертикальный срез как можно скорее. Таким образом, я мог начать делать скриншоты, видео и даже выложить демо версию хорошего качества всего после 4 месяцев работы. Я получил несколько полезных отзывов от игроков ещё до того, как большая часть контента была готова, поэтому было проще вносить изменения, ничего при этом не ломая.,В этот раз я практически ничего не менял в ядре своего движка, и большая часть времени ушла непосредственно на создание игрового контента. Я буду продолжать использовать свой движок в будущих проектах.,Индивидуальный разработчик компьютерных игр.",Как я создаю игры на своём 3D движке в одиночку / Хабр
[<200 https://habr.com/ru/post/662561/>],page2,"Существует , задача: ,Есть 2 емкости: 5 литров и 3 литра. Как отмерить 4 литра жидкости используя , эти 2 емкости?,Понятное дело что тут важно не сколько знание правильного ответа, а знание , решения таких задач. Ведь вместо целевых 4х литров могут спросить отсчитать и 1,2,6,7 литров. ,В этом тексте я решу эту задачу в общем виде при помощи конечного автомата. Так как тут явно можно проследить , и , воздействия. Также я упомяну про , язык Front-End разметки ,. Методика конечного автомата хорошо изучена и поставлена на рельсы. Состоит из 3 фаз.,Состояние определяется количеством жидкости в паре сосудов. Согласно комбинаторике по правилу умножения существует всего 24 состояния. Вот они все перечислены.,из условий задачи),Существует всего 5 , действий с бутылками. Вот их перечень.,Как гласит английская народная пословица “,” (A picture is worth a thousand words). Также мой универский профессор часто говорил, что инженеры - это про схемы. Поэтому представляю блок-схему в виде ориентированного графа состояний для задачи про бутылки.,Я накропал на языке С консольную утилиту, которая прокручивает конечный автомат и сохраняет в файл составленный код на языке dot. Далее бесплатная утилита dot.exe поедает файл *.dot и преобразует его во всем известный *.svg файл. Наконец  браузер Chrome.exe поедает  *.svg и отрисовывает в окне на мониторе. Язык Dot хорош тем что он имеет простой синтаксис. Dot более высокоуровневый язык, чем спецификация *.svg файлов.  ,Глядя на этот , , становится ,, что чтобы отмерить 4 литра надо выполнить следующую  процедуру из 6-ти инструкций:,есть еще одно , решение,Easy!,Вот код Dot графа для тех, кто захочет изучить граф внимательнее.,Можно отрисовать граф на этом сайте и сохранить его в *.svg.,Редактировать *.svg можно при помощи бесплатной программы ,.exe.,Эта задача может служить отличной , для обучения теории конечных автоматов FSM. Конечные автоматы повсеместно используются в промышленной разработке, например, системного программного обеспечения.,Существует 8 , состояний: 1/5_1/3; 1/5_2/3; 2/5_1/3; 2/5_2/3; 3/5_1/3; 3/5_2/3; 4/5_1/3; 4/5+2/3. В эти состояния никак , как только не переливай содержимое сосудов. Если вы захотите , человека на собеседовании, то можно попросить его ""как перелить жидкости так чтобы в каждой бутылине осталось по 2 литра?"" или ""как перелить жидкости так чтобы в каждой бутылке осталось по 1 литру?"".,Формально можно отмерить не только 4 лита, а , , от 1 до 8 включительно.,Как видите, язык Front-End разметки , отлично подходит для автоматической отрисовки , векторной графики. Конечно dot тула несовершенна и даже , графы строит с пересечениями ребер. Если вы знаете тулу которая , , то укажите это в комментариях.,Буду признателен, если пришлете описания подобного рода логических задач в комментариях к тексту.,Пользователь",Задача про две ёмкости для жидкости / Хабр
[<200 https://habr.com/ru/post/662481/>],page2,"Эту статью я написал для , в октябре 2017 года.,Речь идёт об игре ,, которая сейчас выпущена ,. Бесплатная demo версия прилагается.,Я разрабатывал эту игру с января 2016 года в своё свободное время в одиночку. Мною выполнено всё программирование, дизайн игрового процесса, создание графики и музыки. Кроме того, я написал собственный игровой движок с нуля. ,Люди часто спрашивают меня, почему я решил создать свой движок, когда на рынке доступно множество бесплатных универсальных движков. Есть много причин, и о них я попытаюсь рассказать в этой статье. ,Одним из самых больших преимуществ собственного движка является абсолютный контроль кода. Есть возможность настроить его именно так, как это нужно для конкретной задачи. Такой узкоспециализированный движок получается оптимизированным для конкретного типа игр и работает быстрее и надёжнее, чем универсальный движок. ,Универсальные игровые движки называются так, потому что они предназначены для общего использования, и в них заложены функции, которые не всем нужны. Это неизбежно приводит к ""раздуванию"" кода, и к замедлению производительности игр. ,Второе преимущество — это контроль самого процесса разработки. Я считаю, что инструментарий должен быть по-максимуму удобным для разработчика. Что является удобным — зависит от самого типа данной игры. Например, для Speebot — это встроенный редактор уровней, который позволяет быстро создавать новые уровни и незамедлительно их  тестировать. ,Решение использовать собственный движок позволяет мне интегрировать мои собственные инструменты таким образом, чтобы мне было проще и быстрее создавать контент. ,Ещё один большой плюс: не нужно соглашаться с условиями лицензии стороннего движка, подписывать договора и платить проценты от прибыли. ,Ну и наконец: написание собственного движка — это очень интересно. ,Разработка движка началась в январе 2016 года. Я назвал его , (""мечта"" по-японски). Он написан на Haxe, C++ и OpenGL. Я использую модифицированную версию библиотеки ,, которая включает в себя несколько полезных функций, например, позволяет загружать ресурсы и открывает доступ к OpenGL. ,До начала создания движка я почти ничего не знал о разработке 3D игр. Нужно было разбираться в OpenGL читая документацию, форумы и уроки, предназначенные для других языков (Java и C++). Через пару месяцев у меня получился довольно стабильный 3D-визуализатор. ,Кроме самого 3D-визуализатора, нужно было с нуля создать множество разных систем: 2D-визуализатор, машину состояний, систему временных шагов (об этом позже), систему интерфейсов, систему управления мышью, клавиатурой и джойстиками, динамические тени, 3D звуковую систему (используя OpenAL), загрузку моделей (в собственном формате,  основанном на ,), ""скелетные"" анимации, иерархию объектов, эффект зеркального отражения в реальном времени, отображения текста и так далее. ,В конце концов, получилась 3D библиотека, которую можно использовать для чего-то конкретного. Многое из того, что я перечислил, присутствует и в других движках, но в YUME есть несколько отличий. Одно из них — система временных шагов. ,Система временных шагов гарантирует, что движок обрабатывает и показывает кадры игры с определённым интервалом. В YUME нет ограничения по количеству кадров в секунду, т.е. нет привязанности к 30 или 60 кадрам в секунду. Частота кадров может быть любая, а игра всё равно будет работать с одной и той же скоростью, потому что частота обновления  логики не связана с частотой обновления экрана. На компьютерах разных мощностей может быть разная производительность, и время для показа одного кадра может быть любым. А логика всегда привязана к частоте 62.5 циклов в секунду (16 миллисекунд на каждый цикл). В этом основной принцип системы временных шагов, хотя есть некоторые крайние случаи. ,В результате: на более слабых компьютерах уменьшается частота кадров в секунду, но на игровой процесс это не влияет. ,У меня была готовая библиотека, но пока ещё не движок. Пришла пора начать разрабатывать игру, и принимать решения о нужных функциях. К тому времени у меня уже было несколько идей, и я знал, что хотел попробовать поэкспериментировать с 3D ""плитками"". ,Я начал создавать систему игровых уровней, которая использовала что-то похожее на 2D плитки (tilemaps), но с одним дополнительным измерением. Получается, что уровень можно сложить из ""кубиков"", как конструктор. Я создал редактор карт, чтобы ускорить процесс создания уровней. В итоге этот редактор попал в финальную версию игры и доступен  каждому игроку. ,Движок автоматически определяет, какую 3D модель использовать для отображения каждой плитки, в зависимости от соседних плиток. Придумал способ, как объединить все плитки в одну общую 3D модель, чтобы видеокарте не нужно было рисовать каждую плитку индивидуально. Вся карта рисуется за один раз, что очень сильно улучшает производительность.  ,Уровни можно редактировать и тестировать сразу. Для продуктивности — то что нужно. ,Я написал простую систему игровой физики и начал экспериментировать с игровым процессом. Через несколько недель был готов первый прототип игры, в котором игрок мог перемещать цилиндр по 3D уровню.,YUME продолжал развиваться параллельно с разработкой Speebot (примерно в это время я выбрал такое название для игры). Было найдено и устранено несколько проблем с архитектурой движка, добавилось несколько новых функций: физика, частицы, отражения на поверхности воды, инструменты для анимации камеры, интерактивные объекты... Постепенно библиотека эволюционировала в игровой движок. ,Я продолжал добавлять новые элементы игрового процесса, оставляя элементы, которые казались мне интересными, и избавляясь от лишнего. После некоторых экспериментов с художественным направлением графики игры я создал персонажа в Blender — маленького робота с одним колесом. Персонаж нарисован и анимирован в мультяшном стиле. ,В это время я уже работал в основном над самой игрой, а не над движком. Я продолжал создавать новые уровни, декорации, персонажи и элементы игры. ,В конце 2016 года я сделал паузу от разработки игры, и посвятил  несколько месяцев изучению и практике написания музыки. У меня нет музыкального образования, но это — моё хобби. У меня уже был небольшой опыт в композиции музыки для моей предыдущей игры ,, но я хотел улучшить свои навыки, поэтому снова погрузился в изучение музыкальной теории. Для композиции музыки я использовал программу ,, которая  является виртуальным модульным синтезатором. В финальной версии игры Speebot всего 23 трека, которые вместе составляют более часа оригинальной музыки. Я удовлетворён результатом, и продолжу улучшать свои навыки для следующей игры. ,После этого я концентрировался на создании новых уровней, добавлении новых игровых элементов, написании музыки и устранении ошибок. Такой подход к разработке игр мне по душе. Если мне становится скучно заниматься одним и тем же делом, я переключаюсь на другой вид деятельности, и поэтому не теряю интереса к разработке игры. ,После выпуска нескольких демо версий, обработки отзывов от игроков и улучшения игры, Speebot был выпущен в октябре 2017 года. В финальной версии игры 200 уровней, 4 мира, редактор пользовательских уровней, несколько дополнительных режимов игры, и много другого дополнительного контента. ,Самое сложное при разработке игры было сохранять мотивацию и интерес на протяжении всех 20 месяцев. Работать над игрой мне удавалось только по вечерам после университета и работы, и по выходным дням. Но всё равно, это было очень интересно. ,Я продолжу использовать и развивать свой движок YUME в своих будущих играх, и уже начал работать над своим следующим проектом. ,После написания этой статьи я разработал и выпустил ещё две игры на этом движке: сюжетная приключенческая игра ,, и игра-головоломка ,.,Индивидуальный разработчик компьютерных игр.",Как я создал собственный 3D движок и игру на нём за 20 месяцев / Хабр
[<200 https://habr.com/ru/post/662566/>],page2,"По материалам статьи Craig Freedman: ,Уровни изоляции транзакций Serializable и Snapshot обеспечивают согласованное чтение из базы данных. На любом из этих уровней изоляции транзакция может читать только зафиксированные данные. Более того, транзакция может читать одни и те же данные несколько раз, не заботясь о каких-либо параллельных транзакциях, вносящих изменения в эти же данные. Те нежелательные эффекты, которые были продемонстрированы в предыдущих статьях при Read Committed и Repeatable Read, на уровнях изоляции Serializable и Snapshot просто невозможны.,Обратите внимание, что я использовал фразу «не заботясь о каких-либо … вносящих изменения». Такой подбор слов является преднамеренным. На уровне изоляции Serializable ядро SQL Server накладывает блокировку диапазона ключей и удерживает её до окончания транзакции. Блокировка диапазона ключей гарантирует, что после того, как транзакция прочитает данные, никакая другая транзакция не сможет изменить эти данные (даже для вставки фантомных строк) до тех пор, пока не завершится транзакция, удерживающая блокировку. На уровне изоляции Snapshot ядро SQL Server не накладывает никаких блокировок. Таким образом, одновременная транзакция может изменять данные, которые уже прочитаны второй транзакцией. Вторая транзакция просто не замечает этих изменений и продолжает использовать старую версию данных.,Уровень изоляции Serializable основан на пессимистическом контроле параллелизма. Он гарантирует согласованность, предполагая, что две транзакции могут пытаться обновить одни и те же данные, и использует блокировки, чтобы гарантировать, что они этого не cделают, но (за счет уменьшения параллелизма) одна транзакция должна ждать завершения другой, и две транзакции могут заблокироваться. Уровень изоляции Snapshot основан на оптимистичном управлении параллелизмом. Это позволяет транзакциям выполняться без блокировок и с максимальным параллелизмом, но может произойти сбой и последующий откат транзакции, если две транзакции одновременно попытаются изменить одни и те же данные.,Как видим, существуют различия между уровнями изоляции Serializable и Snapshot в уровне параллелизма (которого можно достичь), и в наборе возможных проблем (взаимоблокировки и конфликты обновлений).,Рассмотрим, чем работа в Serializable и Snapshot отличаются с точки зрения обеспечиваемой ими изоляции транзакции. При Serializable всё довольно просто. Чтобы результат двух транзакций считался сериализуемым, они должны выполняться в некотором порядке по одной транзакции за раз.,Snapshot не гарантирует такой уровень изоляции. Несколько лет назад , предложил прекрасный пример, демонстрирующий различия этих уровней. Представьте, что у нас есть мешок, содержащий смесь белых и чёрных шаров. Предположим, мы хотим запустить две транзакции. Одна транзакция перекрашивает каждый белый шар в чёрный шар. Вторая транзакция перекрашивает каждый чёрный шар в белый шар. Если мы запускаем эти транзакции с изоляцией Serializable, они будут исполняться поочерёдно. После первой транзакции останется мешок с шарами только одного цвета. После этого вторая транзакция изменит все эти шары на другой цвет. Есть только два возможных исхода: мешок только с белыми шарами или мешок только с чёрными шарами.,Если мы запускаем эти транзакции с изоляцией Snapshot, появляется третий результат, который невозможен при изоляции Serializable. Каждая транзакция может одновременно делать снимок мешка с шарами в том виде, в каком он был до внесения изменений. Теперь одна транзакция находит белые шары и перекрашивает их в чёрные шары. В то же время другие транзакции находят чёрные шары (но только те шары, которые были чёрными, когда мы сделали снимок, а не те шары, которые первая транзакция изменила на чёрные) и перекрашивает их в белый цвет. В результате, мешок будет содержать смесь белых и чёрных шаров. На самом деле, с этим уровнем изоляции мы правильно поменяли цвет каждого шара.,Следующий рисунок иллюстрирует эти различия:,Мы можем продемонстрировать подобное поведение средствами SQL Server. Обратите внимание, что Snapshot изоляция доступна только с SQL Server 2005 и должна быть явно включена для используемой базы данных:,Начнем с создания простой таблицы с двумя строками, обозначающими два шара разных цветов:,Затем в первом сеансе начните транзакцию с уровнем изоляции Snapshot:,Теперь, прежде чем зафиксировать изменения, запустите во втором сеансе следующее:,Наконец, зафиксируйте транзакцию в первом сеансе и проверьте данные в таблице:,Вот какой получился результат:,Как можно видеть, шар 1, который изначально был чёрным, теперь стал белым, а шар 2, который изначально был белым, стал чёрным. Если вы попробуете тот же эксперимент с уровнем изоляции Serializable, одна транзакция будет ждать завершения другой, и, в зависимости от порядка, оба шара будут белыми или чёрными.,DBA",Serializable vs. Snapshot Isolation Level / Хабр
[<200 https://habr.com/ru/post/662549/>],page2,"Погрузитесь глубоко в новую архитектуру React под названием Fiber и узнайте о двух основных фазах нового алгоритма согласования (reconciliation). Мы подробно рассмотрим, как React обновляет состояние и пропсы и обрабатывает дочерние элементы.,React - это JavaScript библиотека для создания пользовательских интерфейсов. В ее основе лежит ,, который отслеживает изменения в состоянии компонента и проецирует обновленное состояние на экран. В React мы знаем этот процесс как , (reconciliation). Мы вызываем метод ,, фреймворк проверяет, изменилось ли состояние или пропс, и перерендеривает компонент в UI.,Документация React предоставляет , механизма: роль элементов React, методы жизненного цикла и метод ,, а также алгоритм диффиринга (сравнения, diffing), применяемый к дочерним элементам компонента. Дерево иммутабельных элементов React, возвращаемых методом ,, обычно называют ""виртуальный DOM"". Этот термин помог объяснить React людям в самом начале, но он также вызвал путаницу и больше не используется в документации по React. В этой статье я буду называть его деревом React-элементов.,Помимо дерева React-элементов, фреймворк всегда имел дерево внутренних экземпляров (компонентов, узлов DOM и т.д.), используемых для хранения состояния. Начиная с версии 16, React развернул новую реализацию этого дерева внутренних экземпляров и алгоритма, который управляет им, под кодовым названием ,. Чтобы узнать о преимуществах архитектуры Fiber, ознакомьтесь с ,.,Это первая статья из цикла, цель которого - научить вас внутренней архитектуре React. В этой статье я хочу предоставить углубленный обзор важных концепций и структур данных, имеющих отношение к алгоритму. Как только мы получим достаточную базу, мы изучим алгоритм и основные функции, используемые для обхода и обработки fiber-дерева. В следующих статьях цикла будет показано, как React использует алгоритм для выполнения начального рендеринга и обработки обновлений состояния и пропсов. Далее мы перейдем к деталям планировщика, процессу согласования (reconciliation) дочерних элементов и механизму построения списка эффектов.,Здесь я собираюсь дать вам довольно продвинутые знания. Я рекомендую вам прочитать ее, чтобы понять магию, скрывающуюся за внутренними механизмами Concurrent React. Эта серия статей также послужит вам отличным руководством, если вы планируете начать вносить свой вклад в React. Я ,, поэтому здесь будет много ссылок на исходники недавней версии 16.6.0.,Это, безусловно, довольно много, так что не переживайте, если вы не поймете что-то сразу. Это займет время, как и все стоящее. ,Вот простое приложение, которое я буду использовать на протяжении всей серии. У нас есть кнопка, которая просто увеличивает число, отображаемое на экране:,А вот и реализация:,Вы можете поиграть с ним ,. Как вы можете видеть, это простой компонент, который возвращает два дочерних элемента , и , из метода ,. Как только вы нажимаете на кнопку, состояние компонента обновляется внутри обработчика. Это, в свою очередь, приводит к обновлению текста элемента ,.,Во время , (reconciliation) React выполняет различные действия. Например, вот операции высокого уровня, которые React выполняет во время первого рендеринга и после обновления состояния в нашем простом приложении:,обновляет свойство , в , в ,.,извлекает и сравнивает дочерние элементы , и их пропсы,обновляет пропсы элемента ,Есть и другие действия, выполняемые во время ,, такие как вызов , или обновление ,. , Тип работы обычно зависит от типа элемента React. Например, для классового компонента React должен создать экземпляр, в то время как для функционального компонента он этого не делает. Как вы знаете, в React есть много видов элементов, например, классовые и функциональные компоненты, компоненты-хосты (DOM узлы), порталы и т. д. Тип элемента React определяется первым параметром функции ,. Эта функция обычно используется в методе , для создания элемента.,Прежде чем приступить к изучению действий и основного fiber алгоритма, давайте сначала познакомимся со структурами данных, используемыми внутри React.,Каждый компонент в React имеет UI-представление, которое мы можем назвать представлением или шаблоном, возвращаемым методом ,. Вот шаблон для нашего компонента ,:,Когда шаблон проходит через JSX-компилятор, в итоге вы получаете набор React-элементов. Это то, что действительно возвращается из метода , компонентов React, а не HTML. Поскольку от нас не требуется обязательно использовать JSX, метод , для нашего компонента , можно переписать следующим образом:,Вызовы , в методе , создадут две структуры данных:,Вы можете видеть, что React добавляет свойство , к этим объектам, чтобы однозначно идентифицировать их как React элементы. Затем у нас есть свойства ,, , и ,, которые описывают элемент. Значения берутся из того, что вы передаете в функцию ,. Обратите внимание, как React представляет текстовое содержимое в качестве дочерних элементов узлов , и ,. А обработчик клика является частью пропсов элемента ,. Существуют и другие поля в элементах React, например, поле ,, которые выходят за рамки этой статьи.,У элемента React созданного для , нет ни пропсов, ни ключа:,Во время , (reconciliation) данные каждого React элемента, возвращенные из метода ,, объединяются в дерево fiber-узлов. Каждый React элемент имеет соответствующий fiber-узел. В отличие от React элементов, fibers не создаются заново при каждом рендере. Это мутабельные структуры данных, которые хранят состояние компонентов и DOM.,Ранее мы обсуждали, что в зависимости от типа React элемента фреймворк должен выполнять различные действия. В нашем примере приложения для классового компонента , он вызывает методы жизненного цикла и метод ,, в то время как для хост-компонента , (DOM узел) он выполняет мутацию DOM. Таким образом, каждый элемент React преобразуется в Fiber-узел ,, который описывает работу, которую необходимо выполнить.,.,Когда элемент React впервые преобразуется в fiber-узел, React использует данные из элемента для создания fiber в функции ,. При последующих обновлениях React повторно использует fiber-узел и просто обновляет необходимые свойства, используя данные из соответствующего React-элемента. React также может потребоваться переместить узел в иерархии на основе пропсов , или удалить его, если соответствующий элемент React больше не возвращается из метода ,.,Посмотрите функцию ,, чтобы увидеть список всех действий и соответствующих функций, которые React выполняет для существующих fiber-узлов.,Поскольку React создает fiber для каждого React элемента, и поскольку у нас есть дерево этих элементов, у нас будет дерево fiber-узлов. В случае с нашим примером приложения это выглядит следующим образом:,Все fiber-узлы связаны между собой через связный список (linked list), используя следующие свойства fiber-узлов: ,, , и ,. Более подробно о том, почему это работает именно так, читайте в моей статье ,, если вы еще не читали ее.,После первого рендеринга React заканчивает работу с fiber-деревом, которое отражает состояние приложения, использованного для отрисовки пользовательского интерфейса. Это дерево часто называют , (current). Когда React начинает работать над обновлениями, он строит так называемое , дерево, которое отражает будущее состояние, которое будет выведено на экран.,Вся работа выполняется над fibers из дерева ,. Когда React проходит через , дерево, для каждого существующего fiber-узла он создает альтернативный узел, который составляет ,дерево. Этот узел создается с использованием данных из элементов React, возвращаемых методом ,. Как только обновления будут обработаны и вся связанная с ними работа будет завершена, React получит альтернативное дерево, готовое к выводу на экран. Как только это , дерево будет выведено на экран, оно станет , деревом.,Один из основных принципов React - последовательность. React всегда обновляет DOM за один раз - он не показывает частичные результаты. Дерево , служит в качестве ""черновика"", который не виден пользователю, чтобы React мог сначала обработать все компоненты, а затем вывести их изменения на экран.,В исходных текстах вы увидите множество функций, которые берут fiber-узлы из обоих деревьев , и ,. Вот сигнатура одной из таких функций:,Каждый fiber-узел содержит ссылку на свой аналог из другого дерева в поле ,. Узел из , дерева указывает на узел из дерева , и наоборот.,Мы можем думать о компоненте в React как о функции, которая использует состояние и пропсы для вычисления UI-представления. Любые другие действия, такие как мутирование DOM или вызов методов жизненного цикла, следует рассматривать как побочный эффект или, просто, эффект. Эффекты также упоминаются ,:,Вероятно, вам уже приходилось выполнять выборку данных, подписку или , из React компонентов. Мы называем эти операции ""побочными эффектами"" (или сокращенно ""эффектами""), потому что они могут повлиять на другие компоненты и не могут быть выполнены во время рендеринга.,Вы можете видеть, как большинство обновлений состояния и пропсов приводят к побочным эффектам. А поскольку применение эффектов - это один из видов работы, узел fiber - это удобный механизм для отслеживания эффектов в дополнение к обновлениям. Каждый узел волокна может иметь эффекты, связанные с ним. Они кодируются в поле ,.,Таким образом, эффекты в Fiber в основном определяют ,, которую необходимо выполнить для экземпляров после обработки обновлений. Для хост компонентов (DOM элементов) работа заключается в добавлении, обновлении или удалении элементов. Для классовых компонентов React может потребоваться обновление refs и вызов методов жизненного цикла , и ,. Существуют также другие эффекты, соответствующие другим типам fiber-ов.,React обрабатывает обновления очень быстро, и для достижения такого уровня производительности он использует несколько интересных приемов. , Итерация линейного списка намного быстрее, чем дерева, и нет необходимости тратить время на узлы без побочных эффектов.,Цель этого списка - пометить узлы, которые имеют DOM обновления или другие эффекты, связанные с ними. Этот список является подмножеством дерева , и связан посредством свойства , вместо свойства ,, используемого в деревьях , и ,., предложил аналогию для списка эффектов. Ему нравится думать о нем как о рождественской елке, с ""рождественскими огнями"", связывающими все узлы эффектов вместе. Чтобы визуализировать это, давайте представим следующее дерево из fiber-узлов, где выделенные узлы выполняют определенную работу. Например, в результате нашего обновления , был вставлен в DOM, , и , изменили атрибуты, а , запустил метод жизненного цикла. Список эффектов свяжет их вместе, чтобы React мог пропустить другие узлы позже:,Вы можете видеть, как узлы с эффектами связаны друг с другом. При переходе по узлам React использует указатель ,, чтобы определить, с чего начинается список. Таким образом, приведенную выше диаграмму можно представить в виде линейного списка следующим образом:,В каждом React-приложении есть один или несколько элементов DOM, которые выступают в качестве контейнеров. В нашем случае это элемент , с ID ,.,React создает объект , для каждого из этих контейнеров. Вы можете получить к нему доступ, используя ссылку на DOM элемент:,Этот fiber root является местом, где React хранит ссылку на fiber tree. Она хранится в свойстве , fiber root:,Fiber-дерево начинается со , fiber-узла, которым является ,. Он создается внутри и действует как родитель для вашего самого верхнего компонента. Существует связь от fiber-узла , обратно к , через свойство ,:,Вы можете изучить fiber-дерево, обратившись к самому верхнему fiber-узлу , через fiber root. Или вы можете получить отдельный fiber-узел из экземпляра компонента следующим образом:,Теперь рассмотрим структуру fiber-узлов, созданных для компонента ,:,и DOM-элемента ,:,В fiber-узлах довольно много полей. Я описал назначение полей ,, , и , в предыдущих разделах. Теперь давайте посмотрим, зачем нам нужны другие.,Хранит ссылку на экземпляр класса компонента, узла DOM или другой тип React элемента, связанный с fiber-узлом. В общем, можно сказать, что это свойство используется для хранения локального состояния, ассоциированного с fiber.,Определяет функцию или класс, связанный с этим fiber. Для классовых компонентов оно указывает на функцию-конструктор, а для DOM элементов - на HTML-тег. Я довольно часто использую это поле, чтобы понять, с каким элементом связан fiber-узел.,Определяет ,. Он используется в алгоритме согласования (reconciliation), чтобы определить, какую работу нужно выполнить. Как упоминалось ранее, работа варьируется в зависимости от типа React элемента. Функция , сопоставляет элемент React с соответствующим типом fiber-узла. В нашем приложении свойство , для компонента , равно ,, что обозначает ,, а для элемента , - ,, что обозначает ,.,Очередь обновлений состояния, обратных вызовов и обновлений DOM.,Состояние fiber-a, которое было использовано для создания вывода. При обработке обновлений оно отражает состояние, которое в данный момент выводится на экран.,Пропсы fiber-а, которые были использованы для создания вывода во время предыдущего рендера.,Пропсы, которые были обновлены на основе новых данных в React элементах и должны быть применены к дочерним компонентам или DOM элементам.,Уникальный идентификатор с группой дочерних элементов, помогающий React выяснить, какие элементы изменились, были добавлены или удалены из списка. Это связано с функциональностью ""списки и ключи"" React, описанным ,.,Полную структуру fiber-узла можно найти ,. В приведенном выше объяснении я пропустил кучу полей. В частности, я пропустил указатели ,, , и ,, составляющие древовидную структуру данных, которую я ,. И пропустил категорию полей, таких как ,, , и ,, которые специфичны для ,.,React выполняет работу в двух основных фазах: , и ,.,Во время первой фазы , React применяет обновления к компонентам, запланированные через , или ,, и выясняет, что нужно обновить в пользовательском интерфейсе. Если это первоначальный рендеринг, React создает новый fiber-узел для каждого элемента, возвращенного из метода ,. При последующих обновлениях fiber-ы для существующих React элементов используются повторно и обновляются. , Эффекты описывают работу, которая должна быть выполнена во время следующей фазы ,. Во время этой фазы React берет fiber-дерево, помеченное эффектами, и применяет их к экземплярам. Он просматривает список эффектов и выполняет обновления DOM и другие изменения, видимые пользователю., фазы может выполняться асинхронно. React может обработать один или несколько fiber-узлов в зависимости от доступного времени, затем остановиться, чтобы сохранить проделанную работу и уступить какому-либо событию. Затем он продолжает работу с того места, где остановился. Иногда, однако, может потребоваться отбросить проделанную работу и начать все сначала. Такие паузы возможны благодаря тому, что работа, выполняемая в этой фазе, не приводит к каким-либо видимым пользователю изменениям, таким как обновление DOM. , фаза всегда синхронна. Это происходит потому, что работа, выполняемая на этом этапе, приводит к изменениям, видимым пользователю, например, к обновлению DOM. Поэтому React должен выполнять её за один проход.,Вызов методов жизненного цикла - это один из видов работы, выполняемой React. Некоторые методы вызываются на этапе ,, а другие - на этапе ,. Вот список методов жизненного цикла, вызываемых при выполнении первой фазы ,:,[UNSAFE_]componentWillMount (deprecated),[UNSAFE_]componentWillReceiveProps (deprecated),getDerivedStateFromProps,shouldComponentUpdate,[UNSAFE_]componentWillUpdate (deprecated),render,Как вы можете видеть, некоторые устаревшие методы жизненного цикла, выполняемые на этапе ,, помечены как , с версии 16.3. В документации они теперь называются legacy lifecycles. Они будут устаревшими в будущих релизах 16.x, а их аналоги без префикса , будут удалены в версии 17.0. Подробнее об этих изменениях и предлагаемом пути миграции вы можете прочитать ,.,Вам интересно узнать причину этого?,Ну, мы только что узнали, что поскольку фаза , не производит побочных эффектов, таких как обновление DOM, React может обрабатывать обновления компонентов асинхронно (потенциально даже делая это в несколько потоков).Однако жизненные циклы, помеченные ,, часто понимались неправильно и часто использовались не по назначению. Разработчики склонны помещать код с побочными эффектами внутрь этих методов, что может вызвать проблемы с новым подходом к асинхронному рендерингу. Хотя будут удалены только их аналоги без префикса ,, они все еще могут вызвать проблемы в предстоящем Concurrent Mode (от которого вы можете отказаться).,Вот список методов жизненного цикла, выполняемых во время второй фазы ,:,getSnapshotBeforeUpdate,componentDidMount,componentDidUpdate,componentWillUnmount,Поскольку эти методы выполняются в синхронной фазе ,, они могут содержать побочные эффекты и затрагивать DOM.,Итак, теперь у нас есть предпосылки, чтобы взглянуть на обобщенный алгоритм, используемый для обхода дерева и выполнения работы. Давайте погрузимся внутрь.,Алгоритм согласования (reconciliation) всегда начинается с самого верхнего fiber-узла , с помощью функции ,. Например, если вы вызовете , глубоко в дереве компонентов, React начнет с вершины, но быстро пропустит родительские узлы, пока не доберется до компонента, у которого был вызван метод ,.,Все fiber-узлы обрабатываются , (work loop). Вот реализация синхронной части цикла:,В приведенном выше коде переменная , хранит ссылку на fiber-узел из дерева ,, в котором еще есть незавершенная работа. Когда React обходит fiber-дерево, он использует эту переменную, чтобы узнать, есть ли еще какой-нибудь fiber-узел с незавершенной работой. После обработки текущего fiber переменная будет содержать либо ссылку на следующий fiber-узел в дереве, либо ,. В этом случае React выходит из рабочего цикла и готов закоммитить изменения.,Существует 4 основные функции, которые используются для обхода дерева и инициирования или завершения работы:,Чтобы продемонстрировать, как они используются, посмотрите на следующую анимацию обхода fiber-дерева. Для демонстрации я использовал упрощенную реализацию этих функций. Каждая функция берет для обработки fiber-узел, и по мере того, как React спускается по дереву, вы можете видеть, как меняется текущий активный fiber-узел. На анимации хорошо видно, как алгоритм переходит от одной ветви к другой. Сначала он завершает работу для дочерних узлов, а затем переходит к родительским.,Обратите внимание, что прямые вертикальные связи обозначают сиблингов (братьев и сестер), а изогнутые - детей, например, у , нет детей, а у , есть один ребенок ,.,, где можно приостановить воспроизведение и просмотреть текущий узел и состояние функций. Концептуально, вы можете думать о ""begin"" как о ""шаге в"" компонент, а о ""complete"" как о ""шаге из"" него. Вы также можете ,, пока же я объясню, что делают эти функции.,Давайте начнем с первых двух функций , и ,:,Функция , получает fiber-узел из дерева , и начинает работу, вызывая функцию ,. Это функция, которая запускает все действия, которые должны быть выполнены для fiber-а. Для целей данной демонстрации мы просто записываем в лог имя fiber-а, чтобы обозначить, что работа была выполнена. , всегда возвращает указатель на следующего ребенка для обработки в цикле или ,.,Если есть следующий ребенок, он будет присвоен переменной , в функции ,. Однако если дочернего элемента нет, React знает, что достиг конца ветви, и поэтому может завершить текущий узел. , Это делается в функции ,:,Вы можете видеть, что весь код функции заключается в большом цикле ,. React попадает в эту функцию, когда у узла , нет дочерних элементов. После завершения работы для текущего fiber-а, он проверяет, есть ли у него дочерний узел. Если он найден, React выходит из функции и возвращает указатель на сиблинга. Он будет присвоен переменной ,, и React выполнит работу для ветви, начинающейся с этого сиблинга. Важно понимать, что на данный момент React выполнил работу только для предшествующих сиблингов. Он не выполнил работу для родительского узла. ,.,Как видно из реализации, обе функции и , используются в основном для целей итерации, в то время как основная деятельность происходит в функциях , и ,. В следующих статьях цикла мы узнаем, что происходит для компонента , и узла ,, когда React переходит к функциям , и ,.,Фаза начинается с функции ,. Именно здесь React обновляет DOM и вызывает методы жизненного цикла до и после мутации.,Когда React доходит до этой фазы, у него есть 2 дерева и список эффектов. Первое дерево представляет состояние, которое в настоящее время отображается на экране. Затем есть альтернативное дерево, построенное во время фазы ,. Оно называется , или , в источниках и представляет состояние, которое должно быть отражено на экране. Это альтернативное дерево связано с текущим деревом через указатели , и ,.,И затем, есть список эффектов - подмножество узлов из дерева ,, связанное через указатель ,. Помните, что список эффектов - это , выполнения фазы ,. Весь смысл рендеринга в том, чтобы определить, какие узлы должны быть вставлены, обновлены или удалены, и какие компоненты должны вызвать свои методы жизненного цикла. И именно об этом нам говорит список эффектов. ,В целях отладки доступ к , дереву можно получить через свойство , fiber-корня. Доступ к дереву , можно получить через свойство , узла , в текущем дереве.,Основной функцией, выполняемой на этапе commit, является ,. В основном, она делает следующее:,Вызывает метод жизненного цикла , на узлах, помеченных эффектом ,.,Вызывает метод , жизненного цикла для узлов, помеченных эффектом ,.,Выполняет все вставки, обновления и удаления в DOM,Устанавливает дерево , в качестве текущего.,Вызывает метод жизненного цикла , на узлах, помеченных эффектом ,.,Вызывает метод , жизненного цикла для узлов, помеченных эффектом ,.,После вызова метода предварительной мутации ,, React фиксирует все побочные эффекты внутри дерева. Он делает это в два прохода. Первый проход выполняет все вставки, обновления, удаления и размонтирования DOM (хоста). Затем React назначает дерево , на ,, помечая дерево , как ,. Это делается после первого прохода фазы commit, чтобы предыдущее дерево было актуальным во время ,, но до второго прохода, чтобы законченная работа была актуальной во время ,. Во время второго прохода React вызывает все остальные методы жизненного цикла и обратные вызовы. Эти методы выполняются как отдельный проход, так что все размещения, обновления и удаления во всем дереве уже были вызваны.,Вот сниппет функции, которая выполняет описанные выше шаги:,Каждая из этих подфункций реализует цикл, который итерирует список эффектов и проверяет их тип. Когда он находит эффект, соответствующий цели функции, он применяет его.,Вот, например, код, который выполняет итерацию по дереву эффектов и проверяет, имеет ли узел эффект ,:,Для классового компонента это действие означает вызов метода жизненного цикла ,., - это функция, с помощью которой React выполняет обновление DOM. Функция в основном определяет тип операции, которую необходимо выполнить для узла, и выполняет ее:,Интересно, что React вызывает метод , как часть процесса удаления в функции ,., - это функция, в которой React вызывает все оставшиеся методы жизненного цикла , и ,.,Наконец-то мы закончили. Дайте мне знать, что вы думаете о статье или задайте вопросы в комментариях. , У меня в работе еще много статей, в которых подробно рассказывается о планировщике, процессе согласования детей и о том, как строится список эффектов. Я также планирую создать видео, в котором покажу, как отлаживать приложение, используя эту статью в качестве основы.,Web-разработчик, Android-разработчик",Fiber изнутри: погружение в новый алгоритм согласования React / Хабр
[<200 https://habr.com/ru/post/662559/>],page2,"Это гифка, которую я сделал, чтобы показать вступление и как началась история путешествия птички. У меня есть друг, который не боится рисовать, даже если он не обучался рисованию профессионально. Я общаясь с ним как то вдохновился желанием рисовать и не бояться. В google play у меня есть старая игра, которую я делал на unity, когда только начинал работать с движком.,Два комментария к старой игре дали мне желание сделать новую версию, но уже на C++ + SDL2 + OPENGL ES 3.2 + OPENSLES + glm. То есть я даже рад хотя бы двум комментариям о том что людям нравиться моё творчество, чтобы чувствовать себя прекрасно и продолжать делать игры.,Так как у меня нормального опыта не было делать игры полноценные на sdl2, то я использовал разные виды кода, которые как я думал, что они правильные. Но поработав на работе и изучая код, я увидел что есть помимо того что я знаю (я про очереди сообщений), есть ещё mqueue. И только потом я додумался, что можно с помощью очередей сообщений отправлять из одного потока в другой что-нибудь. Вот пример как выглядела реализация.,Перед тем как использовать эту очередь, я удостоверился в том, что в android ndk есть заголовочный файл mqueue.,Я также посмотрел, есть ли OpenAL для android и оказалось, что она не входит в комплект и как почитал в интернете, что лучше писать для android на OpenSLES.,После того как были готовы рисунки для интро, я начал писать код для работы с загрузкой текстур и размещением вершин. В процессе работы я заметил, что можно отражать одну сторону в другую путем замены координат у вершинных буферов. Вот пример как выглядит начальная конфигурация вершин.,Я сначала думал что в unity делают правильно, что отсчитывают от центра и исходил из этого и определял экран как.,вроде такой был код.,Позже я понял неудобство и решил отсчитывать от левого верхнего угла и сделал так.,Вообще из-за того, что у меня нет большого опыта в разработке движков, то я наверное делаю ошибки такие, какие делают начинающие разработчики. Например конструктор спрайта выглядит так.,Вообще когда говорят что глобальные переменные это зло, то я думаю что они просто это от кого то услышали и приняли для себя такое же мнение, но мне например не удобно как оказалось передавать объект Common в конструктор. Лучше бы я просто пробросил с помощью extern размеры экрана и всё было бы чище. Да и ещё я по рассуждал, что можно для каждого шейдера отдельный класс создать, чтобы каждый спрайт заново не получал с помощью glGetUniformLocation позиции в шейдере. То есть после компиляции шейдера можно было бы получить все позиции и для спрайта указать например интерфейс к шейдеру или что нибудь подобное, чтобы просто уже было работать. Да и класс шейдера можно было бы интегрировать со спрайтом так, чтобы в рендере спрайта не менять ничего, если ты сменил шейдер. Хотя может я ошибаюсь, но я проработаю этот вопрос.,Еще я столкнулся с проблемами неправильного размера картинки. Спрайты были, одни короче, другие длиннее. Но я путем проб и ошибок выработал правило.,Вроде бы получилось правильно.,Для загрузки объектов я создал заголовок такого типа.,И если нужно загрузить какой то объект, то мы просто получаем на него ссылку, если он уже был загружен.,Да, можно было с помощью текста указывать какой объект загружать, но мне так больше нравиться, и нравиться еще из-за того, что легко получить эту ссылку на объект, если он уже был загружен. В link содержится все vao, vbo[2] и номера всех текстур.,Главное меню игры я сделал из одного спрайта, но на экране изображено две птицы. В момент рендера я отражаю спрайт по горизонтали и рисую в разных частях экрана. Вот как я составил код.,Оказалось не так уж и сложно отражать объект. Также можно отразить по вертикали, например поменяв местами координаты текстуры.,По OpenAL писать нечего, я сделал музыку специально для 44100 частоты и 16 битного формата вроде. По OpenSLES я скачал спецификацию и почитал немного, понял что надо посмотреть примеры реализации и банально переписал код, чтобы заработало на android.,При портировании на android как оказалось, что там нет mqueue реализации. Я нашел только syscall от ядра linux. Но если был syscall для открытия mq_open, то syscall для отправки не было и я подумал что надо искать другое решение. Так как я больше на C писал и на C++ опыта мало, то я конечно же не знал, что в C++ есть контейнер queue. И это было спасением, я сделал её глобальной рядом с функцией main и sdl потоке отправлял в нее event. А в game () файле я пробросил queue с помощью extern и получал события. И вуаля, всё работает.,Так как архитектуры различны, то я просто в ресурс добавил число 1. Если при прочитывании этой переменной, она не равно единице, то делаем смену из littleEngian в bigEngian.,Насчет шрифта freetype2. Я использовал старую сборку freetype, которая у меня на github, потому что новую так и не смог собрать для android.,Также, чтобы скомпилировать с OpenGLESv3, надо обратить внимание, что в ndk библиотеки с такой версией есть не ниже 18 api. Чтобы решить все проблемы с компиляцией, нужно в каталоге app в файле build.gradle сделать типа такого.,Важно в ndkBuild тоже указать платформу назначение и тогда компиляция сработает.,Ну и указать в app/jni/Application.mk версию api не забыть.,Учитывая прошлый опыт, я не стал на каждую игру заводить отдельный паблик, а сделал один основной и назвал - игры от xverizex. ,Игра, которую я написал, можно найти по кодовому названию в google play.,com.xverizex.fly_bird_2,Правда я всё ещё жду пока одобрят первую версию и пока она не доступна в маркете. Я хочу сделать её бесплатной в google play, а в huawei маркете, если это вообще возможно, то выставить цену на игру. Хотелось бы ещё зарабатывать на том что нравиться.,Игра по своей сути получилась относительно простой и поэтому её возможно было сделать за 5 дней. Да, на unity можно было бы за дня два или один сделать, но мне нравиться C и C++, разумеется я буду писать на том что мне нравиться. ,Это были мои все заметки, которые я запомнил за прошедшие пять дней разработки. Я писал по 12 или более часов почти каждый день и не мог уснуть, потому что было интересно. Но теперь нужно отдохнуть перед следующим заходом. Возможно новый уровень в этой игре или новая игра. ,Разработчик",Как я разрабатывал игру fly bird 2 / Хабр
[<200 https://habr.com/ru/company/timeweb/blog/649365/>],page2,Любопытный к миру человек,"20 игр, чтобы видеть детали, чувствовать нюансы и уловить смысл дизайна / Хабр"
[<200 https://habr.com/ru/post/662527/>],page2,Пользователь,"Установка, настройка и эксплуатация стэка OpenSearch в классической среде / Хабр"
[<200 https://habr.com/ru/post/662596/>],page2,"Не нужно быть сверхразумным искусственным интеллектом, чтобы понять: двигаться навстречу величайшему событию в истории человечества и не готовиться к этому – просто глупо.,Непросто так эти слова являются эпиграфом к данной статье, ведь в век информационных технологий и проходящей незаметно для всех обывателей технической революции только слепой не заметит насколько стремительно проходит процесс компьютеризации и роботизации с внедрением искусственного интеллекта во все сферы деятельности человека.,Увеличение мощности компьютеров, уменьшение их размеров и снижение цены производства – следствия так называемого «закона Мура», который был сформулирован больше пятидесяти лет назад , – тогда еще будущем основателем компании Intel.,На самом деле это просто эмпирическое, то есть основанное на опыте наблюдение: мощность компьютеров, обусловленная увеличением количества транзисторов, уменьшающихся на кристалле интегральной платы, а также ростом их таковой частоты, удваивается каждые 18 месяцев. Тогда как с ценой происходит обратная ситуация – примерно каждые два года она в два раза уменьшается. ,Журнал «Scientific American» привёл такую аналогию: если бы авиапромышленность последние 25 следовала «закону Мура», то сейчас Boeing 767 стоил бы 500 долларов, совершал облет земного шара за 20 минут и затрачивал на это менее 20 литров топлива,Да, цифровой мир живет по своим закона Мура и Курцвейла.,Говоря о стремительном развитии информационных технологий нельзя не сказать уже о вышеупомянутом американском ученном ,, который известен научными технологическими прогнозами, учитывающими появление искусственного интеллекта и средств радикального продления жизни людей. Он придумал так называемый закон ускоренной отдачи, согласно которой развитие технологий происходит экспоненциально, то есть чем мощнее становится та или иная технология, тем большее ускорение в своем развитии она приобретает.,Для большего понимания достаточно вспомнить притчу о шахматах, согласно которой мудрец, изобретший шахматы, по имени Сисса, в качестве награды у короля попросил за первую клетку одно зерно пшеницы, за вторую – два, за третью – четыре и так далее. Нетерпеливый король не задумываясь согласился на условия мудреца, но количество зерна превысило весь урожай пшеницы, собранный за всю историю человечества. Эта простенькая математическая задача демонстрирует высокий рост экспоненциальной последовательности.,Получив представление об окружающей нас обстановке и помечтав о прекрасном будущем, в котором нас будут окружать технологии, способствующие увеличению среднего возраста жизни, о мире в котором нет болезней, в котором нам стоит лишь подумать о чем-либо, а это уже действительность, вернемся в реальность.,Touch ID, Face ID – эти слова уже на слуху у всех, но вы когда-нибудь задумывались как они могут быть использованы против вас? Мы не задумываясь указываем их везде и повсеместно, но не стоит забывать, что существуют еще идентификации по радужной оболочке глаз, геометрии руки, томографии лица, ДНК, акустических характеристик уха, рисунку вен. Такие данные создают полный портрет их носителя и выкладывают на блюдечке всю информацию о человеке.  ,В последнее время очень много людей говорят о том, что банки собирают биометрические данные своих клиентов. Это позволит клиентам, во-первых, пользоваться банковскими услугами через интернет, во-вторых, банк обещает, что скоро их банкоматы будут узнавать своих клиентов в лицо. Также постепенно роботы заменят сотрудников операционистов во всех его отделениях, а для того, чтобы робот смог работать с клиентом нужны биометрические данные этого клиента. В итоге уже сейчас сотрудники многих банков сообщают, что это для вашего блага и удобства. Они пытаются всяческим путем хотят взять биометрические персональные данные - образец вашего голоса и изображения лица. Да-да, именно всяческими путями, об этом подробнее расскажу далее. Желание банков автоматизировать работу с клиентами и заменить банковских работников способных совершать ошибки на автоматы вполне понятно, однако проблема в том, что люди ещё не привыкли доверять банкам и тем более если речь идет о предоставлению банку своих биометрических данных. Люди опасаются, что такие данные банки могут передавать третьим лицам, например, коллекторам. Банки могут утерять такие данные и они утекут в открытый доступ, как было недавно с базами данных клиентов нескольких крупных банков страны. В конце концов с прогрессом дружат не только банки, но от него не отстают, а иногда и идут на шаг впереди и мошенники. Например, банки хотят в ближайшем будущем выдавать кредиты через интернет с распознаванием обратившегося за кредитом человека по его биометрическим данным, если такой человек предоставлял любому банку такие данные ранее при оформлении кредита. Но возможно и здесь мошенники преуспеют и научатся подделывать такие данные, поэтому люди испытывают некоторые опасения.,Биометрия и искусственный интеллект, как мы уже поняли, непосредственно связаны друг с другом. С каждым годом становится все больше данных о клиентах банка, они растут в геометрической прогрессии. С каждой новой порцией ценность отдельно взятой изолированной информации снижается. Иными словами, сигналов становится больше, но при этом каждый из них все слабее. Лишь анализ их совокупности позволит выработать сильные сигналы. Поэтому рано или поздно искусственный интеллект станет неотъемлемой частью.,Теперь давайте рассмотрим такой вопрос. Можно ли отказаться от предоставления биометрии банку и как это может повлиять на позицию банка по выдаче кредита карты или оформление другого банковского продукта? Сегодня иногда бывает так, человек приходит в банк за оформлением документов и его никто не спрашивает «согласен ли гражданин пройти процедуру сдачи биометрии или нет?». На него просто направляют камеру и просят сказать несколько слов. Конечно же такие действия сотрудника банка незаконны в соответствии с законом номер 115 «О противодействии легализации доходов, полученных преступным путем», банкам разрешается размещать и обновлять биометрические персональные данные гражданина в электронной базе, но только при его добровольном согласии на такую процедуру. Но сотрудник банка может сумничать и сказать, что вы уже раньше давали согласие на передачу своих персональных данных банку, однако персональные данные и биометрические данные это разные вещи. Первое это сведение вашего паспорта, а второе это ваше отображение на фото, видео, отпечатки ваших пальцев, радужка оболочки глаз, образцы волос и другие биологические сведения. Поэтому если даже клиент и подписал ранее договор, где есть пункт, что он согласен на передачу, обработку банком персональных данных, у банка все равно нет права записывать биометрические сведения такого человека. Если обратить внимание на статью 11 федерального закона о персональных данных, то в ней конкретизировано, что получать биометрические данные можно только с письменного согласия человека, то есть прежде чем фотографировать, снимать вас на видео и записывать ваш голос сотрудник банка должен показать вам в договоре пункт в котором указано то, что вы согласны предоставить банку свои биометрические данные и только после подписания такого договора и тем самым вашего согласия, на вас уже могут нацеливать объектив камеры. В случае нарушения вашего права и получения банком вашего изображения без вашего письменного согласия, можно обратиться в суд с исковым заявлением о возмещении морального вреда, причиненного незаконными и непрофессиональными действиями банка. ,Таким образом, действуя на опережение, государство должно задуматься о создании действенных механизмов защиты разрабатываемых геномных баз данных, в том числе путем совершенствования правовой системы. Защита же генетической информации и генетических баз данных, гарантирующая их безопасность от всякого рода посягательств, должна осуществляться именно на государственном уровне под контролем соответствующих государственных органов.,Да, это конечно всё понятно, что цепочка человек – биометрические данные – человек может контролироваться законом, но давайте рассмотрим такую цепочку как человек – биометрические данные – робот. Задумались? Может и при нынешнем уровне развития искусственного интеллекта человечеству не грозит восстание машин. Более актуальной угрозой представляется применение недостаточно обученного искусственного интеллекта в жизненно важных сферах, а также попытки его использования для того, чтобы поставить под контроль жизнь человека. ,В будущем может такая угроза, как самостоятельного принятия решений искусственным интеллектом. Ситуация заключается в том, что, когда система получит модель самой себя, то у нас появится искусственный интеллект, который может отказаться выполнять поставленную ему задачу и начнет принимать собственные решения. Для этого необходимо заранее предусмотреть все меры, препятствующие выходу искусственного интеллекта из-под контроля.,Не менее утешительные прогнозы совсем недавно продемонстрировала нейросеть,MT-NLG, разработанная Оксфордским университетом, которая в свою очередь заявила, что: «Способность предоставлять информацию, а не товары и услуги, станет определяющей чертой экономики 21 века. Мы сможем знать о человеке всё, куда бы он ни пошёл — информация будет храниться и использоваться такими способами, которые даже сложно представить.» ,«Не стоит недооценивать искусственный интеллект!» – это главный вывод. По сути мышление – поведенческий навык, а всякий навык имеет свойство утрачиваться, что касается человеческого мозга. Никто не отменял принцип «мягкой силы», используемый искусственным интеллектом на невидимом поле боя, поэтому подытожив все вышеперечисленное, скажу, что жизнь человеческого рода полностью лежит на его же плечах, с какой долей ответственности человек подойдет к своему развитию, так он и будет существовать. ,Пользователь",Друг или враг? Стоит ли бояться искусственного интеллекта? / Хабр
[<200 https://habr.com/ru/post/662576/>],page2,"Привет, дорогой друг! Сегодня я продолжу рассказывать про поиск флага на Hackthebox. Забегу немного вперед: данная статья будет очень короткой. Мне довольно легко и быстро удалось найти флаг.,Итак, само задание:,По-русски: нас просят найти в социальных сетях информацию, которая поможет проникнуть в компанию ""Evil Corp LLC"",Как и любой специалист, я очень ленив. Чтобы не ломать голову, я пошел в Google и написал там такой запрос:,Не долго думая, кликнул на первую ссылку,и попал в Инстаграм ,Итак, мы видим Инстаграм какой то девушки. Где же может быть флаг? А флаг может быть везде и нигде. Не стоит забывать, я могу идти по ложному пути. Требуется поискать признаки флага или направления для его поиска в данном аккаунте.,Есть разные способы и инструменты разбора Инстаграм-аккаунтов, но в данной статье я буду рассказывать, как работать руками, не имея никаких дополнительных инструментов. Только умелые руки, светлая голова и хардкор!,Начинаем внимательно осматривать фотографии, находящиеся в аккаунте. Одна фотография с ноутбуком и бейджем E CORP привлекает мое внимание. Да там еще и комментариев много.,Все! Данное задание хоть и относиться к MEDIUM и считается вроде как более сложным, чем предыдущие, написанные мной ранее. Тем не менее, мне удалось ее решить в течении 15 минут. Вероятно, мне просто повезло и тот, кто ранее нашел флаг в другом месте, просто добавил комментарий с разгадкой в Инстаграм-аккаунт девушки. Как думаете?,До скорой встречи!,Великий и ужасный Сергей Сталь,Редактор: Александра Калюжная,Данная статья написана мной и продублирована на , в целях популяризации информационной безопасности.,специалист по информационной безопастности",Очередные поиски флага на Hackthebox в категории OSINT / Хабр
[<200 https://habr.com/ru/company/hostkey/blog/662592/>],page2,"В нашей , мы упустили, что видеопоток бывает и большего разрешения, поэтому стоит протестировать энкодинг файлов в формате 4К. Для полноты картины мы также сравним энкодинг на решениях от NVIDIA с встроенным GPU от Intel. Некоторые профессионалы полагают, будто достаточно собрать тот же FFmpeg с включенным QuickSync и внешняя видеокарта станет не нужна. Проверим и это утверждение.,Мы не будем подробно расписывать процесс тестирования для видеокарт от NVIDIA и зачем нам FFmpeg, поскольку информация об этом есть в предыдущих статьях (, и , части). Лучше сосредоточимся на новых результатах и полезных лайфхаках.,Используем , из ,, но установим в него видеокарту , с большим количеством блоков энкодинга, 24 ГБ видеопамяти и более высоким энергопотреблением.,Для начала проверим ее работу на количестве потоков, оказавшемся предельным для А4000 по результатам предыдущего теста:,gpu,pwr,gtemp,mtemp,sm,mem,dec,mclk,pclk,bar1,Idx,W,C,C,%,%,%,MHz,MHz,MB,0,97,47,-,92,3,0,7600,1920,33,  ,Удивительно! Мы получили сравнимые с результатом A4000 цифры. Несмотря на большую частоту работы чипа, больший объем используемой видеопамяти и большее энергопотребление, A5000 осилила энкодинг только 14 потоков и спасовала на пятнадцатом. Это фиаско еще раз доказывает, что профессиональные видеоадаптеры предназначены для других целей.,Теперь попробуем запустить трансляцию потока с разрешением 3840x2160 (оно же 4K), благо есть и такая версия ,. Энкодинг силами только центрального процессора захлебнулся уже на одном потоке, когда объем данных кратно увеличился:,  ,Каковы возможности GPU (помним, результаты у A4000 и A5000 сравнимы)? Это 3 потока. ,gpu,pwr,gtemp,mtemp,sm,mem,dec,mclk,pclk,bar1,Idx,W,C,C,%,%,%,MHz,MHz,MB,0,96,46,-,100,3,0,7600,1920,9,Как видим, по потребляемой мощности и загрузке блоков энкодинга видеочип работает явно не в режиме повышенного комфорта, хотя при этом расходуется лишь около 1 ГБ видеопамяти.,Вывод FFmpeg подтверждает, что видеокарта справляется:,А вот 4 потока адаптер уже не переваривает. Хотя загрузка железа остается примерно на тех же значениях, начинаются просадки по кадрам:,Если верить ,, технология QuickSync должна «используя специальные возможности обработки мультимедийных данных графических технологий Intel® для ускорения декодирования и кодирования, позволить процессору параллельно выполнять другие задачи и повышая быстродействие системы».,Для тестов понадобился подходящий процессор Intel (мы нашли машину с Core i9-9900K CPU @ 3.60GHz) и собранная с поддержкой Quick Sync утилита FFmpeg. С первым проблем не возникло (достаточно чипа старше 6-го поколения и наличия в нем GPU, что несложно ,), но сборка FFmpeg под тестовую Ubuntu 20.04 вызвала стойкие ассоциации с практическим освоением Камасутры. Чтобы не заставлять вас тратить драгоценное время, опишем, как нам удалось решить проблему.,Поскольку пакеты в репозиториях сломаны, первым делом нужно собрать и установить в систему библиотеки gmmlib и libva, а также последние версии Intel media driver и Media SDK. Для этого в домашней директории создадим папку GIT, зайдем в нее и выполним последовательно следующие команды (если будет не хватать каких-то зависимостей, установим их из репозитория; мы рекомендуем сделать ,):,Затем нужно собрать FFmpeg с помощью нескольких магических команд:,Стоит убедиться, что у нас появилась поддержка Quick Sync:,Вывод команды должен быть примерно таким:,Ура! Все готово к тестам.,Для начала проверим, как справляется с энкодингом видео в FullHD процессор без Quick Sync: он выдерживает максимум 4 потока, при которых все ядра загружены под 100%,Пятый поток процессор уже не осиливает, поэтому можно смело приступать к тесту с Quick Sync. В скрипте из предыдущей статьи для этого нужно будет заменить энкодер на h264_qsv, и он примет следующий вид (подробнее об использовании QuickSync с FFmpeg можно почитать ,):,Сразу делаем проверку на 6 потоках (+2 к тесту на чистом CPU):,Разница очевидна: загрузка процессора не превышает 50%, а имеющийся запас вычислительных ресурсов позволяет прогнозировать 11 – 12 итоговых потоков.,Ставим 11 потоков:,Загрузка процессора возрастает незначительно, но GPU уже подходит к пределу возможностей. Двенадцатый поток роняет битрейт и скорость обработки до 24 – 28 кадров.,Теперь проверяем потоки в 4K. В отличие от AMD, наш процессор Intel спокойно обрабатывает один поток в таком разрешении и без аппаратного ускорения:,На большее он, увы, не способен. С включенным Quick Sync тестовый компьютер смог вытянуть три потока с разрешением 4K:,Спасовал он только на четвертом, но столько же у нас выдержала и видеокарта Nvidia A5000.,Недостатки у решения, увы, тоже есть. При использовании модуля BMC (к примеру, при управлении машиной через IPMI), вы не получите доступ ко всем возможностям аппаратного ускорения, даже если GPU процессора будет определяться в системе. Придется выбирать между удобством удаленного управления или получением всех плюсов от использования Quick Sync.,Выводы вы можете сделать самостоятельно. Мы лишь отметим, что для энкодинга видео разница в мощности видеокарт не всегда определяется их ценой, а для решения некоторых задач стоит обратить внимание на специализированные технологии внутри центральных процессоров. Также мы использовали для тестов H264, но кодеки HEVC (H265) или VP1 в теории должны дать лучшие результаты, особенно на разрешениях 4K. Если вы самостоятельно проведете подобные тесты с первым (VP1 пока что представлен аппаратно и массово только для декодинга), поделитесь результатами в комментариях.,____________,Стоимость описанных выше экспериментов измерить просто: воспользуйтесь нашим ,.,Например, в самой простой конфигурации она следующая:,машина с A4000 обойдется в 22 000р, 12 потоков - 1800р на поток в месяц;,машина с A5000 обойдется в 31 000р, 14 потоков - 2214р на поток в месяц;,сервер на i9-9900K с QuickSync (QSV) обойдется в 5000-6000р, 11 потоков, 450р на поток.,Серверы для такого необходимо собирать на материнских платах без удаленного управления, что мы умеем. Обращайтесь!,Кстати, все серверы HOSTKEY предоставляются с нашим модулем полного удаленного управления сервером IPMI и ,. Об устройстве последней мы ,.,Пользователь",Многопоточный энкодинг: переплатить вдвое или уйти на «встройку»? / Хабр
[<200 https://habr.com/ru/company/audiomania/blog/662471/>],page2,"Существует множество языков для музыкального программирования. О некоторых из них, например, , или ,, мы рассказывали в блоге. Сегодня поговорим о проекте , — браузерном DAW, реализующем концепцию music-as-code.,В общем случае концепция music-as-code и муз. программирование подразумевают написание треков на специализированных ЯП. Но в отличие от других систем такого класса, , предлагает использовать JavaScript. Инструмент разработал инженер Джеймс Райан в прошлом году и выложил исходники ,.,Для воспроизведения звуков и музыки приложение использует Web Audio API, а также фреймворки , и , и библиотеку ,. Рабочая область Harmonicon похожа на большинство IDE для разработки, так как использует open source редактор ,.,Для настройки звуков в IDE представлен браузер аккордов. Он позволяет выбрать тональность, вид лада (например, мажор или минор), а также октаву (1–6). Далее, система автоматически сгенерирует код, необходимый для воспроизведения этого аккорда — например, для мажорного трезвучия C (CEG) он будет выглядеть так:,Виртуальная среда позволяет работать с синтезатором — он находится в правой части рабочей области — и подключать MIDI-устройства. В целом IDE содержит целую библиотеку стандартных инструментов — там есть клавишные (пианино, орган, гармония), ударные (ксилофон), струнные (арфа, контрабас, скрипка, виолончель) и многие другие. Сохранять музыкальные файлы можно как в облако, так и на локальный диск.,Также есть список шаблонов с аудиоэффектами. Например, чтобы добавить задержку в генерируемый трек, достаточно прописать команду:,Следующая строка добавляет эффект тремоло:,Документация с описанием функций и синтаксиса — ,. ,Стоит заметить, что проект Harmonicon является экспериментальным, поэтому в его работе можно встретить баги — в том числе ошибки при компиляции. Для повышения стабильности стоит работать в браузере Chrome. Но в целом эта IDE — неплохой инструмент, который позволяет познакомиться с концепцией music-as-code.,Существуют и другие виртуальные среды, позволяющие взять легкий старт в музыкальном программировании. Одна из известных — ,, которая также , установки приложений и работает из браузера. Для написания композиций используется реализация LISP с компиляцией в JavaScript — Clojurescript. Треки воспроизводятся при помощи Web Audio API. Стоит заметить, что инструмент плохо подходит для написания сложных композиций, но помогает быстро погрузиться в тему.,Еще один интересный проект — открытый визуальный ЯП ,. Программист работает не с кодом, а функциональными объектами, которые объединяются в патчи — по аналогии с модульными синтезаторами и патч-кордами для соединений. Pd подходит для цифровой обработки сигналов и на нем можно , алгоритмическую музыку.,Пользователь",Музыка как код — опробовать концепцию можно прямо в браузере / Хабр
[<200 https://habr.com/ru/post/662594/>],page2,"API Gateway является одним из обязательных компонентов архитектуры современных систем. Он может решать различные задачи:,Проксирование вызовов из одной точки в разные сервисы в разных форматах. Контроль доступа к ним.,Мониторинг.,Аутентификация и авторизация.,Агрегация данных (один запрос к API Gateway - несколько запросов к бэку).,Управление таймаутами.,Кэширование данных.,Валидация.,И многое другое.,При отсутствии API Gateway все эти задачи ложатся на бэкенд. И если для монолита это может быть оправдано, то в микросервисах любое изменение общей логики повлечёт доработку всех зависимых сервисов.,Есть много интересных готовых решений, в т.ч. и для нашего кейса (GraphQL <-> gRPC), но ни одно из них не подошло нам в полной мере.,На нашем проекте API Gateway нужен в основном для запросов с фронта. Когда мы только начинали, основным видом взаимодействия был REST. Он достаточно прост для понимания и реализации, но в процессе развития проекта начали проявляться его недостатки:,При обновлении сущности, даже если обновить нужно пару полей, мы обновляли весь объект. Да, можно использовать метод PATCH и парсить на бэке только переданные поля, рассматривая отдельно ситуации, когда поле передано со значением null и когда поле не передано, но это не стандартный подход для REST.,Не всегда при запросе на получение сущности нужны все поля. Можно сделать несколько методов вместо одного, но тогда для каждого кейса придётся делать отдельный метод. А если для одной и той же структуры у пользователей с разными ролями будут разные доступы к полям, которые, к тому же, могут меняться, реализовать это, используя REST, невозможно.,Нет пакетной обработки. Для получения данных из каждого сервиса фронт делает отдельные запросы.,GraphQL, ,, успешно решает эти проблемы. В качестве бонуса в GraphQL в отличие от REST есть ,. Их использование даёт возможность делать свои обработчики для полей запроса, императивно добавлять кэширование, валидацию и прочее.,С API для фронта разобрались, переходим на бэк. Проблемы остаются те же, за исключением пакетной обработки (на бэке таких кейсов нет и не предвидится). Можно изначально делать API бэка на GraphQL, это бы значительно упростило создание API Gateway. Но не всё так просто. ,Мы пишем на Java, используем ,, и для работы с GraphQL там есть неплохие на первый взгляд решения как для ,, так и для ,. С серверной частью проблем нет, но клиентская далека от совершенства., очень прост и минималистичен.,Однако такой клиент не даёт возможности делать частичное обновление или выбирать поля для формирования ответа., очень гибкий. Можно передавать какой угодно набор полей, можно использовать ,, можно задавать список полей, которые мы хотим получить в ответе.,Но это очень громоздко. К тому же при формировании запроса можно допустить ошибку.,В gRPC есть возможность генерировать типобезопасные контроллеры и клиенты. А для ограничения списка полей, которые возвращаются в ответ, есть специальный тип - ,.,В gRPC в качестве формата запросов-ответов используется protobuf. Его же мы используем для сообщений в асинхронном взаимодействии через kafka. Т.о. нам не нужно плодить форматы, и мы можем использовать на бэке что-то одно.,Часто, когда говорят о преимуществах gRPC, упоминают возможность потоковой передачи данных, снижающей накладные расходы. Мы к этому пока не пришли, но хотелось бы иметь такую возможность в будущем.,В protobuf есть ,, но её мы решили по-своему (об этом будет написано далее).,Ранее я писал, что на бэке мы используем Java в качестве основного языка программирования, поэтому хотелось бы найти именно готовое решение на нём. Единственное, что удалось найти, это проект ,. Выглядит интересно, но это не коробочное решение, а именно библиотека. К тому же для каждого эндпоинта нужно писать свой обработчик, что не очень удобно, т.к. при его добавлении/изменении придётся пересобирать проект. Ещё один минус - проект последнее время не развивается. Однако именно в его исходниках удалось почерпнуть много полезной информации для создания собственного API Gateway.,Ещё один интересный проект - , - написан на TypeScript и предоставляет возможность маппинга из GraphQL не только в gRPC, но и используя другие способы взаимодействия. Но он не даёт возможности кастомизировать маппинг типов и добавлять свои обработчики для запросов. Всё это можно было бы сделать самим, но пишем мы на Java.,Есть решения на Go, но они предполагают генерацию API Gateway из контракта, что нас не устраивает. В итоге было принято решение написать свой API Gateway на нашем стэке технологий.,Список основных требований к концепту:,Сервис должен быть написан на Java/Quarkus.,В сервисе не должно быть кодогенерации DTO и стабов, которые необходимо использовать для вызова сервисов, чтобы не пришлось пересобирать проект каждый раз после изменения API какого-либо вызываемого сервиса.,В сервисе должна быть возможность ""горячего обновления"" контракта. При изменении сервисов на бэке не должно быть необходимости править код API Gateway.,Поддержка batch-запросов., - зависимости фреймворка для работы с YAML-конфигом и gRPC., - добавляет скаляры GraphQL. Из коробки поддерживается не так много типов, поэтому без использования этой зависимости не обойтись., - добавляет директивы для валидации запросов., - нужно для логирования запросов-ответов от gRPC-сервера в формате JSON., - нужно для корректного логирования в формате JSON., - перчик., - нужно для конфигурации серверной части GraphQL с помощью... Vertx.,Хотя Quarkus и является основным фреймворком для приложения, но он, к сожалению, больше заточен на подход code-first, а мы строго придерживаемся подхода contract-first. К тому же при подходе code-first нам бы не удалось реализовать какой-то универсальный обработчик. Поэтому фактически от Quarkus в проекте будет только DI, конфигурация и запросы к gRPC-сервисам. А для создания и настройки основного обработчика больше подходит Vertx.,у Quarkus под капотом находится Vertx, что бывает очень полезно для написания каких-то низкоуровневых вещей. Серверная часть Quarkus полностью основана на Vertx, реактивные REST/GraphQL клиенты под капотом используют HTTP-клиент из Vertx, работа с БД идёт через библиотеки Vertx. Однако не все компоненты, для которых есть реализация в Vertx, переиспользуются в Quarkus. Серверная часть для GraphQL и обёртка для работы с gRPC в Quarkus написана без использования Vertx.,Как сконфигурировать серверную часть GraphQL, можно посмотреть в , и ,. В итоге у меня получилась такая конфигурация:,В основном методе выполняются следующие действия:,парсится схема graphql,добавляются обработчики аннотаций (в данном случае это , из библиотеки ,, который обеспечивает обработку директивы ,),добавляются обработчики для методов , и , (в нашем случае это один обработчик для всех методов),добавляются скаляры ,, ,, , для поддержки соответствующих типов,добавляется , для логирования.,Тут стоит обратить внимание на ,. За счёт этого параметра мы делаем возможным выполнение , к API Gateway.,У меня роутинг зашит прямо в конфигурационном файле приложения. Это оптимальное решение для концепта, но для реального рабочего проекта лучше использовать централизованное хранилище (БД, внешний кэш), т.к. при большом количестве сервисов конфигурационный файл будет очень громоздким и нечитаемым.,Подробнее про конфигурацию Quarkus-приложения можно почитать в ,. Для простоты я не делал маппинг методов GraphQL в методы gRPC и считаю, что у они у нас будут называться одинаково. Т.о. мне остаётся лишь смаппить метод GraphQL в сервис gRPC.,Proto - бинарный формат со строгой типизацией. Поэтому для формирования запроса и парсинга ответа необходимо иметь у себя соответствующий контракт, на основе которого генерируются DTO запросов/ответов и стабы. С учётом того, что контракты gRPC-сервисов лежат в них самих, получать их нужно именно оттуда. Примеров не так много, я нашёл ,, и выглядит он страшновато. В нём нас интересует метод ,. После небольшого рефакторинга у меня получилось следующее:, - полное название сервиса, которое я получаю из конфига, например ,., В коде клиента стоит обратить внимание на метод ,, возвращающий ответ, обёрнутый в ,. В Quarkus активно используется библиотека ,, а Uni - это одно из представлений результата, аналог Mono из ,.,Из полученного ответа нам требуется извлечь дескриптор сервиса, в котором хранится информация о сущностях и методах. В ответе сервиса приходит , в бинарном виде. Из него необходимо получить информацию о контракте с учётом всех его зависимостей, т.к. контракт может содержать информацию из разных файлов, включая proto-файлы из стандартной библиотеки и импортированных библиотек.,Маппинг JSON-запроса в запрос к gRPC-сервису я подсмотрел в проекте Rejoiner, который упоминал ранее, и добавил туда маппинг классов-обёрток для nullable-типов.,В protobuf нельзя передать null-значение. В отдельных случаях можно использовать стандартные классы-обёртки, которые есть в библиотеке, такие как StringValue, BooleanValue, рассматривая их значение по умолчанию как null. Но если для строк (в нашем случае) это будет работать, т.к. значением по умолчанию там является пустая строка, то 0 для числовых значений для нас неприемлем. Я подготовил небольшой proto-файл с обёртками для примитивов, в котором используется google.protobuf.NullValue для корректной работы с null.,Ограничения маппинга:,Нельзя смаппить запрос, принимающий более одного параметра.,Нет маппинга для ,. Исключением являются кастомные nullable-типы.,Для перечислений на стороне gRPC сервисов необходимо задавать какое-то неиспользуемое значение по умолчанию, которое можно принимать за null.,Нет маппинга для ,, потому что в GraphQL нет аналога. Не то, чтобы я часто пользовался этим типом в gRPC, но в паре REST-методов он есть.,Маппинг запрашиваемых полей в FieldMask возможен, только если FieldMask лежит в корне структуры в proto. В дальнейшем это будет исправлено.,Кроме самого запроса необходимо подготовить дескриптор метода, который включает в себя тип запроса (синхронный запрос, стриминг), название метода и маршаллеры запроса/ответа.,Запрос делается аналогично запросу на получение дескриптора сервиса.,В самом простом случае для преобразования ответа в JSON можно было бы использовать метод ,, но там нет и быть не может нормального маппинга nullable-значений, даты и даты-времени. В остальном маппинг в JSON достаточно прост и представляет собой рекурсивный обход ответа и преобразование его в Map<String, Object>.,Готовый проект можно посмотреть на ,. Он включает в себя ,, , и ,. Несмотря на имеющиеся ограничения, этот проект может быть основой полноценного API Gateway.,Одним из требований к приложению было наличие возможности перезагрузки контракта без остановки приложения. Это не имеет смысла, если хранить контракт в самом приложении (как сейчас), но если держать его в БД/кэше/хранилище схем, функция очень полезная. У меня это реализуется за счёт пересоздания ,.,Новый контракт становится актуальным сразу после загрузки.,Изначально на каждый запрос с фронта делался запрос на получение дескриптора. И хотя выполняется он достаточно быстро, нет смысла делать его каждый раз, т.к. контракт меняется нечасто. Поэтому ответ можно закэшировать. Для кэширования я взял библиотеку ,, использующую ,.,Несмотря на то, что кэшируемый метод возвращает Uni, кэшируется именно результат. При повторном запросе по тому же ключу запрос в gRPC-сервис за дескриптором уже не делается. Если возвращается ошибка при запросе к дескриптору, результат не кэшируется.,Умный, красивый, скромный",GraphQL &lt;-&gt; gRPC API Gateway на Java / Хабр

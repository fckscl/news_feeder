link,next,text,title
[<200 https://habr.com/ru/post/662670/>],page2,"Средства межсетевого экранирования стали де-факто атрибутом любой сетевой инфраструктуры. Почтовому трафику тоже необходимы средства фильтрации. Поэтому в современных , реалях тяжело представить почтовую инфраструктуру организации без , (,)., (ЭП) является одним из векторов атак, как средство для доставки ВПО клиентам с целью проникновения в корпоративную сеть организации. Но у ЭП есть и еще один враг - спам, мешающий работе и заполняющий полезное дисковое пространство на почтовых серверах. Для решения данных проблем, уже разработаны решения: коммерческие и распространяемых под свободными лицензиями. Из коммерческих наибольшей популярностью пользуется продукт ,. Но все мы знаем про проблемы ""окирпичичвания"" и , окончания лицензий вендоров в текущих условиях, поэтому попробуем посмотреть в сторону свободно распространяемых продуктов.,Свободные решения считаются более сложными в настройке и требуют опыта настройки и администрирования. Конечно, можно накатить , и поставить туда , с использованием , и ,. Однако, если вам, как и мне, хотелось бы раскатать одну виртуалку (в которой уже из коробки есть все необходимое), зайти на вебку и все там настроить - тогда ваш выбор - ,.,Многие слышали о Proxmox благодаря решению для виртуализации - ,, как замену , от VMWare. Поэтому людям, знакомых с ,, , не покажется чем-то новым в установке и администрировании, так как это тот же Debian с тем же Web-интерфейсом, только заточен под ЭП.,Честно сказать, я думал, что мало кто использует данный программный продукт и скептически относился к нему, как замена для ESA. Однако, информация с , меня удивила и придала мне уверенности, что я не один такой.,Как говорилось выше, почтовый шлюз является аналогией межсетевого экранирования для почты. Поэтому ставить PMG, как и любой другой шлюз, нужно , (на границе попадания почтового трафика извне) ,. Таким образом, между SMTP-сервером отправителя (или спамера) и SMTP-сервером получателя есть барьер в виде почтового шлюза.,PMG поставляется в виде ,инсталлятора. Поэтому на чем его устанавливать - решение на вкус и цвет каждого. Хоть на старый ПК, хоть на сервак, хоть использованием виртуализации.,Установка предельно простая, описана в официальной документации и мало чем отличается от установки типичной ОС из ISO-инсталлятора. Пользователи PVE вообще не заметят существенной разницы.,После успешной установки, для управления PMG необходимо перейти в браузере по адресу: , и ввести учетные данные, указанные при установке.,Если описывать настройку всех возможностей, то статья превратится в документацию сайта разработчика, поэтому опишу кратко основные (необходимые) параметры., Для настройки (администрирования) механизмов почтового шлюза представлены следующие ,:, - настройка цепочек правил контентной фильтрации писем (аналог , у ESA). Касается обработки писем и действий над ними;, - настройка основных параметров самого шлюза. Включение/отключение механизмов защиты, настройки сети, ретрансляции, антивирусного и спам движков, управление пользователями, настройка кластера, бекапирования, сертификатов;, - управление почтовыми очередями, карантинами, настройка Black/White листов, просмотр трекинга сообщений;,PMG из коробки уже наделен цепочками правил в ,, готовыми к работе на страже вашего почтового трафика. Подробно останавливаться на этом не вижу смысла, кто работал с ESA - поймет, и допилит под себя, кто впервые на это смотрит - необходимо понять суть. Суть в том, что для построений цепочек правил (,) существуют следующие объекты:, - действия, применяемые при попадании в правило (Rule). Например, доставить письмо пользователю, дропнуть письмо, поместить в карантин, удалить вложения, оповестить администратора и т.д.;, - сгруппированные по какому-либо признаку списки объектов, относящихся к отправителю или получателю (конкретные адреса, домены, IP-адреса, регулярные выражения и т.д.);, - сгруппированные по какому-либо признаку списки объектов, относящихся к содержимому электронного письма (картинки, ссылки, вложения, офисные файлы и т.д.);, - сгруппированные по какому-либо признаку списки объектов, относящихся к временному интервалу, например, нерабочее время или ночь;,Соответственно, подобно составлению ACL, комбинирование данных объектов в цепочку - это и есть правила. Образно работает это так:, Если мне прислали письмо от , (адрес из Blacklist в ,), в письме офисный документ .docx (файл из ,) - заблокировать письмо или отправить в карантин (действие из ,).,Основной раздел для настройки работы почтового шлюза. В этом разделе первым делом нужно настроить ,.,В разделе , в поле , указываем IP-адрес или доменное имя SMTP-сервера, на который нужно отправлять письма дальше (сервер, обслуживающий ваш домен)., В разделе , необходимо добавить ,, обслуживаемые вашим SMTP-сервером. Делается это для того, чтобы PMG понимал, какие письма ему обрабатывать и отправлять дальше., В разделе , вы можете изменить какие порты PMG должен слушать. По-умолчанию 25 порт (,) служит для приема писем снаружи (из Интернета). Порт 26 (,) является релеем для получения писем от вашего почтового сервера и отправки их затем наружу (на другие почтовые домены)., В разделе , необходимо указать какому домену какой SMTP-сервер использовать для пересылки. У вас может быть несколько обслуживаемых доменов и на каждый из этих доменов может быть свой SMTP-сервер., В разделе , необходимо указать , - сети с которых будет , прием для пересылки на другие домены. Делается это для того, чтобы письма пересылались наружу только с ваших доверенных SMTP-серверов., В разделе , можно включить TLS при отправки и получении сообщений. Это означает, что при включенном TLS PMG будет пытаться отправлять письма наружу с использованием расширения ESMTP - ,, а так же сможет такие такие письма принять., В разделе , можно включить подпись исходящих сообщений. О том как добавить в PMG свой ключ подписи - написано в документации., В разделе , можете указать те адреса и домены, которые не будут проходить проверки, включенные в разделе ,., В разделе , настраивайте механизмы проверок в зависимости от политики вашей организации. От себя хочу посоветовать изменять стандартный баннер и не показывать всем, что вы используете.,На этом основное конфигурирование почтового шлюза заканчивается. Все остальные настройки корректируются в зависимости от ваших личных предпочтений и требований безопасности.,В качестве антиспам-решения PMG использует под капотом ,. По-умолчанию включен и готов к работе из коробки. Для изменения параметров антиспама используются разделы:, ,В качестве АВЗ PMG использует движок ,. По-умолчанию включен и готов к работе из коробки. Для изменения параметров антивирусного движка используются разделы:, , позволяет работать в режиме кластера. Это означает, что у вас может быть 2 почтовых шлюза (для балансировки нагрузки или отказоустойчивости). При этом режиме работы, настройки и политики применяемые на одном шлюзе синхронизируются с другим и наоборот (аналог стека у коммутаторов)., так же из коробки позволяет сделать вам кластер из , нод. Аналогией является кластер в PVE, где несколько физических гипервизоров можно объединить в кластер.,Настройка банально проста и происходит в разделе ,. Для настройки объединения нод PMG в кластер необходимо:,На мастер ноде создать кластер (нажать кнопку ,) и подождать завершения операции;,На мастер ноде нажать на кнопку , и скопировать себе , и ,;,На ноде, которую хотите добавить в кластер нажать кнопку , и ввести , и ,, скопированного с мастер-ноды.,Кластер готов. Теперь настройки применяемые на одной из нод будут применены и на другой. Все просто, не так ли?,И это все? Конечно, , нет. Данная статья рассчитана на то, чтобы познакомить вас с таким замечательным, на мой взгляд, решением, как ,. Конечно до , ему еще далеко, но из того, что предлагает , - это ,. Настройки, приведеные в статье позволяют лишь подготовить PMG для пересылки писем от внешнего отправителя на внутренний почтовый сервер и наоборот. Как я говорил в самом начале, прелесть данного решения в готовности к бою со спамом из коробки, при минимальных затратах на его настройку. Главная задача настройки сводится к ""направлению"" , через почтовый шлюз. Прелесть данного решения еще и в том, что под капотом , с , и т.д., которые уже взаимодействуют между собой. Все, что вам остается - тюнить правила и политики. Если не хватает возможностей с вебки - лезем по , на PMG, устанавливаем пакеты, конфигурируем файлы, танцуем с бубном - все в ваших руках, все как мы любим. Ну и, конечно же - чтение документации. У PMG есть свои утилиты для управления политиками, а так же Rest API.,Не забудьте указать на вашем SMTP-сервере (MTA) в качестве , - PMG с портом 26, для отправки всех писем наружу через шлюз. Так же не забудьте настроить , на вашем пограничном оборудовании, чтобы , с портом 25 указывал на 25 порт PMG. Дерзайте!,DevOps",ЧудESA защиты корпоративной почты или внедрение свободных почтовых шлюзов на базе Proxmox Mail Gateway / Хабр
[<200 https://habr.com/ru/post/662561/>],page2,"Существует , задача: ,Есть 2 емкости: 5 литров и 3 литра. Как отмерить 4 литра жидкости используя , эти 2 емкости?,Понятное дело что тут важно не сколько знание правильного ответа, а знание , решения таких задач. Ведь вместо целевых 4х литров могут спросить отсчитать и 1,2,6,7 литров. ,В этом тексте я решу эту задачу в общем виде при помощи конечного автомата. Так как тут явно можно проследить , и , воздействия. Также я упомяну про , язык Front-End разметки ,. Методика конечного автомата хорошо изучена и поставлена на рельсы. Состоит из 3 фаз.,Состояние определяется количеством жидкости в паре сосудов. Согласно комбинаторике по правилу умножения существует всего 24 состояния. Вот они все перечислены.,из условий задачи),Существует всего 5 , действий с бутылками. Вот их перечень.,Как гласит английская народная пословица “,” (A picture is worth a thousand words). Также мой универский профессор часто говорил, что инженеры - это про схемы. Поэтому представляю блок-схему в виде ориентированного графа состояний для задачи про бутылки.,Я накропал на языке С консольную утилиту, которая прокручивает конечный автомат и сохраняет в файл составленный код на языке dot. Далее бесплатная утилита dot.exe поедает файл *.dot и преобразует его во всем известный *.svg файл. Наконец  браузер Chrome.exe поедает  *.svg и отрисовывает в окне на мониторе. Язык Dot хорош тем что он имеет простой синтаксис. Dot более высокоуровневый язык, чем спецификация *.svg файлов.  ,Глядя на этот , , становится ,, что чтобы отмерить 4 литра надо выполнить следующую  процедуру из 6-ти инструкций:,есть еще одно , решение,Easy!,Вот код Dot графа для тех, кто захочет изучить граф внимательнее.,Можно отрисовать граф на этом сайте и сохранить его в *.svg.,Редактировать *.svg можно при помощи бесплатной программы ,.exe.,Эта задача может служить отличной , для обучения теории конечных автоматов FSM. Конечные автоматы повсеместно используются в промышленной разработке, например, системного программного обеспечения.,Существует 8 , состояний: 1/5_1/3; 1/5_2/3; 2/5_1/3; 2/5_2/3; 3/5_1/3; 3/5_2/3; 4/5_1/3; 4/5+2/3. В эти состояния никак , как только не переливай содержимое сосудов. Если вы захотите , человека на собеседовании, то можно попросить его ""как перелить жидкости так чтобы в каждой бутылине осталось по 2 литра?"" или ""как перелить жидкости так чтобы в каждой бутылке осталось по 1 литру?"".,Формально можно отмерить не только 4 лита, а , , от 1 до 8 включительно.,Как видите, язык Front-End разметки , отлично подходит для автоматической отрисовки , векторной графики. Конечно dot тула несовершенна и даже , графы строит с пересечениями ребер. Если вы знаете тулу которая , , то укажите это в комментариях.,Буду признателен, если пришлете описания подобного рода логических задач в комментариях к тексту.,Пользователь",Задача про две ёмкости для жидкости / Хабр
[<200 https://habr.com/ru/post/662576/>],page2,"Привет, дорогой друг! Сегодня я продолжу рассказывать про поиск флага на Hackthebox. Забегу немного вперед: данная статья будет очень короткой. Мне довольно легко и быстро удалось найти флаг.,Итак, само задание:,По-русски: нас просят найти в социальных сетях информацию, которая поможет проникнуть в компанию ""Evil Corp LLC"",Как и любой специалист, я очень ленив. Чтобы не ломать голову, я пошел в Google и написал там такой запрос:,Не долго думая, кликнул на первую ссылку,и попал в Инстаграм ,Итак, мы видим Инстаграм какой то девушки. Где же может быть флаг? А флаг может быть везде и нигде. Не стоит забывать, я могу идти по ложному пути. Требуется поискать признаки флага или направления для его поиска в данном аккаунте.,Есть разные способы и инструменты разбора Инстаграм-аккаунтов, но в данной статье я буду рассказывать, как работать руками, не имея никаких дополнительных инструментов. Только умелые руки, светлая голова и хардкор!,Начинаем внимательно осматривать фотографии, находящиеся в аккаунте. Одна фотография с ноутбуком и бейджем E CORP привлекает мое внимание. Да там еще и комментариев много.,Все! Данное задание хоть и относиться к MEDIUM и считается вроде как более сложным, чем предыдущие, написанные мной ранее. Тем не менее, мне удалось ее решить в течении 15 минут. Вероятно, мне просто повезло и тот, кто ранее нашел флаг в другом месте, просто добавил комментарий с разгадкой в Инстаграм-аккаунт девушки. Как думаете?,До скорой встречи!,Великий и ужасный Сергей Сталь,Редактор: Александра Калюжная,Данная статья написана мной и продублирована на , в целях популяризации информационной безопасности.,специалист по информационной безопастности",Очередные поиски флага на Hackthebox в категории OSINT / Хабр
[<200 https://habr.com/ru/post/662566/>],page2,"По материалам статьи Craig Freedman: ,Уровни изоляции транзакций Serializable и Snapshot обеспечивают согласованное чтение из базы данных. На любом из этих уровней изоляции транзакция может читать только зафиксированные данные. Более того, транзакция может читать одни и те же данные несколько раз, не заботясь о каких-либо параллельных транзакциях, вносящих изменения в эти же данные. Те нежелательные эффекты, которые были продемонстрированы в предыдущих статьях при Read Committed и Repeatable Read, на уровнях изоляции Serializable и Snapshot просто невозможны.,Обратите внимание, что я использовал фразу «не заботясь о каких-либо … вносящих изменения». Такой подбор слов является преднамеренным. На уровне изоляции Serializable ядро SQL Server накладывает блокировку диапазона ключей и удерживает её до окончания транзакции. Блокировка диапазона ключей гарантирует, что после того, как транзакция прочитает данные, никакая другая транзакция не сможет изменить эти данные (даже для вставки фантомных строк) до тех пор, пока не завершится транзакция, удерживающая блокировку. На уровне изоляции Snapshot ядро SQL Server не накладывает никаких блокировок. Таким образом, одновременная транзакция может изменять данные, которые уже прочитаны второй транзакцией. Вторая транзакция просто не замечает этих изменений и продолжает использовать старую версию данных.,Уровень изоляции Serializable основан на пессимистическом контроле параллелизма. Он гарантирует согласованность, предполагая, что две транзакции могут пытаться обновить одни и те же данные, и использует блокировки, чтобы гарантировать, что они этого не cделают, но (за счет уменьшения параллелизма) одна транзакция должна ждать завершения другой, и две транзакции могут заблокироваться. Уровень изоляции Snapshot основан на оптимистичном управлении параллелизмом. Это позволяет транзакциям выполняться без блокировок и с максимальным параллелизмом, но может произойти сбой и последующий откат транзакции, если две транзакции одновременно попытаются изменить одни и те же данные.,Как видим, существуют различия между уровнями изоляции Serializable и Snapshot в уровне параллелизма (которого можно достичь), и в наборе возможных проблем (взаимоблокировки и конфликты обновлений).,Рассмотрим, чем работа в Serializable и Snapshot отличаются с точки зрения обеспечиваемой ими изоляции транзакции. При Serializable всё довольно просто. Чтобы результат двух транзакций считался сериализуемым, они должны выполняться в некотором порядке по одной транзакции за раз.,Snapshot не гарантирует такой уровень изоляции. Несколько лет назад , предложил прекрасный пример, демонстрирующий различия этих уровней. Представьте, что у нас есть мешок, содержащий смесь белых и чёрных шаров. Предположим, мы хотим запустить две транзакции. Одна транзакция перекрашивает каждый белый шар в чёрный шар. Вторая транзакция перекрашивает каждый чёрный шар в белый шар. Если мы запускаем эти транзакции с изоляцией Serializable, они будут исполняться поочерёдно. После первой транзакции останется мешок с шарами только одного цвета. После этого вторая транзакция изменит все эти шары на другой цвет. Есть только два возможных исхода: мешок только с белыми шарами или мешок только с чёрными шарами.,Если мы запускаем эти транзакции с изоляцией Snapshot, появляется третий результат, который невозможен при изоляции Serializable. Каждая транзакция может одновременно делать снимок мешка с шарами в том виде, в каком он был до внесения изменений. Теперь одна транзакция находит белые шары и перекрашивает их в чёрные шары. В то же время другие транзакции находят чёрные шары (но только те шары, которые были чёрными, когда мы сделали снимок, а не те шары, которые первая транзакция изменила на чёрные) и перекрашивает их в белый цвет. В результате, мешок будет содержать смесь белых и чёрных шаров. На самом деле, с этим уровнем изоляции мы правильно поменяли цвет каждого шара.,Следующий рисунок иллюстрирует эти различия:,Мы можем продемонстрировать подобное поведение средствами SQL Server. Обратите внимание, что Snapshot изоляция доступна только с SQL Server 2005 и должна быть явно включена для используемой базы данных:,Начнем с создания простой таблицы с двумя строками, обозначающими два шара разных цветов:,Затем в первом сеансе начните транзакцию с уровнем изоляции Snapshot:,Теперь, прежде чем зафиксировать изменения, запустите во втором сеансе следующее:,Наконец, зафиксируйте транзакцию в первом сеансе и проверьте данные в таблице:,Вот какой получился результат:,Как можно видеть, шар 1, который изначально был чёрным, теперь стал белым, а шар 2, который изначально был белым, стал чёрным. Если вы попробуете тот же эксперимент с уровнем изоляции Serializable, одна транзакция будет ждать завершения другой, и, в зависимости от порядка, оба шара будут белыми или чёрными.,DBA",Serializable vs. Snapshot Isolation Level / Хабр
[<200 https://habr.com/ru/company/audiomania/blog/662471/>],page2,"Существует множество языков для музыкального программирования. О некоторых из них, например, , или ,, мы рассказывали в блоге. Сегодня поговорим о проекте , — браузерном DAW, реализующем концепцию music-as-code.,В общем случае концепция music-as-code и муз. программирование подразумевают написание треков на специализированных ЯП. Но в отличие от других систем такого класса, , предлагает использовать JavaScript. Инструмент разработал инженер Джеймс Райан в прошлом году и выложил исходники ,.,Для воспроизведения звуков и музыки приложение использует Web Audio API, а также фреймворки , и , и библиотеку ,. Рабочая область Harmonicon похожа на большинство IDE для разработки, так как использует open source редактор ,.,Для настройки звуков в IDE представлен браузер аккордов. Он позволяет выбрать тональность, вид лада (например, мажор или минор), а также октаву (1–6). Далее, система автоматически сгенерирует код, необходимый для воспроизведения этого аккорда — например, для мажорного трезвучия C (CEG) он будет выглядеть так:,Виртуальная среда позволяет работать с синтезатором — он находится в правой части рабочей области — и подключать MIDI-устройства. В целом IDE содержит целую библиотеку стандартных инструментов — там есть клавишные (пианино, орган, гармония), ударные (ксилофон), струнные (арфа, контрабас, скрипка, виолончель) и многие другие. Сохранять музыкальные файлы можно как в облако, так и на локальный диск.,Также есть список шаблонов с аудиоэффектами. Например, чтобы добавить задержку в генерируемый трек, достаточно прописать команду:,Следующая строка добавляет эффект тремоло:,Документация с описанием функций и синтаксиса — ,. ,Стоит заметить, что проект Harmonicon является экспериментальным, поэтому в его работе можно встретить баги — в том числе ошибки при компиляции. Для повышения стабильности стоит работать в браузере Chrome. Но в целом эта IDE — неплохой инструмент, который позволяет познакомиться с концепцией music-as-code.,Существуют и другие виртуальные среды, позволяющие взять легкий старт в музыкальном программировании. Одна из известных — ,, которая также , установки приложений и работает из браузера. Для написания композиций используется реализация LISP с компиляцией в JavaScript — Clojurescript. Треки воспроизводятся при помощи Web Audio API. Стоит заметить, что инструмент плохо подходит для написания сложных композиций, но помогает быстро погрузиться в тему.,Еще один интересный проект — открытый визуальный ЯП ,. Программист работает не с кодом, а функциональными объектами, которые объединяются в патчи — по аналогии с модульными синтезаторами и патч-кордами для соединений. Pd подходит для цифровой обработки сигналов и на нем можно , алгоритмическую музыку.,Пользователь",Музыка как код — опробовать концепцию можно прямо в браузере / Хабр
[<200 https://habr.com/ru/post/662533/>],page2,"Много лет назад я занимался созданием маленьких Flash игр и публиковал их на сайте Newgrounds. Сейчас я делаю полноценные игры для ПК.,На сегодняшний день у меня 4 законченные коммерческие игры в Steam, и самая последняя из них — выпущенная в 2021 году Pilie Pals, о процессе создания которой я расскажу в этой статье. Я работал над игрой всего примерно 6 месяцев, по вечерам после работы и на выходных.,Я занимаюсь дизайном, программированием, графикой, звуками и музыкой в одиночку. Мне это нравится, и таким образом я могу часто переключаться с одного вида деятельности на другой, благодаря чему не теряю интерес к разработке игры.,Я написал собственный 3D игровой движок ,, используя Haxe, C++ и OpenGL, и на данный момент он используется тремя моими играми. Подробности и причины создания собственного движка приведены ,. Такой подход меня вполне удовлетворяет, и я не планирую его менять.,Трейлер к игре. Доступна ,, а также есть ,.  ,В начале разработки Pilie Pals я скопировал папку проекта своей предыдущей игры , и удалил все ресурсы и файлы кода, связанные с игрой. Остался ""чистый"" движок, с которым можно экспериментировать, чтобы создать прототип следующей игры.,Перед тем, как описывать создание Pilie Pals, я объясню несколько главных принципов работы моего движка.,Мой движок почти полностью основывается на внешних данных (data-driven). Это значит, что я создаю набор файлов с данными, а движок их читает и обрабатывает.,Я стараюсь избегать жёсткого прописывания чего-либо в исходном коде игры. По возможности, вся информация хранится в отдельных файлах: игровые объекты, уровни, переводы текстов, визуальные и звуковые эффекты, и даже некоторая игровая логика, написанная ,. Большая часть данных хранится в формате JSON.,Самое большое преимущество такого подхода заключается в том, что я могу создавать и править игру, не выходя из неё. При изменении какого-либо файла с данными движок автоматически перегружает ту часть данных, которая изменилась, без необходимости перезапускать или пересобирать что-то вручную. То же самое происходит с другими ресурсами, например, с текстурами, 3D моделями и звуками.,Ещё один плюс — это потенциальная поддержка пользовательских модификаций игры. Недавно я выложил новое обновление для игры, которое добавляет русскую локализацию, ,.,Два главных компонента сцены игры в моём движке — это сущности и карты.,Сущность — игровой объект, который я описываю в JSON файле, чтобы впредь создавать экземпляры объектов такого вида. Например, персонаж игрока — это сущность.,Файл описания сущности содержит информацию:,Какие 3D модели содержит данная сущность, и как их отображать,Какие анимации могут совершать модели этой сущности, и как осуществляются переходы из одного состояния в другое,Какие эффекты может запускать данная сущность,Какие звуки может воспроизводить данная сущность,Какие области соприкосновения есть у данной сущности,Какие у сущности могут быть состояния, и каково поведение сущности в разных состояниях,Другие данные для использования в игровой логике: специальные тэги, группы, и т.д.,Стоит отметить, что у каждого экземпляра сущности есть собственная машина состояний, и у каждого состояния есть последовательность действий, которую сущность может проигрывать.,Например, можно задать состояние ""walk"" (ходьба) для сущности персонажа игры, и последовательность действий этого состояния может содержать команды, которые запускают нужную анимацию 3D модели, проигрывают звуки шагов и показывают эффект поднимающийся с земли пыли. Всё это описывается в текстовом файле, который можно редактировать и сразу тестировать, и это сильно ускоряет процесс разработки.,Карта — это файл данных, который содержит описание расположения сущностей. В этом файле также есть многоуровневая сетка ""плиток"", из которой можно построить ландшафт. Я не меняю файлы карт вручную — карты создаются с помощью встроенного редактора.,Так как движок знает всю информацию о сущностях, которые располагаются на карте, он в состоянии автоматически оптимизировать некоторые вещи. Например, если сущность является статичной и никогда не двигается (например, дерево или ландшафт), то движок автоматически объединяет её вместе с другими статичными сущностями, которые используют одинаковую текстуру, и создаёт одну общую 3D модель. Это значительно сокращает количество отображаемых видеокартой объектов, что сильно улучшает  производительность игры.,Некоторая часть игровой логики может быть описана в текстовых файлах, но она используется только для создания игровых сценариев, а не основной функциональности игры. Такая логика, как система соприкосновений, поведения искусственного интеллекта, правила игрового процесса и т.д. —  программируется в исходном коде Haxe, который превращается в C++ при компиляции.,Есть 2 категории файлов кода, связанных с логикой:,Процессоры логики сущностей — могут быть присоединены к отдельным сущностям, используются для обработки индивидуальной логики объектов (например, искусственного интеллекта). Не каждой сущности нужен процессор логики.,Ядро — одиночный класс, который описывает общую логику правил игрового процесса.,У меня была идея о пошаговой игре-головоломке, в которой игрок может управлять сразу несколькими персонажами, способными поднимать и переносить всякие предметы. Логика игры должна была быть пошаговой, потому что я хотел записывать каждый игровой шаг, чтобы дать игроку возможность отменить свои шаги, т.е. вернуться в прошлое.,Сначала я написал ядро логики и реализовал пошаговую систему. Для каждой сущности, которая является частью головоломки, ядро выставляет состояние.,Игрок может выделить персонажа и выполнить действие (например, переместиться, или поднять предмет), которое меняет состояние игрового мира. Игра потом вычисляет, является ли возможным новое состояние мира (например, не столкнулся ли игрок с препятствием), и проверяет, нужна ли какая-либо реакция на это изменение, со стороны других элементов  головоломки (например, если игрок положил какой-то предмет на кнопку, то у кнопки должно измениться состояние на ""нажатая""). Если все проверки пройдены, то изменения применяются и добавляются в историю шагов.,Игрок может отменить последний шаг, просто вернувшись в предыдущее состояние мира.,В этом заключается основная функциональность игрового ядра. На самом деле, всё немного сложнее, потому что мне нужно обеспечить плавные, анимированные переходы между состояниями, позволить элементам быть переносимыми другими элементами, и делать другие интересные вещи — но всё это добавляется к базовому ""фундаменту"" логики игры.,Реализация такой системы и всех крайностей заняла у меня примерно неделю. Оставшееся время заняли размышления о том, какую игру я хочу сделать. В это время у меня появилась идея о том, что персонажи могли бы переносить других персонажей, и даже создавать стопки из друг друга.,Я создал 4 уровня игры, используя мой существующий редактор карт, чтобы убедиться, что игровой процесс действительно интересный. Результат мне понравился, и я продолжил разработку.,Теперь у меня был рабочий прототип игры, и можно было начинать экспериментировать с художественными стилями. Я остановился на мультяшном визуальном стиле, и весь месяц занимался созданием 3D моделей, анимаций, эффектов, интерфейса, переходами, и т.д. Игра разбита на 5 тематических миров.,Я использую Blender для создания 3D моделей и анимаций, и GIMP для создания текстур.,Пластиковый вид игры достигнут с помощью написанного мною шейдера, который применяет заранее приготовленные данные об освещении к моделям. Работает это так: берётся заготовленная картинка освещённой сферы, применяется к некоторым частям модели, смешивая цветовые данные текстуры на основе нормалей модели в пространстве экрана.,Идея такого эффекта появилась у меня после работы с программами 3D моделирования и скульптуры. Подобный эффект в таких программах иногда называется MatCap. Мой подход заключается в том, что я смешиваю такой приём с традиционными алгоритмами освещения в реальном времени. Получается интересный эффект, который обрабатывается видеокартой очень быстро.,Весь месяц я улучшал User Experience игры: добивался плавности анимаций, хорошей чувствительности управления, чистоты графических элементов, удобности интерфейса.,Было реализовано меню паузы, меню выбора уровня, система последовательности уровней и системы сохранения — практически весь этот функционал у меня уже был создан для предыдущих игр, поэтому можно было повторно использовать часть этого кода, откорректировав некоторые визуальные вещи.,Работать над интерфейсами довольно утомительно, но плохой UX раздражает игроков, поэтому важно сделать всё правильно с самого начала.,Я решил сделать отполированный, полноценный ""вертикальный срез"" как можно быстрее. Таким образом я смог бы проверить, как бы выглядел готовый проект. Так как в игре на тот момент было мало контента, можно было свободно вносить глобальные изменения без особых проблем.,Я пишу музыку и создаю звуки в виртуальном модульном синтезаторе SunVox.  Я — самоучка, и создание музыки у меня занимает довольно много времени.,В моём движке есть система динамического звукового окружения, которая может генерировать окружающий шум в реальном времени, используя подготовленные звуки (например, шум волн или крики птиц), меняя их частоту и воспроизводя в разных направлениях и в разных комбинациях. Эта информация описана в отдельном JSON файле.,На тот момент у меня была полностью ""отполированная"" игра, в которой было всего 4 уровня. Я создал ещё 6, и выпустил демо версию игры.,Так я стал собирать отзывы от игроков на ранней стадии разработки, и смог на их основе улучшить User Experience игры.,Примерно в это время я добавил систему подсказок в игру. О ней я подробно написал ,.,Следующие два месяца я делал новые уровни, добавлял новые игровые элементы, используя ту систему сущностей, которую я описал выше. В исходном коде игры уже практически не было никаких изменений, потому что ядро игры уже было полностью готово.,Эти два месяца я в основном работал в редакторе карт.,В целом я удовлетворён результатом.,В начале разработки игры у меня уже было довольно чёткое представление о том, какую игру я хочу сделать. Поэтому процесс разработки прошёл намного плавнее и быстрее, чем обычно. Так я осознал важность наличия чёткой цели с самого начала.,Также оказалось хорошей идеей сделать отполированный вертикальный срез как можно скорее. Таким образом, я мог начать делать скриншоты, видео и даже выложить демо версию хорошего качества всего после 4 месяцев работы. Я получил несколько полезных отзывов от игроков ещё до того, как большая часть контента была готова, поэтому было проще вносить изменения, ничего при этом не ломая.,В этот раз я практически ничего не менял в ядре своего движка, и большая часть времени ушла непосредственно на создание игрового контента. Я буду продолжать использовать свой движок в будущих проектах.,Индивидуальный разработчик компьютерных игр.",Как я создаю игры на своём 3D движке в одиночку / Хабр
[<200 https://habr.com/ru/company/hostkey/blog/662592/>],page2,"В нашей , мы упустили, что видеопоток бывает и большего разрешения, поэтому стоит протестировать энкодинг файлов в формате 4К. Для полноты картины мы также сравним энкодинг на решениях от NVIDIA с встроенным GPU от Intel. Некоторые профессионалы полагают, будто достаточно собрать тот же FFmpeg с включенным QuickSync и внешняя видеокарта станет не нужна. Проверим и это утверждение.,Мы не будем подробно расписывать процесс тестирования для видеокарт от NVIDIA и зачем нам FFmpeg, поскольку информация об этом есть в предыдущих статьях (, и , части). Лучше сосредоточимся на новых результатах и полезных лайфхаках.,Используем , из ,, но установим в него видеокарту , с большим количеством блоков энкодинга, 24 ГБ видеопамяти и более высоким энергопотреблением.,Для начала проверим ее работу на количестве потоков, оказавшемся предельным для А4000 по результатам предыдущего теста:,gpu,pwr,gtemp,mtemp,sm,mem,dec,mclk,pclk,bar1,Idx,W,C,C,%,%,%,MHz,MHz,MB,0,97,47,-,92,3,0,7600,1920,33,  ,Удивительно! Мы получили сравнимые с результатом A4000 цифры. Несмотря на большую частоту работы чипа, больший объем используемой видеопамяти и большее энергопотребление, A5000 осилила энкодинг только 14 потоков и спасовала на пятнадцатом. Это фиаско еще раз доказывает, что профессиональные видеоадаптеры предназначены для других целей.,Теперь попробуем запустить трансляцию потока с разрешением 3840x2160 (оно же 4K), благо есть и такая версия ,. Энкодинг силами только центрального процессора захлебнулся уже на одном потоке, когда объем данных кратно увеличился:,  ,Каковы возможности GPU (помним, результаты у A4000 и A5000 сравнимы)? Это 3 потока. ,gpu,pwr,gtemp,mtemp,sm,mem,dec,mclk,pclk,bar1,Idx,W,C,C,%,%,%,MHz,MHz,MB,0,96,46,-,100,3,0,7600,1920,9,Как видим, по потребляемой мощности и загрузке блоков энкодинга видеочип работает явно не в режиме повышенного комфорта, хотя при этом расходуется лишь около 1 ГБ видеопамяти.,Вывод FFmpeg подтверждает, что видеокарта справляется:,А вот 4 потока адаптер уже не переваривает. Хотя загрузка железа остается примерно на тех же значениях, начинаются просадки по кадрам:,Если верить ,, технология QuickSync должна «используя специальные возможности обработки мультимедийных данных графических технологий Intel® для ускорения декодирования и кодирования, позволить процессору параллельно выполнять другие задачи и повышая быстродействие системы».,Для тестов понадобился подходящий процессор Intel (мы нашли машину с Core i9-9900K CPU @ 3.60GHz) и собранная с поддержкой Quick Sync утилита FFmpeg. С первым проблем не возникло (достаточно чипа старше 6-го поколения и наличия в нем GPU, что несложно ,), но сборка FFmpeg под тестовую Ubuntu 20.04 вызвала стойкие ассоциации с практическим освоением Камасутры. Чтобы не заставлять вас тратить драгоценное время, опишем, как нам удалось решить проблему.,Поскольку пакеты в репозиториях сломаны, первым делом нужно собрать и установить в систему библиотеки gmmlib и libva, а также последние версии Intel media driver и Media SDK. Для этого в домашней директории создадим папку GIT, зайдем в нее и выполним последовательно следующие команды (если будет не хватать каких-то зависимостей, установим их из репозитория; мы рекомендуем сделать ,):,Затем нужно собрать FFmpeg с помощью нескольких магических команд:,Стоит убедиться, что у нас появилась поддержка Quick Sync:,Вывод команды должен быть примерно таким:,Ура! Все готово к тестам.,Для начала проверим, как справляется с энкодингом видео в FullHD процессор без Quick Sync: он выдерживает максимум 4 потока, при которых все ядра загружены под 100%,Пятый поток процессор уже не осиливает, поэтому можно смело приступать к тесту с Quick Sync. В скрипте из предыдущей статьи для этого нужно будет заменить энкодер на h264_qsv, и он примет следующий вид (подробнее об использовании QuickSync с FFmpeg можно почитать ,):,Сразу делаем проверку на 6 потоках (+2 к тесту на чистом CPU):,Разница очевидна: загрузка процессора не превышает 50%, а имеющийся запас вычислительных ресурсов позволяет прогнозировать 11 – 12 итоговых потоков.,Ставим 11 потоков:,Загрузка процессора возрастает незначительно, но GPU уже подходит к пределу возможностей. Двенадцатый поток роняет битрейт и скорость обработки до 24 – 28 кадров.,Теперь проверяем потоки в 4K. В отличие от AMD, наш процессор Intel спокойно обрабатывает один поток в таком разрешении и без аппаратного ускорения:,На большее он, увы, не способен. С включенным Quick Sync тестовый компьютер смог вытянуть три потока с разрешением 4K:,Спасовал он только на четвертом, но столько же у нас выдержала и видеокарта Nvidia A5000.,Недостатки у решения, увы, тоже есть. При использовании модуля BMC (к примеру, при управлении машиной через IPMI), вы не получите доступ ко всем возможностям аппаратного ускорения, даже если GPU процессора будет определяться в системе. Придется выбирать между удобством удаленного управления или получением всех плюсов от использования Quick Sync.,Выводы вы можете сделать самостоятельно. Мы лишь отметим, что для энкодинга видео разница в мощности видеокарт не всегда определяется их ценой, а для решения некоторых задач стоит обратить внимание на специализированные технологии внутри центральных процессоров. Также мы использовали для тестов H264, но кодеки HEVC (H265) или VP1 в теории должны дать лучшие результаты, особенно на разрешениях 4K. Если вы самостоятельно проведете подобные тесты с первым (VP1 пока что представлен аппаратно и массово только для декодинга), поделитесь результатами в комментариях.,____________,Стоимость описанных выше экспериментов измерить просто: воспользуйтесь нашим ,.,Например, в самой простой конфигурации она следующая:,машина с A4000 обойдется в 22 000р, 12 потоков - 1800р на поток в месяц;,машина с A5000 обойдется в 31 000р, 14 потоков - 2214р на поток в месяц;,сервер на i9-9900K с QuickSync (QSV) обойдется в 5000-6000р, 11 потоков, 450р на поток.,Серверы для такого необходимо собирать на материнских платах без удаленного управления, что мы умеем. Обращайтесь!,Кстати, все серверы HOSTKEY предоставляются с нашим модулем полного удаленного управления сервером IPMI и ,. Об устройстве последней мы ,.,Пользователь",Многопоточный энкодинг: переплатить вдвое или уйти на «встройку»? / Хабр
[<200 https://habr.com/ru/post/662608/>],page2,"Я так давно пользуюсь услугами Github, что уже начал забывать, как это страшно потерять код, который целый день сочинял и отлаживал. Раньше для сохранения кода я использовал дискетки, потом cd-rom и переносной жесткий диск, потом пришли флешки. Все это для того, чтобы перенести код с рабочего компьютера на домашний и не потерять. И все эти,устройства постоянно ломались, терялись, у них заканчивался срок службы и т.п. ,Потом я завел свои ""облака"" и хранил код на своем железе и рабочих компьютерах. И наконец появился Github. По началу что-то ещё дублировалось на своих серверах и внешних дисках, но к сегодняшнему дню я настолько привык к сервису Github, все настолько удобно и надёжно, что страх того, что ""дискетка"" может сломаться, постепенно улетучился.,И тут на тебе! Оказывается, в любой момент, по не зависящим от меня причинам, меня могут отключить от этого технологического чуда! ,И ладно бы, если б я ходил на работу и писал неизвестно что, неизвестно зачем. Но у меня-то код весь свой, и кроме него ничего нет. Оказывается, что за время существования Github там у меня завелись сотни репозиториев и просто так - вручную - их не скопируешь.,Тем более, что работаем мы сегодня не на одном компьютере, и понять, где что лежит, сразу невозможно - все лежит в Github-е.,Короче. Слепил я программку для быстрого копирования всех своих репозиториев из Github, из всех своих пользователей и организаций, к которым у меня есть доступ. Программа на Go, потому что я последнее время использую только Go (ну, не считая vue и javascript для webapp).,Программа использует 'gh' (github-cli) и 'git'. С помощью gh получаем список репозиториев, а с помощью git клонируем репозиторий со всей историей коммитов, со всеми ветками и тегами. Это все можно сделать из командной строки, но если у вас много репозиториев, то легче с помощью программы.,Вот эти команды:,Как уже говорилось выше, gh выдает список репозиториев, а git клонирует репозиторий со всеми потрохами.,Перед использование программы в gh нужно залогиниться, т.е. выполнить комманду 'gh login' и убедиться, что у вас настроен доступ к Github по ключу ssh.,Далее, полученные архивы можно перенести на любой git-хостинг простыми командами:,Программа имеет параметры:,Пример запуска программы:,В этом случае программа будет искать репозитории у пользователя myuser и в организации myorg и загрузит только репозиторий github-backup пользователя myuser, как указано в пареметре -limit, если этот параметр убрать, то будут загружены все репозитории пользователя myuser и организации myorg. Если права для 'gh' и 'git' позволяют вам пользоваться приватными репозиториями, то будут загружены публичные и приватные репозитории, если не позволяют, то только публичные.,Программа размещена в привычном для меня ,, доступ пока еще есть:,Всё же надеюсь, что доступ к Github у нас останется, но, оказывается, в этом мире бэкап нужен всегда! ,Вот ещё один адресс на ,, пока он запасной: ,С уважением,,Kirill Scherba,Программист (вечный)",Быстрый бэкап всех ваших репозиториев Github / Хабр
[<200 https://habr.com/ru/company/timeweb/blog/649365/>],page2,Любопытный к миру человек,"20 игр, чтобы видеть детали, чувствовать нюансы и уловить смысл дизайна / Хабр"
[<200 https://habr.com/ru/post/662559/>],page2,"Это гифка, которую я сделал, чтобы показать вступление и как началась история путешествия птички. У меня есть друг, который не боится рисовать, даже если он не обучался рисованию профессионально. Я общаясь с ним как то вдохновился желанием рисовать и не бояться. В google play у меня есть старая игра, которую я делал на unity, когда только начинал работать с движком.,Два комментария к старой игре дали мне желание сделать новую версию, но уже на C++ + SDL2 + OPENGL ES 3.2 + OPENSLES + glm. То есть я даже рад хотя бы двум комментариям о том что людям нравиться моё творчество, чтобы чувствовать себя прекрасно и продолжать делать игры.,Так как у меня нормального опыта не было делать игры полноценные на sdl2, то я использовал разные виды кода, которые как я думал, что они правильные. Но поработав на работе и изучая код, я увидел что есть помимо того что я знаю (я про очереди сообщений), есть ещё mqueue. И только потом я додумался, что можно с помощью очередей сообщений отправлять из одного потока в другой что-нибудь. Вот пример как выглядела реализация.,Перед тем как использовать эту очередь, я удостоверился в том, что в android ndk есть заголовочный файл mqueue.,Я также посмотрел, есть ли OpenAL для android и оказалось, что она не входит в комплект и как почитал в интернете, что лучше писать для android на OpenSLES.,После того как были готовы рисунки для интро, я начал писать код для работы с загрузкой текстур и размещением вершин. В процессе работы я заметил, что можно отражать одну сторону в другую путем замены координат у вершинных буферов. Вот пример как выглядит начальная конфигурация вершин.,Я сначала думал что в unity делают правильно, что отсчитывают от центра и исходил из этого и определял экран как.,вроде такой был код.,Позже я понял неудобство и решил отсчитывать от левого верхнего угла и сделал так.,Вообще из-за того, что у меня нет большого опыта в разработке движков, то я наверное делаю ошибки такие, какие делают начинающие разработчики. Например конструктор спрайта выглядит так.,Вообще когда говорят что глобальные переменные это зло, то я думаю что они просто это от кого то услышали и приняли для себя такое же мнение, но мне например не удобно как оказалось передавать объект Common в конструктор. Лучше бы я просто пробросил с помощью extern размеры экрана и всё было бы чище. Да и ещё я по рассуждал, что можно для каждого шейдера отдельный класс создать, чтобы каждый спрайт заново не получал с помощью glGetUniformLocation позиции в шейдере. То есть после компиляции шейдера можно было бы получить все позиции и для спрайта указать например интерфейс к шейдеру или что нибудь подобное, чтобы просто уже было работать. Да и класс шейдера можно было бы интегрировать со спрайтом так, чтобы в рендере спрайта не менять ничего, если ты сменил шейдер. Хотя может я ошибаюсь, но я проработаю этот вопрос.,Еще я столкнулся с проблемами неправильного размера картинки. Спрайты были, одни короче, другие длиннее. Но я путем проб и ошибок выработал правило.,Вроде бы получилось правильно.,Для загрузки объектов я создал заголовок такого типа.,И если нужно загрузить какой то объект, то мы просто получаем на него ссылку, если он уже был загружен.,Да, можно было с помощью текста указывать какой объект загружать, но мне так больше нравиться, и нравиться еще из-за того, что легко получить эту ссылку на объект, если он уже был загружен. В link содержится все vao, vbo[2] и номера всех текстур.,Главное меню игры я сделал из одного спрайта, но на экране изображено две птицы. В момент рендера я отражаю спрайт по горизонтали и рисую в разных частях экрана. Вот как я составил код.,Оказалось не так уж и сложно отражать объект. Также можно отразить по вертикали, например поменяв местами координаты текстуры.,По OpenAL писать нечего, я сделал музыку специально для 44100 частоты и 16 битного формата вроде. По OpenSLES я скачал спецификацию и почитал немного, понял что надо посмотреть примеры реализации и банально переписал код, чтобы заработало на android.,При портировании на android как оказалось, что там нет mqueue реализации. Я нашел только syscall от ядра linux. Но если был syscall для открытия mq_open, то syscall для отправки не было и я подумал что надо искать другое решение. Так как я больше на C писал и на C++ опыта мало, то я конечно же не знал, что в C++ есть контейнер queue. И это было спасением, я сделал её глобальной рядом с функцией main и sdl потоке отправлял в нее event. А в game () файле я пробросил queue с помощью extern и получал события. И вуаля, всё работает.,Так как архитектуры различны, то я просто в ресурс добавил число 1. Если при прочитывании этой переменной, она не равно единице, то делаем смену из littleEngian в bigEngian.,Насчет шрифта freetype2. Я использовал старую сборку freetype, которая у меня на github, потому что новую так и не смог собрать для android.,Также, чтобы скомпилировать с OpenGLESv3, надо обратить внимание, что в ndk библиотеки с такой версией есть не ниже 18 api. Чтобы решить все проблемы с компиляцией, нужно в каталоге app в файле build.gradle сделать типа такого.,Важно в ndkBuild тоже указать платформу назначение и тогда компиляция сработает.,Ну и указать в app/jni/Application.mk версию api не забыть.,Учитывая прошлый опыт, я не стал на каждую игру заводить отдельный паблик, а сделал один основной и назвал - игры от xverizex. ,Игра, которую я написал, можно найти по кодовому названию в google play.,com.xverizex.fly_bird_2,Правда я всё ещё жду пока одобрят первую версию и пока она не доступна в маркете. Я хочу сделать её бесплатной в google play, а в huawei маркете, если это вообще возможно, то выставить цену на игру. Хотелось бы ещё зарабатывать на том что нравиться.,Игра по своей сути получилась относительно простой и поэтому её возможно было сделать за 5 дней. Да, на unity можно было бы за дня два или один сделать, но мне нравиться C и C++, разумеется я буду писать на том что мне нравиться. ,Это были мои все заметки, которые я запомнил за прошедшие пять дней разработки. Я писал по 12 или более часов почти каждый день и не мог уснуть, потому что было интересно. Но теперь нужно отдохнуть перед следующим заходом. Возможно новый уровень в этой игре или новая игра. ,Разработчик",Как я разрабатывал игру fly bird 2 / Хабр
[<200 https://habr.com/ru/post/662594/>],page2,"API Gateway является одним из обязательных компонентов архитектуры современных систем. Он может решать различные задачи:,Проксирование вызовов из одной точки в разные сервисы в разных форматах. Контроль доступа к ним.,Мониторинг.,Аутентификация и авторизация.,Агрегация данных (один запрос к API Gateway - несколько запросов к бэку).,Управление таймаутами.,Кэширование данных.,Валидация.,И многое другое.,При отсутствии API Gateway все эти задачи ложатся на бэкенд. И если для монолита это может быть оправдано, то в микросервисах любое изменение общей логики повлечёт доработку всех зависимых сервисов.,Есть много интересных готовых решений, в т.ч. и для нашего кейса (GraphQL <-> gRPC), но ни одно из них не подошло нам в полной мере.,На нашем проекте API Gateway нужен в основном для запросов с фронта. Когда мы только начинали, основным видом взаимодействия был REST. Он достаточно прост для понимания и реализации, но в процессе развития проекта начали проявляться его недостатки:,При обновлении сущности, даже если обновить нужно пару полей, мы обновляли весь объект. Да, можно использовать метод PATCH и парсить на бэке только переданные поля, рассматривая отдельно ситуации, когда поле передано со значением null и когда поле не передано, но это не стандартный подход для REST.,Не всегда при запросе на получение сущности нужны все поля. Можно сделать несколько методов вместо одного, но тогда для каждого кейса придётся делать отдельный метод. А если для одной и той же структуры у пользователей с разными ролями будут разные доступы к полям, которые, к тому же, могут меняться, реализовать это, используя REST, невозможно.,Нет пакетной обработки. Для получения данных из каждого сервиса фронт делает отдельные запросы.,GraphQL, ,, успешно решает эти проблемы. В качестве бонуса в GraphQL в отличие от REST есть ,. Их использование даёт возможность делать свои обработчики для полей запроса, императивно добавлять кэширование, валидацию и прочее.,С API для фронта разобрались, переходим на бэк. Проблемы остаются те же, за исключением пакетной обработки (на бэке таких кейсов нет и не предвидится). Можно изначально делать API бэка на GraphQL, это бы значительно упростило создание API Gateway. Но не всё так просто. ,Мы пишем на Java, используем ,, и для работы с GraphQL там есть неплохие на первый взгляд решения как для ,, так и для ,. С серверной частью проблем нет, но клиентская далека от совершенства., очень прост и минималистичен.,Однако такой клиент не даёт возможности делать частичное обновление или выбирать поля для формирования ответа., очень гибкий. Можно передавать какой угодно набор полей, можно использовать ,, можно задавать список полей, которые мы хотим получить в ответе.,Но это очень громоздко. К тому же при формировании запроса можно допустить ошибку.,В gRPC есть возможность генерировать типобезопасные контроллеры и клиенты. А для ограничения списка полей, которые возвращаются в ответ, есть специальный тип - ,.,В gRPC в качестве формата запросов-ответов используется protobuf. Его же мы используем для сообщений в асинхронном взаимодействии через kafka. Т.о. нам не нужно плодить форматы, и мы можем использовать на бэке что-то одно.,Часто, когда говорят о преимуществах gRPC, упоминают возможность потоковой передачи данных, снижающей накладные расходы. Мы к этому пока не пришли, но хотелось бы иметь такую возможность в будущем.,В protobuf есть ,, но её мы решили по-своему (об этом будет написано далее).,Ранее я писал, что на бэке мы используем Java в качестве основного языка программирования, поэтому хотелось бы найти именно готовое решение на нём. Единственное, что удалось найти, это проект ,. Выглядит интересно, но это не коробочное решение, а именно библиотека. К тому же для каждого эндпоинта нужно писать свой обработчик, что не очень удобно, т.к. при его добавлении/изменении придётся пересобирать проект. Ещё один минус - проект последнее время не развивается. Однако именно в его исходниках удалось почерпнуть много полезной информации для создания собственного API Gateway.,Ещё один интересный проект - , - написан на TypeScript и предоставляет возможность маппинга из GraphQL не только в gRPC, но и используя другие способы взаимодействия. Но он не даёт возможности кастомизировать маппинг типов и добавлять свои обработчики для запросов. Всё это можно было бы сделать самим, но пишем мы на Java.,Есть решения на Go, но они предполагают генерацию API Gateway из контракта, что нас не устраивает. В итоге было принято решение написать свой API Gateway на нашем стэке технологий.,Список основных требований к концепту:,Сервис должен быть написан на Java/Quarkus.,В сервисе не должно быть кодогенерации DTO и стабов, которые необходимо использовать для вызова сервисов, чтобы не пришлось пересобирать проект каждый раз после изменения API какого-либо вызываемого сервиса.,В сервисе должна быть возможность ""горячего обновления"" контракта. При изменении сервисов на бэке не должно быть необходимости править код API Gateway.,Поддержка batch-запросов., - зависимости фреймворка для работы с YAML-конфигом и gRPC., - добавляет скаляры GraphQL. Из коробки поддерживается не так много типов, поэтому без использования этой зависимости не обойтись., - добавляет директивы для валидации запросов., - нужно для логирования запросов-ответов от gRPC-сервера в формате JSON., - нужно для корректного логирования в формате JSON., - перчик., - нужно для конфигурации серверной части GraphQL с помощью... Vertx.,Хотя Quarkus и является основным фреймворком для приложения, но он, к сожалению, больше заточен на подход code-first, а мы строго придерживаемся подхода contract-first. К тому же при подходе code-first нам бы не удалось реализовать какой-то универсальный обработчик. Поэтому фактически от Quarkus в проекте будет только DI, конфигурация и запросы к gRPC-сервисам. А для создания и настройки основного обработчика больше подходит Vertx.,у Quarkus под капотом находится Vertx, что бывает очень полезно для написания каких-то низкоуровневых вещей. Серверная часть Quarkus полностью основана на Vertx, реактивные REST/GraphQL клиенты под капотом используют HTTP-клиент из Vertx, работа с БД идёт через библиотеки Vertx. Однако не все компоненты, для которых есть реализация в Vertx, переиспользуются в Quarkus. Серверная часть для GraphQL и обёртка для работы с gRPC в Quarkus написана без использования Vertx.,Как сконфигурировать серверную часть GraphQL, можно посмотреть в , и ,. В итоге у меня получилась такая конфигурация:,В основном методе выполняются следующие действия:,парсится схема graphql,добавляются обработчики аннотаций (в данном случае это , из библиотеки ,, который обеспечивает обработку директивы ,),добавляются обработчики для методов , и , (в нашем случае это один обработчик для всех методов),добавляются скаляры ,, ,, , для поддержки соответствующих типов,добавляется , для логирования.,Тут стоит обратить внимание на ,. За счёт этого параметра мы делаем возможным выполнение , к API Gateway.,У меня роутинг зашит прямо в конфигурационном файле приложения. Это оптимальное решение для концепта, но для реального рабочего проекта лучше использовать централизованное хранилище (БД, внешний кэш), т.к. при большом количестве сервисов конфигурационный файл будет очень громоздким и нечитаемым.,Подробнее про конфигурацию Quarkus-приложения можно почитать в ,. Для простоты я не делал маппинг методов GraphQL в методы gRPC и считаю, что у они у нас будут называться одинаково. Т.о. мне остаётся лишь смаппить метод GraphQL в сервис gRPC.,Proto - бинарный формат со строгой типизацией. Поэтому для формирования запроса и парсинга ответа необходимо иметь у себя соответствующий контракт, на основе которого генерируются DTO запросов/ответов и стабы. С учётом того, что контракты gRPC-сервисов лежат в них самих, получать их нужно именно оттуда. Примеров не так много, я нашёл ,, и выглядит он страшновато. В нём нас интересует метод ,. После небольшого рефакторинга у меня получилось следующее:, - полное название сервиса, которое я получаю из конфига, например ,., В коде клиента стоит обратить внимание на метод ,, возвращающий ответ, обёрнутый в ,. В Quarkus активно используется библиотека ,, а Uni - это одно из представлений результата, аналог Mono из ,.,Из полученного ответа нам требуется извлечь дескриптор сервиса, в котором хранится информация о сущностях и методах. В ответе сервиса приходит , в бинарном виде. Из него необходимо получить информацию о контракте с учётом всех его зависимостей, т.к. контракт может содержать информацию из разных файлов, включая proto-файлы из стандартной библиотеки и импортированных библиотек.,Маппинг JSON-запроса в запрос к gRPC-сервису я подсмотрел в проекте Rejoiner, который упоминал ранее, и добавил туда маппинг классов-обёрток для nullable-типов.,В protobuf нельзя передать null-значение. В отдельных случаях можно использовать стандартные классы-обёртки, которые есть в библиотеке, такие как StringValue, BooleanValue, рассматривая их значение по умолчанию как null. Но если для строк (в нашем случае) это будет работать, т.к. значением по умолчанию там является пустая строка, то 0 для числовых значений для нас неприемлем. Я подготовил небольшой proto-файл с обёртками для примитивов, в котором используется google.protobuf.NullValue для корректной работы с null.,Ограничения маппинга:,Нельзя смаппить запрос, принимающий более одного параметра.,Нет маппинга для ,. Исключением являются кастомные nullable-типы.,Для перечислений на стороне gRPC сервисов необходимо задавать какое-то неиспользуемое значение по умолчанию, которое можно принимать за null.,Нет маппинга для ,, потому что в GraphQL нет аналога. Не то, чтобы я часто пользовался этим типом в gRPC, но в паре REST-методов он есть.,Маппинг запрашиваемых полей в FieldMask возможен, только если FieldMask лежит в корне структуры в proto. В дальнейшем это будет исправлено.,Кроме самого запроса необходимо подготовить дескриптор метода, который включает в себя тип запроса (синхронный запрос, стриминг), название метода и маршаллеры запроса/ответа.,Запрос делается аналогично запросу на получение дескриптора сервиса.,В самом простом случае для преобразования ответа в JSON можно было бы использовать метод ,, но там нет и быть не может нормального маппинга nullable-значений, даты и даты-времени. В остальном маппинг в JSON достаточно прост и представляет собой рекурсивный обход ответа и преобразование его в Map<String, Object>.,Готовый проект можно посмотреть на ,. Он включает в себя ,, , и ,. Несмотря на имеющиеся ограничения, этот проект может быть основой полноценного API Gateway.,Одним из требований к приложению было наличие возможности перезагрузки контракта без остановки приложения. Это не имеет смысла, если хранить контракт в самом приложении (как сейчас), но если держать его в БД/кэше/хранилище схем, функция очень полезная. У меня это реализуется за счёт пересоздания ,.,Новый контракт становится актуальным сразу после загрузки.,Изначально на каждый запрос с фронта делался запрос на получение дескриптора. И хотя выполняется он достаточно быстро, нет смысла делать его каждый раз, т.к. контракт меняется нечасто. Поэтому ответ можно закэшировать. Для кэширования я взял библиотеку ,, использующую ,.,Несмотря на то, что кэшируемый метод возвращает Uni, кэшируется именно результат. При повторном запросе по тому же ключу запрос в gRPC-сервис за дескриптором уже не делается. Если возвращается ошибка при запросе к дескриптору, результат не кэшируется.,Умный, красивый, скромный",GraphQL &lt;-&gt; gRPC API Gateway на Java / Хабр
[<200 https://habr.com/ru/post/662596/>],page2,"Не нужно быть сверхразумным искусственным интеллектом, чтобы понять: двигаться навстречу величайшему событию в истории человечества и не готовиться к этому – просто глупо.,Непросто так эти слова являются эпиграфом к данной статье, ведь в век информационных технологий и проходящей незаметно для всех обывателей технической революции только слепой не заметит насколько стремительно проходит процесс компьютеризации и роботизации с внедрением искусственного интеллекта во все сферы деятельности человека.,Увеличение мощности компьютеров, уменьшение их размеров и снижение цены производства – следствия так называемого «закона Мура», который был сформулирован больше пятидесяти лет назад , – тогда еще будущем основателем компании Intel.,На самом деле это просто эмпирическое, то есть основанное на опыте наблюдение: мощность компьютеров, обусловленная увеличением количества транзисторов, уменьшающихся на кристалле интегральной платы, а также ростом их таковой частоты, удваивается каждые 18 месяцев. Тогда как с ценой происходит обратная ситуация – примерно каждые два года она в два раза уменьшается. ,Журнал «Scientific American» привёл такую аналогию: если бы авиапромышленность последние 25 следовала «закону Мура», то сейчас Boeing 767 стоил бы 500 долларов, совершал облет земного шара за 20 минут и затрачивал на это менее 20 литров топлива,Да, цифровой мир живет по своим закона Мура и Курцвейла.,Говоря о стремительном развитии информационных технологий нельзя не сказать уже о вышеупомянутом американском ученном ,, который известен научными технологическими прогнозами, учитывающими появление искусственного интеллекта и средств радикального продления жизни людей. Он придумал так называемый закон ускоренной отдачи, согласно которой развитие технологий происходит экспоненциально, то есть чем мощнее становится та или иная технология, тем большее ускорение в своем развитии она приобретает.,Для большего понимания достаточно вспомнить притчу о шахматах, согласно которой мудрец, изобретший шахматы, по имени Сисса, в качестве награды у короля попросил за первую клетку одно зерно пшеницы, за вторую – два, за третью – четыре и так далее. Нетерпеливый король не задумываясь согласился на условия мудреца, но количество зерна превысило весь урожай пшеницы, собранный за всю историю человечества. Эта простенькая математическая задача демонстрирует высокий рост экспоненциальной последовательности.,Получив представление об окружающей нас обстановке и помечтав о прекрасном будущем, в котором нас будут окружать технологии, способствующие увеличению среднего возраста жизни, о мире в котором нет болезней, в котором нам стоит лишь подумать о чем-либо, а это уже действительность, вернемся в реальность.,Touch ID, Face ID – эти слова уже на слуху у всех, но вы когда-нибудь задумывались как они могут быть использованы против вас? Мы не задумываясь указываем их везде и повсеместно, но не стоит забывать, что существуют еще идентификации по радужной оболочке глаз, геометрии руки, томографии лица, ДНК, акустических характеристик уха, рисунку вен. Такие данные создают полный портрет их носителя и выкладывают на блюдечке всю информацию о человеке.  ,В последнее время очень много людей говорят о том, что банки собирают биометрические данные своих клиентов. Это позволит клиентам, во-первых, пользоваться банковскими услугами через интернет, во-вторых, банк обещает, что скоро их банкоматы будут узнавать своих клиентов в лицо. Также постепенно роботы заменят сотрудников операционистов во всех его отделениях, а для того, чтобы робот смог работать с клиентом нужны биометрические данные этого клиента. В итоге уже сейчас сотрудники многих банков сообщают, что это для вашего блага и удобства. Они пытаются всяческим путем хотят взять биометрические персональные данные - образец вашего голоса и изображения лица. Да-да, именно всяческими путями, об этом подробнее расскажу далее. Желание банков автоматизировать работу с клиентами и заменить банковских работников способных совершать ошибки на автоматы вполне понятно, однако проблема в том, что люди ещё не привыкли доверять банкам и тем более если речь идет о предоставлению банку своих биометрических данных. Люди опасаются, что такие данные банки могут передавать третьим лицам, например, коллекторам. Банки могут утерять такие данные и они утекут в открытый доступ, как было недавно с базами данных клиентов нескольких крупных банков страны. В конце концов с прогрессом дружат не только банки, но от него не отстают, а иногда и идут на шаг впереди и мошенники. Например, банки хотят в ближайшем будущем выдавать кредиты через интернет с распознаванием обратившегося за кредитом человека по его биометрическим данным, если такой человек предоставлял любому банку такие данные ранее при оформлении кредита. Но возможно и здесь мошенники преуспеют и научатся подделывать такие данные, поэтому люди испытывают некоторые опасения.,Биометрия и искусственный интеллект, как мы уже поняли, непосредственно связаны друг с другом. С каждым годом становится все больше данных о клиентах банка, они растут в геометрической прогрессии. С каждой новой порцией ценность отдельно взятой изолированной информации снижается. Иными словами, сигналов становится больше, но при этом каждый из них все слабее. Лишь анализ их совокупности позволит выработать сильные сигналы. Поэтому рано или поздно искусственный интеллект станет неотъемлемой частью.,Теперь давайте рассмотрим такой вопрос. Можно ли отказаться от предоставления биометрии банку и как это может повлиять на позицию банка по выдаче кредита карты или оформление другого банковского продукта? Сегодня иногда бывает так, человек приходит в банк за оформлением документов и его никто не спрашивает «согласен ли гражданин пройти процедуру сдачи биометрии или нет?». На него просто направляют камеру и просят сказать несколько слов. Конечно же такие действия сотрудника банка незаконны в соответствии с законом номер 115 «О противодействии легализации доходов, полученных преступным путем», банкам разрешается размещать и обновлять биометрические персональные данные гражданина в электронной базе, но только при его добровольном согласии на такую процедуру. Но сотрудник банка может сумничать и сказать, что вы уже раньше давали согласие на передачу своих персональных данных банку, однако персональные данные и биометрические данные это разные вещи. Первое это сведение вашего паспорта, а второе это ваше отображение на фото, видео, отпечатки ваших пальцев, радужка оболочки глаз, образцы волос и другие биологические сведения. Поэтому если даже клиент и подписал ранее договор, где есть пункт, что он согласен на передачу, обработку банком персональных данных, у банка все равно нет права записывать биометрические сведения такого человека. Если обратить внимание на статью 11 федерального закона о персональных данных, то в ней конкретизировано, что получать биометрические данные можно только с письменного согласия человека, то есть прежде чем фотографировать, снимать вас на видео и записывать ваш голос сотрудник банка должен показать вам в договоре пункт в котором указано то, что вы согласны предоставить банку свои биометрические данные и только после подписания такого договора и тем самым вашего согласия, на вас уже могут нацеливать объектив камеры. В случае нарушения вашего права и получения банком вашего изображения без вашего письменного согласия, можно обратиться в суд с исковым заявлением о возмещении морального вреда, причиненного незаконными и непрофессиональными действиями банка. ,Таким образом, действуя на опережение, государство должно задуматься о создании действенных механизмов защиты разрабатываемых геномных баз данных, в том числе путем совершенствования правовой системы. Защита же генетической информации и генетических баз данных, гарантирующая их безопасность от всякого рода посягательств, должна осуществляться именно на государственном уровне под контролем соответствующих государственных органов.,Да, это конечно всё понятно, что цепочка человек – биометрические данные – человек может контролироваться законом, но давайте рассмотрим такую цепочку как человек – биометрические данные – робот. Задумались? Может и при нынешнем уровне развития искусственного интеллекта человечеству не грозит восстание машин. Более актуальной угрозой представляется применение недостаточно обученного искусственного интеллекта в жизненно важных сферах, а также попытки его использования для того, чтобы поставить под контроль жизнь человека. ,В будущем может такая угроза, как самостоятельного принятия решений искусственным интеллектом. Ситуация заключается в том, что, когда система получит модель самой себя, то у нас появится искусственный интеллект, который может отказаться выполнять поставленную ему задачу и начнет принимать собственные решения. Для этого необходимо заранее предусмотреть все меры, препятствующие выходу искусственного интеллекта из-под контроля.,Не менее утешительные прогнозы совсем недавно продемонстрировала нейросеть,MT-NLG, разработанная Оксфордским университетом, которая в свою очередь заявила, что: «Способность предоставлять информацию, а не товары и услуги, станет определяющей чертой экономики 21 века. Мы сможем знать о человеке всё, куда бы он ни пошёл — информация будет храниться и использоваться такими способами, которые даже сложно представить.» ,«Не стоит недооценивать искусственный интеллект!» – это главный вывод. По сути мышление – поведенческий навык, а всякий навык имеет свойство утрачиваться, что касается человеческого мозга. Никто не отменял принцип «мягкой силы», используемый искусственным интеллектом на невидимом поле боя, поэтому подытожив все вышеперечисленное, скажу, что жизнь человеческого рода полностью лежит на его же плечах, с какой долей ответственности человек подойдет к своему развитию, так он и будет существовать. ,Пользователь",Друг или враг? Стоит ли бояться искусственного интеллекта? / Хабр
[<200 https://habr.com/ru/post/662628/>],page2,"Техника стирания типов - type erasure - известна довольно давно, хоть её и долго не замечали. Тем не менее кажется только в последнюю декаду она стала из игрушки и костылей превращаться в мощный инструмент, использующийся каждый день в разработке.,Если спросить современного С++ разработчика какие примеры type erasure он видел / использовал, то вероятно он ответит что то про std::function и возможно про std::any, но это лишь малая часть всех применений этого замечательного инструмента!,В статье я постараюсь описать все возможные виды type erasure в современном С++, но начать стоит с определения.,Стирание типов - потеря части информации о типе так, чтобы получившийся после ""стирания всего ненужного"" тип был общим для всех стираемых, а значит его можно будет использовать в рамках системы типов, опираясь только на оставшуюся после стирания информацию.,Начнём с того, что было уже в С и о чём часто забывают говоря об erasure, - мы стёрли всю информацию о типе под указателем, не можем ничего прочитать, но с другой стороны доступ к данным у нас абсолютно без оверхеда! Достаточно , тип. Часто внутри именно на этом и построены другие более сложные стирания. Ну и конечно примерно в эту труху из байтов компилятор перетирает всю нашу систему типов в процессе работы.,Кстати, насчёт байтов:, так исторически сложилось, что в С все использовали чары для работы с сырыми байтами, поэтому для них в языке С++ исключение и указатель на них можно приводить к указателю на любой другой тип. Это не обходится без последствий и иногда из-за этого строки теряют некоторые оптимизации, поэтому сначала добавили std::byte, а потом начали потихоньку заменять чары (char8_t since C++20), но это уже совсем другая история. В контексте стирания типов нам важно, что мы получили способность читать данные из стёртого типа, а составив массив мы получим ещё и верхнюю границу размера типа, что конечно немного, но с void и так нельзя.,Вот мы тут про указатели поговорили. Но вы к ним приглядитесь получше. Видите стирание типов? А оно есть, стирает бесконечное число типов массивов T[1] T[2] T[3] и т.д., при этом он массивы в С неявно приводятся к указателям. Это конечно прогрессивное решение для времён зарождения С... И спорное. Кстати заметьте - ссылка из С++ не делает ничего подобного, под ней вы уверены что лежит ровно одна штука! А под указателем от 0 до бесконечности! Тут мы уже точно знаем тип, но не знаем сколько там штук.,Ну, кажется с сишными стираниями покончено. Перейдём к С++?,И ВНЕЗАПНО , ! Вот уж тут многие незамечают никакого стирания. Но на самом деле мы стираем все возможные массивы чаров, указатели, строки (а некоторые в плохом коде ещё и массивы байтов(НЕ ДЕЛАЙТЕ ТАК)),Получаем мы одну удобную обёртку для всего этого, с которой можно без потери производительности и понятности работать, сократив количество бойлер плейта на написание всяких перегрузок до 0. Хотя стоит отметить, что относительно массивов char нам приходится хранить размер, который мы могли бы знать на компиляции.,Чуть шире , - обёртка над лежащими в памяти подряд элементами типа Т. Стирается только контейнер в котором лежали элементы(например string, vector, сишный массив, std::array и т.д.),И только тут наконец мы дошли до ,, которые могли бы прийти к вам в голову первыми при упоминании type erasure.,Если включить в эту группу также похожие на них , и прочие ,а потом хорошенько приглядеться, то окажется что это всё одна и та же форма стирания типов, а именно разделение байт хранящих значение и функций обрабатывающих эти данные + замена обрабатывающих функций на рантайме с помощью чего-то наподобие vtable.,Я бы отнёс это в общую категорию ""стирание типов для использования в полиморфном контексте"" как бы сложно это ни звучало,Сейчас в С++ уже есть удобные , для унификации создания таких типов, хотя пока они и не в стандартной библиотеке.,Но не думайте, что это всё! Сначала разберём интересный случай - , мы вроде как добиваемся полиморфного поведения через std::visit, но с другой стороны сохраняем всю полноту информации о типах(и теряем только информацию о том что же там хранится в данный момент). К какой категории это относить - пусть решают учёные, а мы просто пропустим эту штуковину,Для подготовки к последнему мы должны упомянуть ещё один сишный способ стирания,Указатель... На функцию.под тип ,мы можем положить произвольную функцию и вызвать, а значит добиться полиморфного поведения. При этом важно подметить, что функция возвращающая ничего и принимающая , это не меньшие возможности, как могло бы показаться(т.к. мы снизили количество входных аргументов), а буквально любая функция, так как void* можно реинтерпретировать как что угодно, в том числе пак параметров + указатель на возвращаемое значение,Осталось посмотреть на последний и самый оригинальный тип стирания типов - корутины!,Представим вот такой С++20 код,Эти 2 ""таски"" делают разные вещи, под ними внутри генерируются разные типы ""стейт машин"", но для наблюдателя их тип одинаковый. А вот поведение при исполнении - разное. Получается мы тут где то стёрли тип! При этом мы получили возможность сохранить состояние вместе с кодом (в отличие от простого указателя на функцию),Дополняет это всё std::coroutine_handle<void>, который стирает тип любого другого хендла.То есть мы стёрли тип хендла на корутину, которая стёрла тип состояния корутины, которое стёрло в себе полиморное поведение объекта... Кажется эта штука набирает обороты. Интересно как будет выглядеть С++ будущего и какие техники мы сейчас не замечаем также, как в С не замечали стирания типов?,Пользователь",Обзор всего доступного в С++ type erasure / Хабр
[<200 https://habr.com/ru/post/662632/>],page2,"Приветствую, читатель! Хочу рассказать о том, как удалось прикрутить гугл аналитику к Telegram боту на aiogram.,Дело в том, что гугл объявил о закрытии Universal Analytics и , на Google Analytics 4. Погуглив стало ясно что инфы про новую аналитику крайне мало, а о её работе с питоном инфы в принципе нет.,Первая проблема связанна с тем, что гугл аналитика предназначена для того что бы отслеживать сайты посредством встраивания трекера во фронтэнд или приложение с SDK.,Бот находится по адресу , (Он позволяет проверять баланс крипто-кошелька в сети BSC, но сейчас не об этом). Его можно добавить как ""сайт"", но встроить счетчик аналитики просто так не выйдет, так как фронтэндом является сам телеграм, а мы лишь пишем бекэнд и взаимодействуем через Telegram bot API.,Очевидно, из-за этого обстоятельства нам не подходят стандартные способы сбора аналитики. А так же любые библиотеки, которыми так славится питон, являются устаревшими из-за перехода на новую версию.,На помощь приходит Measurement Protocol API. Эта функция всё еще находится в бете, но гугл обещает не вносить значительных изменений на данном этапе, однако существуют ,, которые планируют исправить к моменту релиза. ,Здесь стоит обратить внимание, что Measurement Protocol существовал и ранее, но его значительно изменили, и теперь есть две версии документации, ,.,Для того что бы зафиксировать событие нам нужно оправить следующие данные:,На этот хост:   ,Где:,MEASUREMENT_ID - идентификатор потока данных (G- код),API_SECRET - значение секретного ключа, который нужно создать во вкладке Measurement Protocol API.,Таким образом удалось собрать данные о нажатии кнопки:,Но возникла следующая проблема: нет записей о количестве активных пользователей. Решением стало вручную отправлять параметр времени взаимодействия в каждом ивенте.,Поскольку никаких данных о географии мы собрать не можем, было решено отправлять код языка, который установлен у пользователей в клиенте телеграма, отдельным параметром. Эти данные можно вытащить из объекта message. ,Так как бот был написан на aiogram, который в свою очередь взял за основу aiohttp, именно его и будем использовать для отправки аналитики. Итоговый код выглядит так:   ,Вызываем функцию во всех хендлерах, которые хотим трекать, следующим образом:,Пользователь",Telegram bot на aiogram + Google Analytics 4 / Хабр
[<200 https://habr.com/ru/post/662549/>],page2,"Погрузитесь глубоко в новую архитектуру React под названием Fiber и узнайте о двух основных фазах нового алгоритма согласования (reconciliation). Мы подробно рассмотрим, как React обновляет состояние и пропсы и обрабатывает дочерние элементы.,React - это JavaScript библиотека для создания пользовательских интерфейсов. В ее основе лежит ,, который отслеживает изменения в состоянии компонента и проецирует обновленное состояние на экран. В React мы знаем этот процесс как , (reconciliation). Мы вызываем метод ,, фреймворк проверяет, изменилось ли состояние или пропс, и перерендеривает компонент в UI.,Документация React предоставляет , механизма: роль элементов React, методы жизненного цикла и метод ,, а также алгоритм диффиринга (сравнения, diffing), применяемый к дочерним элементам компонента. Дерево иммутабельных элементов React, возвращаемых методом ,, обычно называют ""виртуальный DOM"". Этот термин помог объяснить React людям в самом начале, но он также вызвал путаницу и больше не используется в документации по React. В этой статье я буду называть его деревом React-элементов.,Помимо дерева React-элементов, фреймворк всегда имел дерево внутренних экземпляров (компонентов, узлов DOM и т.д.), используемых для хранения состояния. Начиная с версии 16, React развернул новую реализацию этого дерева внутренних экземпляров и алгоритма, который управляет им, под кодовым названием ,. Чтобы узнать о преимуществах архитектуры Fiber, ознакомьтесь с ,.,Это первая статья из цикла, цель которого - научить вас внутренней архитектуре React. В этой статье я хочу предоставить углубленный обзор важных концепций и структур данных, имеющих отношение к алгоритму. Как только мы получим достаточную базу, мы изучим алгоритм и основные функции, используемые для обхода и обработки fiber-дерева. В следующих статьях цикла будет показано, как React использует алгоритм для выполнения начального рендеринга и обработки обновлений состояния и пропсов. Далее мы перейдем к деталям планировщика, процессу согласования (reconciliation) дочерних элементов и механизму построения списка эффектов.,Здесь я собираюсь дать вам довольно продвинутые знания. Я рекомендую вам прочитать ее, чтобы понять магию, скрывающуюся за внутренними механизмами Concurrent React. Эта серия статей также послужит вам отличным руководством, если вы планируете начать вносить свой вклад в React. Я ,, поэтому здесь будет много ссылок на исходники недавней версии 16.6.0.,Это, безусловно, довольно много, так что не переживайте, если вы не поймете что-то сразу. Это займет время, как и все стоящее. ,Вот простое приложение, которое я буду использовать на протяжении всей серии. У нас есть кнопка, которая просто увеличивает число, отображаемое на экране:,А вот и реализация:,Вы можете поиграть с ним ,. Как вы можете видеть, это простой компонент, который возвращает два дочерних элемента , и , из метода ,. Как только вы нажимаете на кнопку, состояние компонента обновляется внутри обработчика. Это, в свою очередь, приводит к обновлению текста элемента ,.,Во время , (reconciliation) React выполняет различные действия. Например, вот операции высокого уровня, которые React выполняет во время первого рендеринга и после обновления состояния в нашем простом приложении:,обновляет свойство , в , в ,.,извлекает и сравнивает дочерние элементы , и их пропсы,обновляет пропсы элемента ,Есть и другие действия, выполняемые во время ,, такие как вызов , или обновление ,. , Тип работы обычно зависит от типа элемента React. Например, для классового компонента React должен создать экземпляр, в то время как для функционального компонента он этого не делает. Как вы знаете, в React есть много видов элементов, например, классовые и функциональные компоненты, компоненты-хосты (DOM узлы), порталы и т. д. Тип элемента React определяется первым параметром функции ,. Эта функция обычно используется в методе , для создания элемента.,Прежде чем приступить к изучению действий и основного fiber алгоритма, давайте сначала познакомимся со структурами данных, используемыми внутри React.,Каждый компонент в React имеет UI-представление, которое мы можем назвать представлением или шаблоном, возвращаемым методом ,. Вот шаблон для нашего компонента ,:,Когда шаблон проходит через JSX-компилятор, в итоге вы получаете набор React-элементов. Это то, что действительно возвращается из метода , компонентов React, а не HTML. Поскольку от нас не требуется обязательно использовать JSX, метод , для нашего компонента , можно переписать следующим образом:,Вызовы , в методе , создадут две структуры данных:,Вы можете видеть, что React добавляет свойство , к этим объектам, чтобы однозначно идентифицировать их как React элементы. Затем у нас есть свойства ,, , и ,, которые описывают элемент. Значения берутся из того, что вы передаете в функцию ,. Обратите внимание, как React представляет текстовое содержимое в качестве дочерних элементов узлов , и ,. А обработчик клика является частью пропсов элемента ,. Существуют и другие поля в элементах React, например, поле ,, которые выходят за рамки этой статьи.,У элемента React созданного для , нет ни пропсов, ни ключа:,Во время , (reconciliation) данные каждого React элемента, возвращенные из метода ,, объединяются в дерево fiber-узлов. Каждый React элемент имеет соответствующий fiber-узел. В отличие от React элементов, fibers не создаются заново при каждом рендере. Это мутабельные структуры данных, которые хранят состояние компонентов и DOM.,Ранее мы обсуждали, что в зависимости от типа React элемента фреймворк должен выполнять различные действия. В нашем примере приложения для классового компонента , он вызывает методы жизненного цикла и метод ,, в то время как для хост-компонента , (DOM узел) он выполняет мутацию DOM. Таким образом, каждый элемент React преобразуется в Fiber-узел ,, который описывает работу, которую необходимо выполнить.,.,Когда элемент React впервые преобразуется в fiber-узел, React использует данные из элемента для создания fiber в функции ,. При последующих обновлениях React повторно использует fiber-узел и просто обновляет необходимые свойства, используя данные из соответствующего React-элемента. React также может потребоваться переместить узел в иерархии на основе пропсов , или удалить его, если соответствующий элемент React больше не возвращается из метода ,.,Посмотрите функцию ,, чтобы увидеть список всех действий и соответствующих функций, которые React выполняет для существующих fiber-узлов.,Поскольку React создает fiber для каждого React элемента, и поскольку у нас есть дерево этих элементов, у нас будет дерево fiber-узлов. В случае с нашим примером приложения это выглядит следующим образом:,Все fiber-узлы связаны между собой через связный список (linked list), используя следующие свойства fiber-узлов: ,, , и ,. Более подробно о том, почему это работает именно так, читайте в моей статье ,, если вы еще не читали ее.,После первого рендеринга React заканчивает работу с fiber-деревом, которое отражает состояние приложения, использованного для отрисовки пользовательского интерфейса. Это дерево часто называют , (current). Когда React начинает работать над обновлениями, он строит так называемое , дерево, которое отражает будущее состояние, которое будет выведено на экран.,Вся работа выполняется над fibers из дерева ,. Когда React проходит через , дерево, для каждого существующего fiber-узла он создает альтернативный узел, который составляет ,дерево. Этот узел создается с использованием данных из элементов React, возвращаемых методом ,. Как только обновления будут обработаны и вся связанная с ними работа будет завершена, React получит альтернативное дерево, готовое к выводу на экран. Как только это , дерево будет выведено на экран, оно станет , деревом.,Один из основных принципов React - последовательность. React всегда обновляет DOM за один раз - он не показывает частичные результаты. Дерево , служит в качестве ""черновика"", который не виден пользователю, чтобы React мог сначала обработать все компоненты, а затем вывести их изменения на экран.,В исходных текстах вы увидите множество функций, которые берут fiber-узлы из обоих деревьев , и ,. Вот сигнатура одной из таких функций:,Каждый fiber-узел содержит ссылку на свой аналог из другого дерева в поле ,. Узел из , дерева указывает на узел из дерева , и наоборот.,Мы можем думать о компоненте в React как о функции, которая использует состояние и пропсы для вычисления UI-представления. Любые другие действия, такие как мутирование DOM или вызов методов жизненного цикла, следует рассматривать как побочный эффект или, просто, эффект. Эффекты также упоминаются ,:,Вероятно, вам уже приходилось выполнять выборку данных, подписку или , из React компонентов. Мы называем эти операции ""побочными эффектами"" (или сокращенно ""эффектами""), потому что они могут повлиять на другие компоненты и не могут быть выполнены во время рендеринга.,Вы можете видеть, как большинство обновлений состояния и пропсов приводят к побочным эффектам. А поскольку применение эффектов - это один из видов работы, узел fiber - это удобный механизм для отслеживания эффектов в дополнение к обновлениям. Каждый узел волокна может иметь эффекты, связанные с ним. Они кодируются в поле ,.,Таким образом, эффекты в Fiber в основном определяют ,, которую необходимо выполнить для экземпляров после обработки обновлений. Для хост компонентов (DOM элементов) работа заключается в добавлении, обновлении или удалении элементов. Для классовых компонентов React может потребоваться обновление refs и вызов методов жизненного цикла , и ,. Существуют также другие эффекты, соответствующие другим типам fiber-ов.,React обрабатывает обновления очень быстро, и для достижения такого уровня производительности он использует несколько интересных приемов. , Итерация линейного списка намного быстрее, чем дерева, и нет необходимости тратить время на узлы без побочных эффектов.,Цель этого списка - пометить узлы, которые имеют DOM обновления или другие эффекты, связанные с ними. Этот список является подмножеством дерева , и связан посредством свойства , вместо свойства ,, используемого в деревьях , и ,., предложил аналогию для списка эффектов. Ему нравится думать о нем как о рождественской елке, с ""рождественскими огнями"", связывающими все узлы эффектов вместе. Чтобы визуализировать это, давайте представим следующее дерево из fiber-узлов, где выделенные узлы выполняют определенную работу. Например, в результате нашего обновления , был вставлен в DOM, , и , изменили атрибуты, а , запустил метод жизненного цикла. Список эффектов свяжет их вместе, чтобы React мог пропустить другие узлы позже:,Вы можете видеть, как узлы с эффектами связаны друг с другом. При переходе по узлам React использует указатель ,, чтобы определить, с чего начинается список. Таким образом, приведенную выше диаграмму можно представить в виде линейного списка следующим образом:,В каждом React-приложении есть один или несколько элементов DOM, которые выступают в качестве контейнеров. В нашем случае это элемент , с ID ,.,React создает объект , для каждого из этих контейнеров. Вы можете получить к нему доступ, используя ссылку на DOM элемент:,Этот fiber root является местом, где React хранит ссылку на fiber tree. Она хранится в свойстве , fiber root:,Fiber-дерево начинается со , fiber-узла, которым является ,. Он создается внутри и действует как родитель для вашего самого верхнего компонента. Существует связь от fiber-узла , обратно к , через свойство ,:,Вы можете изучить fiber-дерево, обратившись к самому верхнему fiber-узлу , через fiber root. Или вы можете получить отдельный fiber-узел из экземпляра компонента следующим образом:,Теперь рассмотрим структуру fiber-узлов, созданных для компонента ,:,и DOM-элемента ,:,В fiber-узлах довольно много полей. Я описал назначение полей ,, , и , в предыдущих разделах. Теперь давайте посмотрим, зачем нам нужны другие.,Хранит ссылку на экземпляр класса компонента, узла DOM или другой тип React элемента, связанный с fiber-узлом. В общем, можно сказать, что это свойство используется для хранения локального состояния, ассоциированного с fiber.,Определяет функцию или класс, связанный с этим fiber. Для классовых компонентов оно указывает на функцию-конструктор, а для DOM элементов - на HTML-тег. Я довольно часто использую это поле, чтобы понять, с каким элементом связан fiber-узел.,Определяет ,. Он используется в алгоритме согласования (reconciliation), чтобы определить, какую работу нужно выполнить. Как упоминалось ранее, работа варьируется в зависимости от типа React элемента. Функция , сопоставляет элемент React с соответствующим типом fiber-узла. В нашем приложении свойство , для компонента , равно ,, что обозначает ,, а для элемента , - ,, что обозначает ,.,Очередь обновлений состояния, обратных вызовов и обновлений DOM.,Состояние fiber-a, которое было использовано для создания вывода. При обработке обновлений оно отражает состояние, которое в данный момент выводится на экран.,Пропсы fiber-а, которые были использованы для создания вывода во время предыдущего рендера.,Пропсы, которые были обновлены на основе новых данных в React элементах и должны быть применены к дочерним компонентам или DOM элементам.,Уникальный идентификатор с группой дочерних элементов, помогающий React выяснить, какие элементы изменились, были добавлены или удалены из списка. Это связано с функциональностью ""списки и ключи"" React, описанным ,.,Полную структуру fiber-узла можно найти ,. В приведенном выше объяснении я пропустил кучу полей. В частности, я пропустил указатели ,, , и ,, составляющие древовидную структуру данных, которую я ,. И пропустил категорию полей, таких как ,, , и ,, которые специфичны для ,.,React выполняет работу в двух основных фазах: , и ,.,Во время первой фазы , React применяет обновления к компонентам, запланированные через , или ,, и выясняет, что нужно обновить в пользовательском интерфейсе. Если это первоначальный рендеринг, React создает новый fiber-узел для каждого элемента, возвращенного из метода ,. При последующих обновлениях fiber-ы для существующих React элементов используются повторно и обновляются. , Эффекты описывают работу, которая должна быть выполнена во время следующей фазы ,. Во время этой фазы React берет fiber-дерево, помеченное эффектами, и применяет их к экземплярам. Он просматривает список эффектов и выполняет обновления DOM и другие изменения, видимые пользователю., фазы может выполняться асинхронно. React может обработать один или несколько fiber-узлов в зависимости от доступного времени, затем остановиться, чтобы сохранить проделанную работу и уступить какому-либо событию. Затем он продолжает работу с того места, где остановился. Иногда, однако, может потребоваться отбросить проделанную работу и начать все сначала. Такие паузы возможны благодаря тому, что работа, выполняемая в этой фазе, не приводит к каким-либо видимым пользователю изменениям, таким как обновление DOM. , фаза всегда синхронна. Это происходит потому, что работа, выполняемая на этом этапе, приводит к изменениям, видимым пользователю, например, к обновлению DOM. Поэтому React должен выполнять её за один проход.,Вызов методов жизненного цикла - это один из видов работы, выполняемой React. Некоторые методы вызываются на этапе ,, а другие - на этапе ,. Вот список методов жизненного цикла, вызываемых при выполнении первой фазы ,:,[UNSAFE_]componentWillMount (deprecated),[UNSAFE_]componentWillReceiveProps (deprecated),getDerivedStateFromProps,shouldComponentUpdate,[UNSAFE_]componentWillUpdate (deprecated),render,Как вы можете видеть, некоторые устаревшие методы жизненного цикла, выполняемые на этапе ,, помечены как , с версии 16.3. В документации они теперь называются legacy lifecycles. Они будут устаревшими в будущих релизах 16.x, а их аналоги без префикса , будут удалены в версии 17.0. Подробнее об этих изменениях и предлагаемом пути миграции вы можете прочитать ,.,Вам интересно узнать причину этого?,Ну, мы только что узнали, что поскольку фаза , не производит побочных эффектов, таких как обновление DOM, React может обрабатывать обновления компонентов асинхронно (потенциально даже делая это в несколько потоков).Однако жизненные циклы, помеченные ,, часто понимались неправильно и часто использовались не по назначению. Разработчики склонны помещать код с побочными эффектами внутрь этих методов, что может вызвать проблемы с новым подходом к асинхронному рендерингу. Хотя будут удалены только их аналоги без префикса ,, они все еще могут вызвать проблемы в предстоящем Concurrent Mode (от которого вы можете отказаться).,Вот список методов жизненного цикла, выполняемых во время второй фазы ,:,getSnapshotBeforeUpdate,componentDidMount,componentDidUpdate,componentWillUnmount,Поскольку эти методы выполняются в синхронной фазе ,, они могут содержать побочные эффекты и затрагивать DOM.,Итак, теперь у нас есть предпосылки, чтобы взглянуть на обобщенный алгоритм, используемый для обхода дерева и выполнения работы. Давайте погрузимся внутрь.,Алгоритм согласования (reconciliation) всегда начинается с самого верхнего fiber-узла , с помощью функции ,. Например, если вы вызовете , глубоко в дереве компонентов, React начнет с вершины, но быстро пропустит родительские узлы, пока не доберется до компонента, у которого был вызван метод ,.,Все fiber-узлы обрабатываются , (work loop). Вот реализация синхронной части цикла:,В приведенном выше коде переменная , хранит ссылку на fiber-узел из дерева ,, в котором еще есть незавершенная работа. Когда React обходит fiber-дерево, он использует эту переменную, чтобы узнать, есть ли еще какой-нибудь fiber-узел с незавершенной работой. После обработки текущего fiber переменная будет содержать либо ссылку на следующий fiber-узел в дереве, либо ,. В этом случае React выходит из рабочего цикла и готов закоммитить изменения.,Существует 4 основные функции, которые используются для обхода дерева и инициирования или завершения работы:,Чтобы продемонстрировать, как они используются, посмотрите на следующую анимацию обхода fiber-дерева. Для демонстрации я использовал упрощенную реализацию этих функций. Каждая функция берет для обработки fiber-узел, и по мере того, как React спускается по дереву, вы можете видеть, как меняется текущий активный fiber-узел. На анимации хорошо видно, как алгоритм переходит от одной ветви к другой. Сначала он завершает работу для дочерних узлов, а затем переходит к родительским.,Обратите внимание, что прямые вертикальные связи обозначают сиблингов (братьев и сестер), а изогнутые - детей, например, у , нет детей, а у , есть один ребенок ,.,, где можно приостановить воспроизведение и просмотреть текущий узел и состояние функций. Концептуально, вы можете думать о ""begin"" как о ""шаге в"" компонент, а о ""complete"" как о ""шаге из"" него. Вы также можете ,, пока же я объясню, что делают эти функции.,Давайте начнем с первых двух функций , и ,:,Функция , получает fiber-узел из дерева , и начинает работу, вызывая функцию ,. Это функция, которая запускает все действия, которые должны быть выполнены для fiber-а. Для целей данной демонстрации мы просто записываем в лог имя fiber-а, чтобы обозначить, что работа была выполнена. , всегда возвращает указатель на следующего ребенка для обработки в цикле или ,.,Если есть следующий ребенок, он будет присвоен переменной , в функции ,. Однако если дочернего элемента нет, React знает, что достиг конца ветви, и поэтому может завершить текущий узел. , Это делается в функции ,:,Вы можете видеть, что весь код функции заключается в большом цикле ,. React попадает в эту функцию, когда у узла , нет дочерних элементов. После завершения работы для текущего fiber-а, он проверяет, есть ли у него дочерний узел. Если он найден, React выходит из функции и возвращает указатель на сиблинга. Он будет присвоен переменной ,, и React выполнит работу для ветви, начинающейся с этого сиблинга. Важно понимать, что на данный момент React выполнил работу только для предшествующих сиблингов. Он не выполнил работу для родительского узла. ,.,Как видно из реализации, обе функции и , используются в основном для целей итерации, в то время как основная деятельность происходит в функциях , и ,. В следующих статьях цикла мы узнаем, что происходит для компонента , и узла ,, когда React переходит к функциям , и ,.,Фаза начинается с функции ,. Именно здесь React обновляет DOM и вызывает методы жизненного цикла до и после мутации.,Когда React доходит до этой фазы, у него есть 2 дерева и список эффектов. Первое дерево представляет состояние, которое в настоящее время отображается на экране. Затем есть альтернативное дерево, построенное во время фазы ,. Оно называется , или , в источниках и представляет состояние, которое должно быть отражено на экране. Это альтернативное дерево связано с текущим деревом через указатели , и ,.,И затем, есть список эффектов - подмножество узлов из дерева ,, связанное через указатель ,. Помните, что список эффектов - это , выполнения фазы ,. Весь смысл рендеринга в том, чтобы определить, какие узлы должны быть вставлены, обновлены или удалены, и какие компоненты должны вызвать свои методы жизненного цикла. И именно об этом нам говорит список эффектов. ,В целях отладки доступ к , дереву можно получить через свойство , fiber-корня. Доступ к дереву , можно получить через свойство , узла , в текущем дереве.,Основной функцией, выполняемой на этапе commit, является ,. В основном, она делает следующее:,Вызывает метод жизненного цикла , на узлах, помеченных эффектом ,.,Вызывает метод , жизненного цикла для узлов, помеченных эффектом ,.,Выполняет все вставки, обновления и удаления в DOM,Устанавливает дерево , в качестве текущего.,Вызывает метод жизненного цикла , на узлах, помеченных эффектом ,.,Вызывает метод , жизненного цикла для узлов, помеченных эффектом ,.,После вызова метода предварительной мутации ,, React фиксирует все побочные эффекты внутри дерева. Он делает это в два прохода. Первый проход выполняет все вставки, обновления, удаления и размонтирования DOM (хоста). Затем React назначает дерево , на ,, помечая дерево , как ,. Это делается после первого прохода фазы commit, чтобы предыдущее дерево было актуальным во время ,, но до второго прохода, чтобы законченная работа была актуальной во время ,. Во время второго прохода React вызывает все остальные методы жизненного цикла и обратные вызовы. Эти методы выполняются как отдельный проход, так что все размещения, обновления и удаления во всем дереве уже были вызваны.,Вот сниппет функции, которая выполняет описанные выше шаги:,Каждая из этих подфункций реализует цикл, который итерирует список эффектов и проверяет их тип. Когда он находит эффект, соответствующий цели функции, он применяет его.,Вот, например, код, который выполняет итерацию по дереву эффектов и проверяет, имеет ли узел эффект ,:,Для классового компонента это действие означает вызов метода жизненного цикла ,., - это функция, с помощью которой React выполняет обновление DOM. Функция в основном определяет тип операции, которую необходимо выполнить для узла, и выполняет ее:,Интересно, что React вызывает метод , как часть процесса удаления в функции ,., - это функция, в которой React вызывает все оставшиеся методы жизненного цикла , и ,.,Наконец-то мы закончили. Дайте мне знать, что вы думаете о статье или задайте вопросы в комментариях. , У меня в работе еще много статей, в которых подробно рассказывается о планировщике, процессе согласования детей и о том, как строится список эффектов. Я также планирую создать видео, в котором покажу, как отлаживать приложение, используя эту статью в качестве основы.,Web-разработчик, Android-разработчик",Fiber изнутри: погружение в новый алгоритм согласования React / Хабр
[<200 https://habr.com/ru/post/662622/>],page2,"  ,Число кибератак стремительно растет: если раньше их количество исчислялось десятками в месяц, сейчас насчитывают тысячи инцидентов только за одну неделю: «Касперский» , о 8-кратном увеличении числа DDoS-атак на российские организации. В то же время недавнее , SearchInform сообщает, что из 900 опрошенных ими компаний 95% ограничиваются только антивирусом в вопросах защиты от кибератак. В таких условиях вероятность столкнуться с инцидентом ИБ значительно выше, чем может показаться. Поэтому все же стоит отбросить сомнения, вроде «да кому наша информация нужна», и заранее разобраться, что делать, если инцидент уже случился.  ,Из очевидного: мониторить события безопасности антивирусов и межсетевых экранов. Помимо этого, стоит обратить внимание на эти пункты.  ,Запуск ПО, неразрешенного к установке или запуску пользователям. ,Неизвестный сетевой трафик. ,  ,Активная деятельность пользователя в нерабочее время. ,Подозрительная загруженность систем. ,Подозрительные изменения в реестре. ,Сообщения о критической системной ошибке и перезагрузка систем. ,Аномальная работа с файлами и программами. ,Подозрительное изменение настроек операционной системы, антивирусного и прикладного ПО. , ,Неизвестная активность на электронной почте. ,Аномалии в журналах авторизации. ,Сначала расставим приоритеты. Для этого нужно оценить уровень критичности скомпрометированных информационных ресурсов и технических средств: критичный, высокий, средний или низкий. От того, насколько серьезен ущерб, зависит количество времени, которое потребуется на расследование, и оперативность принятия решений. ,Важный момент: если вы обнаружили кибератаку, ни в коем случае не отключайте технику от питания, это может привести к потере ценных индикаторов, необходимых для расследования, и сбою системы при запуске.,Теперь перейдем непосредственно к реагированию на кибератаку., ,В первую очередь, важно минимизировать количество скомпрометированных объектов инфраструктуры, ограничить дальнейшее продвижение злоумышленников по сети.,Перенастроить правила на маршрутизирующем оборудовании (в случае вирусной атаки лучше сделать это так, чтобы остался доступ в интернет, а возможность добраться до других машин в локальной сети – нет. Это нужно, чтобы предотвратить самоуничтожение вредоносного ПО и всех следов, которые оно может оставить);,Настроить правила на уровне межсетевого экрана операционной системы;,Отключить сетевой кабель;,Физически разорвать кабель (например, перерезать ножом, такой радикальный способ уместен, если нет быстрого доступа к портам технических средств; возможно, сервер находится в запертом на ключ шкафу).,На этом этапе важно сохранить как можно больше информации о состоянии инфраструктуры, составить отчет об инциденте и зафиксировать все данные. Это позволит использовать их в качестве юридически значимых свидетельств для привлечения злоумышленников к ответственности.,Журналы событий безопасности установленных средств защиты информации. Это возможно, только если в них было настроено логирование событий.,Журналы событий операционной системы (лог-файлы, даты создания\модификации файлов). Это нужно, чтобы понять, какие действия в системе были совершены и в какое время: попытки авторизации, запуск приложений и изменение файлов.,Дампы оперативной памяти, swap-раздела или pagefile.sys. Позволяет зафиксировать состояние системы в момент инцидента, чтобы можно было проанализировать его, пока система восстанавливается. Если вредоносная программа исполняется, не копируя себя на скомпрометированную систему, эти данные станут единственным доступным источником для расследования инцидента. Если же в систему проник вирус-шифровальщик, из оперативной памяти можно вытащить ключи шифрования для восстановления данных. Кроме того, дамп оперативной памяти покажет, какие действия совершал сотрудник в момент инцидента, это поможет доказать факт утечки данных.,Трафик маршрутизаторов и коммутационного оборудования. Эта информация пригодится для определения источника инцидента, уязвимостей, через которые удалось атаковать инфраструктуру, и вектора распространения. В случае утечки, по логам трафика можно понять, какие файлы похитили.,Посекторная копия физического диска. Полная копия диска компьютера со всеми журналами, пользовательскими и системными файлами – это возможность восстановить удаленные данные и изучить все связанные с потенциальным вредоносом файлы. ,Есть несколько способов это сделать. Все зависит от характера атаки и степени повреждения данных. При выборе метода не стоит забывать о трудозатратах и временных затратах, зачастую утраченная информация стоит меньше, чем потраченные на ее восстановление ресурсы.  ,Применить ПО для восстановления удаленной информации. Покупать его может быть невыгодно для компании, стоит оно дорого. Поэтому, если бесплатные решения не смогли восстановить поврежденную информацию, целесообразнее будет обратиться к специалистам.,Обратиться к специалистам по восстановлению данных.,Воспользоваться резервными копиями. Самый надежный и простой вариант. Но для этого в компании изначально должно быть настроено резервное копирование.,  ,На автоматизированном рабочем месте сотрудника рекомендуется полностью переустановить систему и все ПО. Это сэкономит время и деньги на восстановление поврежденных данных.  ,  ,Если причиной инцидента стало вредоносное ПО, можно задействовать сканирование антивирусным программным обеспечением. Если такой возможности нет, используйте отдельные портативные решения, которые решат проблему точечно.  ,Лучше по этому вопросу обратиться сразу к специалистам. Чтобы качественно изучить кибератаку, нужно обладать пулом специальных навыков и знаний об устройстве операционных систем, типичных видах атак и их особенностях. ,Когда станет понятно, что стало причиной инцидента и где нашлись уязвимости, важно заняться их устранением. Иначе все ресурсы будут потрачены впустую, и за этим инцидентом последуют еще десятки. Это может стать пятым этапом реагирования.,Ждать, пока у вас случится инцидент, чтобы проверить все эти этапы на практике, мы, конечно, не будем. Просто пройдемся по готовому кейсу для понимания  ,  ,Ограничиваем скомпрометированное техническое устройство от локальной сети с сохранением доступа к интернету: настраиваем либо межсетевой экран, либо коммутирующее оборудование. На ОС семейства Windows это можно сделать с помощью команды в netsh:,Для ОС семейства Linux можно воспользоваться межсетевым экраном iptables:  ,<server_net> — сеть, в которой расположен сервер (например, 10.0.2.0/24),,<local_net> – сеть, в которую запрещен любой доступ с сервера (например,10.0.0.0/24).,. Делаем дампы оперативной памяти (можно с помощью Belkasoft RAM Capturer).  Созданные дампы лучше скопировать на диск и сохранить. Его можно будет использовать для приобщения к материалам проверки и для проведения компьютерно-технической экспертизы. Упаковка диска должна быть такой, чтобы к нему невозможно было получить доступ без видимого нарушению.,. Копируем лог-файлы. Для ОС семейства Windows мы делаем это так:,Запускаем программу «Просмотр событий» (win+r, «eventvwr.msc»).,Выбираем необходимый журнал и нажимаем кнопку «Сохранить все события как…» в окне «Действия».,Если так сохранить лог-файлы не получается, можно просто скопировать все файлы из папки «С:/Windows/System32/winevt/Logs».,. Сохраняем файлы реестра «SYSTEM», «SOFTWARE», «SAM», «SECURITY» из папки «\Windows\System32\config» и «NTUSER.DAT» из домашней папки пользователя «\Users\<имя учетной записи>\». ,. Сохраняем настроенные расширения браузеров. Их местоположения:,«C:\Users\XXX\AppData\Local\Google\Chrome\User Data\Default\Extensions\»,«C:\Users\XXX\AppData\Local\Google\Chrome\User Data\ChromeDefaultData\Extensions\»,«C:\Users\XXX\AppData\Roaming\Mozilla\Firefox\Profiles\[profileID].default\addons.sqlite»,«C:\Users\XXX\AppData\Roaming\Mozilla\Firefox\Profiles\[profileID].default\extensions.sqlite»,Opera,«C:\Users\<USER>\AppData\Local\Opera\Opera\widgets\»,Для ОС семейства Linux нужно просто сохранить файлы из директории «/var/logs». Скопировать системные и недавно появившиеся/изменившиеся файлы.,В случае удаления данных:,Самостоятельно восстанавливаем удаленные данные с помощью специальных программ.,Делаем посекторный образ диска и отдаем на восстановление в специализированную лабораторию. Это пункт для тех, кто потерял информацию высокого уровня критичности.,Восстанавливаем резервные копии, если они есть.,В случае шифрования данных:,Обращаемся в компанию по разработке антивирусного ПО, которая может предоставить инструменты для противодействия конкретному виду шифровальщика. Если данные возможно восстановить, то скорее всего такая компания может предоставить дешифратор для файлов.,При расследовании инцидента есть вероятность, что эксперты смогут помочь с дешифрованием файлов, если процесс реагирования на инцидент был последовательным и никакие данные не уничтожены.,Собираем все сохраненные данные и идем к сторонней компании, специализирующейся на расследовании инцидентов., Это может быть:,обновление ПО до актуальной версии;,обновление групповых политик на контроллере домена.,Пользователь",У нас кибератака. Что делать? / Хабр
[<200 https://habr.com/ru/post/662634/>],page2,"Тестирование — неотъемлемая часть разработки игр, и мобильные тайтлы не исключение. Рынок полон устройств самых разных форм-факторов, мощности и совместимости. При этом чем больше игроков, тем игра успешнее и тем доход больше. Чтобы игроков было как можно больше, нужна поддержка максимального количества смартфонов и планшетов. Чтобы этого добиться, нужно “отполировать билд” для работы даже на самых бюджетных девайсах.,Раз тестирование необходимо, возникает вопрос: как его проводить? Самый простой способ — набрать пул девайсов и запускать на них игру, отслеживать баги и устранять их. Способ надежный, но он сильно зависит от количества тестировщиков, и требует большего размера затрат, т.к. есть риск утери или повреждения устройств. Отслеживание ошибок также затрудняется при физическом тестировании, т.к. здесь информация для разработчика определяется только тем, насколько ответственно к процессу подходит тестировщик.,Ускорить, оптимизировать и автоматизировать процесс можно и нужно. Это можно сделать, установив тестовую ферму. Меня зовут Максим Шагов, я работаю QA лидом в мобильном игровом паблишинге и расскажу как это сделать и что это такое.,Если коротко, ферма - тестовый ПК, аналогичный тем, на которых тестируют компьютерные игры, только там нет видеокарты (не нужна), а остальное железо более бюджетное. Предназначение такой установки — создать место, где сотрудник может получить устройство для теста игры без прямого к нему физического доступа. ,Это особенно полезно, если в компании большой парк девайсов, и следить за ними довольно сложно. Чем больше сотрудников, тем сложнее, ведь каждому нужно устройство. Ферма же позволяет держать девайсы подключенными все время, и выдавать их сотрудникам без лишних проволочек и бюрократии.,Для пользователя работа с фермой аналогична работе с функциями удаленного воспроизведения, которая есть на некоторых игровых консолях. Все смартфоны и планшеты подключены к ферме, развернутой на Ubuntu. В нашем случае мы основывали ее на исходном коде, бесплатно доступном на GitHub, который доработали и оптимизировали под потребности компании, а также обновили, поскольку изначальный разработчик прекратил поддержку 4-5 лет назад. На данный момент ферма поддерживает все версии Android, вплоть до 12-й. Работы над поддержкой iOS также ведутся. ,Принцип работы в деталях выглядит так: на базе Ubuntu развернут сервер, который использует Rethink как базу данных. Выбор Ubuntu объясняется просто — она лучше всего подходит для работы в силу своего функционала. Подключение пользователя к устройствам производится через веб-интерфейс, а сами устройства подсоединяются к ПК через через Android Debug Bridge. Он показывает каждую конкретную сессию каждого пользователя на том или устройстве в текущий момент времени. Администратору при этом доступна история всех операций. Подключиться к интерфейсу можно даже со смартфона, и использовать его для тестирования другого устройства, например зайти с iOS-смартфона и работать на Android-планшете. ,Учитывая, что работа по сути ведется через веб-соединение, может возникнуть вопрос: “Не вызовет ли слабое соединение затруднений в работе?”. Как показали наши тесты, сверхмощного соединения такая установка не требует, подходит даже вполне бюджетное подключение в 100 МБит/с, а работать с ней может до 120 человек одновременно.,Во-первых, как уже говорилось, чем больше девайсов поддерживает игру, тем лучше. Тестирование на большом парке разных устройств — лучший способ эту поддержку обеспечить. Ферма позволяет существенно ускорить и упростить процесс, в том числе облегчая доступ к смартфонам и планшетам. ,Во-вторых, это не привязывает тестирование к офису компании. Сотруднику не нужно брать девайс физически, а значит нет необходимости оставаться с ним на рабочем месте, во избежание утери или нарушения соглашений о неразглашении. Подключиться для тестирования можно и с рабочего ПК и с собственного устройства, соблюдая должные меры безопасности.,В-третьих, тестовые устройства легче контролировать через ферму. Она отслеживает, кто, где и когда берет какой смартфон, что помогает оптимизировать график и определять возможные причины ошибок и вылетов. ,В-четвертых, ферма поддерживает макросы, что позволяет одновременно тестировать одну и ту же игровую сессию на разных устройствах. Это серьезно сокращает время тестирования и трудозатраты сотрудников. Альтернативные способы, например эмуляция, не всегда покрывают такие потребности. ,Наконец, через ферму можно записывать видео и снимать скриншоты автоматически. Это помогает при вылетах или каких-то проблемах сразу прикладывать к отчетам об ошибках подтверждения. ,Есть, разумеется, и минусы, но они, скорее, индивидуальные. Например, для каждого форм-фактора и девайсов с разным разрешением нужно записывать макросы индивидуально. Чем больше макросов, тем больше требуется поддержки. Сейчас мы работаем над решением этой проблемы. ,Также в перспективе мы планируем довести работу системы до дублирования сессий с одного устройства на остальные через Wi-Fi, и снятия через него логов со всех одновременно. Это позволит проводить тестирование сразу всех устройств с одного девайса из любой точки офиса. ,“На самом деле рецепт простой”. Решение доступно и инди-разработчикам, и большим компаниям. Мы, например, используем для фермы ПК на базе процессора i7, с 32 ГБ оперативной памяти, SSD на 500 ГБ и жестким диском на 2 ТБ для записи логов, скриншотов и видео. Помимо этого, потребуются версия Ubuntu 20.0.4 и докер, куда устанавливается контейнер с готовым сервером, чтобы перезапустить ферму в случае сбоя. Сделать последнее очень просто, это сможет даже начинающий специалист. Сервер можно развернуть и на менее мощных системах, все зависит от количества устройств и частоты пользования. ,Что касается индивидуальной доработки, ее сможет осуществить любой сотрудник с уровнем подготовки системного администратора. Доработка происходит итерационно, и это непрерывный процесс, но основная часть работы по установке и настройке осуществляется за короткие сроки. В нашем случае изучение материала потребовало 1 месяц, а сам сервер был развернут за 4 рабочих дня.,Для пользователя работа с фермой схожа с работой на самих устройствах. Это выглядит как работа в эмуляторе, но только с использованием ресурсов телефона. При этом администратор фермы видит все сессии в реальном времени и может в любой момент посмотреть логи, что существенно экономит время, т.к. причина “вылета” приложения становится известна практически сразу.,Поддерживаются все функции смартфонов, включая различные языки, ввод с клавиатуры, мультитач и жесты. Сами устройства находятся в специальном боксе в непосредственной близости и их можно отслеживать в том числе и физически. ,Безусловно, дополнительная нагрузка создается, в первую очередь из-за подключения в режиме debug bridge, что влияет на аккумулятор. По заявлениям производителей, срок службы устройства в таких условиях может снизиться на 20%. Однако на практике это не играет большой роли, особенно, учитывая скорость, с которой рынок обновляет девайсы — заменять их требуется чаще во избежание отставания от конкурентов, чем по причине выхода из строя устройств. Однако иметь запас запчастей и аккумуляторов все-таки стоит.,Такое решение может не подойти только сильно таргетированным командам, однако большой вопрос, остались ли такие в наше время. У Android большой пул устройств и широкий диапазон параллельно поддерживаемых версий ОС. С iOS ситуация более или менее похожая. Поэтому ферма — гораздо более удобный вариант. Правильно подобранный парк устройств позволит оптимизировать работу под максимально широкий диапазон. Компания сможет более оперативно выявлять ошибки и устранять их, сокращая временные затраты, что особенно важно в индустрии, где игра должна быть в идеальном состоянии до релиза, иначе потерянные на старте игроки могут уже и не вернуться.  ,Пользователь",Тестовая ферма: как упростить тестирование мобильных игр с минимумом затрат / Хабр
[<200 https://habr.com/ru/post/662658/>],page2,"Друзья, всем пример. Меня зовут Макс и я хочу познакомить вас с IsEngine framework.,Это новый фреймворк отечественной разработки для простого и быстрого создания web-приложений.,Я занимаюсь его разработкой в течение последних 5 лет. Сейчас готовлю его к релизу, и поэтому начинаю продвигать.,Чтобы понять любой продукт, вообще любую систему, нужно понимать, какие задачи она решает. И здесь самый первый и самый главный вопрос - это для кого она создана. Для программистов? Разработчиков? Или для обычных людей (я имею ввиду не специалистов в web'е).,Я позиционирую isEngine как фреймворк от разработчиков для разработчиков. Я достаточно давно знаком с web'ом в целом, первые сайты начал создавать году в 2002, еще когда только поступил в техникум. Это, наверное, обычная для многих история. Теперь вот непосредственно в разработке, с 2017-го.,Здесь я хочу обратить внимание на то, что когда ты создаешь сайт для себя... Не важно, сам или нанимая подрядчика, в конце концов тебе потом им пользоваться, и в конечном итоге это твое приобретение. Так вот, когда ты занимаешься этим для себя, ты стараешься использовать самые передовые технологии, и это разумнее всего. Но когда ты представляешь студию, которая занимается разработкой большого числа проектов, и часть из них (если не все) потом обслуживает, ситуация предстает в совершенно ином свете.,А обслуживание, хочу отметить, приносит далеко не столько же денег, сколько сама разработка, а времени занимает, ну почти столько же. И время - это еще один важный ресурс. Надо понимать, что время ограничено, и есть «потолок» проектов, которые один человек физически может обслуживать.,И здесь ты уже смотришь не на то, какие технологии в топе, а на то, как быстро можно их внедрять, насколько они «дружат» между собой и насколько безболезненно их можно обновлять.,Конечно, большую роль по-прежнему играют возможности того или иного решения. Но если для себя ты можешь какими-то недостатками пренебречь, то в выборе решений для заказчика это всегда компромисс между тем, что нужно сейчас и тем, что будет в перспективе дальнейшего развития. И хотя на рынке существует множество решений, они в большинстве своем не сильно отличаются друг от друга.,isEngine автоматизирует многие процессы. Например, редиректы с «www» на «без www», с «http» на «https» и наоборот без нужды влезать в конфиг сервера. Он налету генерирует «robots» и «sitemap» по запросу.,Широко настраивается router. Например, вы можете задать обязательное расширение для конечных страниц, использование или отключение индексных страниц. Это полезно, если вы хотите обновить сайт, но при этом важно оставить всю накопленную «ссылочную массу».,Шаблонизатор имеет огромные возможности. Например, вы можете создать несколько админок, личный кабинет, закрытые разделы и так далее. Можно вынести часть элементов шаблона в блоки и использовать их между шаблонами.,Есть реализация API, причем тремя способами: в виде обозначения части пути, в виде номера для части пути или сделать весь сервер только для API. Ну и конечно можно отключить его совсем.,Что еще? Есть автогенерация SEO, различных меню.,Так как это наша, российская разработка, я очень внимательно отношусь к поддержке кириллицы в частности и к поддержке мультиязычности в общем. Так, в isEngine «из коробки» идет очень простая и легко настраиваемая мультиязычность. Мне кажется, это будет полезно не только нам, но и в других странах, особенно где не используется латинский алфавит.,На мой взгляд, все классные разработки развивались естественным путем. Они завоевали рынок за счет того, что сами по себе были классные, удобные и нравились людям.,Я много слышал о том, что в нашей стране отечественные проекты принимаются плохо. Но это не так. Есть много примеров web-разработок, я не буду их здесь называть, которые востребованы у нас. Ими пользуются. И не потому, что «партия распорядилась», а потому что они действительно нравятся людям.,Конечно, я хочу, чтобы isEngine тоже пользовались. Чтобы вокруг него сформировалось русскоязычное сообщество, пусть не большое, но дружное. Для этого нужно привлечь внимание к проекту. Пишется документация, готовятся обучающие ролики, будут проводится прямые трансляции.,Вот мы приступаем к очередному проекту. У него есть его собственная, внутренняя, сложность. И мы выбираем фреймворк.,А фреймворки нужны для того, чтобы упростить нам жизнь, освободить от написания однотипного кода. Но по мере их развития, кодовая база разрастается и они начинают привносить свою долю сложности в проект.,Поэтому мы оказываемся в двух ситуациях. Вариант недостаточности, когда фреймворка недостаточно, чтобы перекрыть внутреннюю сложность. И тогда нам придется либо дорабатывать его, либо подключать дополнительные инструменты. И второй вариант - вариант избыточности. Когда необходимый нам функционал перекрывается, но оставшаяся часть висит «мертвым грузом», а это излишняя трата ресурсов.,Ну может, и нагнетаю. Но реально такие ситуации везде. Вопрос, насколько это критично.,isEngine я разрабатывал с тем учетом, чтобы в нем были инструменты, позволяющие максимально эффективно его использовать.,Например, вы можете использовать его как библиотеку в своем проекте. Причем там такая модульная система, где классы сгруппированы по функционалу. Не нужна работа с базами данных - не подключайте этот класс. Нужны хелперы - берите только хелперы.,Библиотека и ядро изолированы. Вы можете расширить существующее ядро фреймворка, можете написать свое. Можете расширить функционал существующих классов, можете создать свои компоненты, используя наследование. Можете создавать свои хелперы.,Для кастомизации системы - просто неограниченное число возможностей.,Установке и обновлению я уделял особое внимание. Во-первых, чтобы они происходили максимально просто, быстро и безболезненно для пользователя. Во-вторых, чтобы любое обновление не могло снести все надстройки, которые вы сделали.,Почему-то сложилось мнение, что пользуясь отечественным, вы почти всегда получаете продукт более низкого качества, чем зарубежный аналог. Я сейчас не буду касаться причин. Здесь хочется сказать, что у меня не так, что здесь все отлично! Но правда в том, что isEngine не лишен недостатков. Я работаю над тем, чтобы он был идеальным. Но нельзя все сделать сразу и также нельзя все сделать одному.,Так что в моем случае, это следствие того, что я занимаюсь фреймворком в одиночку, без поддержки.,Я стараюсь быть объективным.,Сейчас я приступаю к работе над релизом. До этого надо документацию причесать. Опять же для себя, потому что я сам часто ей пользуюсь. И составить дорожную карту. Там будет все написано про планы на ближайшее будущее.,Чтобы isEngine успешно развивался, им должны пользоваться. Я, в свою очередь, стараюсь сделать его максимально полезным.,Если вы хотите помочь фреймворку, начните использовать его в своих проектах.,Подписывайтесь на наши каналы, вступайте в наши группы. Ссылки на них есть ниже. Комментируйте, задавайте вопросы.,Я жду вас в рядах сообщества.,Спасибо.,Ссылки:, , , ,Пользователь",PHP фреймворк отечественной разработки / Хабр
[<200 https://habr.com/ru/company/productivity_inside/blog/662668/>],page2,Пользователь,Дайджест интересных материалов для мобильного разработчика #440 (18 — 24 апреля) / Хабр
[<200 https://habr.com/ru/post/662652/>],page2,"Модальные диалоги не такая и сложная задача в разработке. Разве что можно легко запутаться в коде, если нужно вызывать модальные окна по цепочке. Но это очень монотонная и неинтересная работа с повторяющейся логикой, которую подчас копируют из компонента в компонет с незначительными изменениями.,Хорошим решением было бы создать функцию, которая принимала бы компонент диалога и управляла бы его рендерингом в шаблоне, а этот диалог можно ""промисифицировать"" и работать с ним как с асинхронной функцией. Как например в этой библиотеке ,. К сожалению она давно не обновлялась и не опддерживает Vue 3.,А еще отличной идеей для организации кода было бы отдельные хуки, код в которых выполнялся бы в зависимости от действий пользователя. ,Все это есть в плагине ,, о котором я хочу вам рассказать.,Конечный результат можно посмотреть в ,. Код немного отличается от того, что в посте.,Начнем с создания нового проекта на Vue 3. Введем в консоли:,Мы получили стандартный шаблон нового проекта. Далее установим библиотеку согласно документации в ,.,Заменим код в , на такой:,Теперь перейдем в файл App.vue. Первым делом исправим код шаблона. Для работы библиотеки нам обязательно нужно добавить в него компонент , и удалим ,:,Теперь изучим как использовать функцию ,. Используем новый синтаксис , для раздела ,. , первым аргументом принимает компонент, который будет использоваться как модальное окно, а вторым входные данные для него. Функция возвращает обьект с методами для работы с модальным окном, так метод , вызывает диалог, а хук , принимает код, который выполниться если пользователь нажмет на ""согласен"". Можно заставить появляться компонерт , при нажатии на лого и передать ему значение пропса ,:,Никакой дополнительной логики не требуется. Компонент отрисовывается после вызова функции , и исчезает после того, как пользователь отреагирует на диалог.,Теперь напишем что-то более приближенное к реальному использованию.,Создадим новый компонент , в папке ,:,Обратите внимание, что для полноценной работы нужно добавить два входящих события в модальный диалог: ,.,А теперь используем его для подтверждения какого-либо действия, например, чтобы спрятать логотип. Логику кода, который будет исполняться после согласия пользователя, поместим в коллбек хука ,.,Что делать, если у нас есть много случаев, когда требуется подтверждение каких-либо действий? Не писать же каждый раз , заново?,Можно написать функцию, которая автоматизирует этот процесс для нас.,Теперь используем ее для подтверждения перехода по внешним ссылкам:,Функция , позволяет упростить работу с модальными окнами, переиспользование логики и создание цепочек последовательных диалогов. Она берет на себе заботу о рендере модального окна, передачу входящих параметров в компонент и получении данныт от него. Она очень гибкая — ее очень легко подстроить под ваши нужды.,Это не все ее возможности. Например, если концеция хуков вам не близка, можно заменить их на работу с промисом, который возвращает функция ,. И даже использовать её в Options API.,Пользователь",Простой способ создания и переиспользования модальных диалогов во Vue 3 / Хабр
